{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n\n# Generalized parameter-shift rules\n\n.. meta::\n\n    :property=\"og:description\": Reconstruct quantum functions and compute their derivatives.\n    :property=\"og:image\": https://pennylane.ai/qml/_images/thumbnail_genpar.png\n\n.. related::\n\n   tutorial_expressivity_fourier_series Investigating quantum models as Fourier series\n   tutorial_rotoselect Leveraging trigonometry to choose circuits with Rotoselect\n   tutorial_quantum_analytic_descent Building multivariate models with QAD\n\n\n*Author: David Wierichs (Xanadu resident). Posted: 23 August 2021.*\n\nIn this demo we will look at univariate quantum functions, i.e., those that\ndepend on a single parameter. We will investigate the form such functions usually take\nand demonstrate how we can *reconstruct* them as classical functions, capturing the full\ndependence on the input parameter.\nOnce we have this reconstruction, we use it to compute analytically exact derivatives\nof the quantum function. We implement this in two ways:\nfirst, by using autodifferentiation on the classical function that is produced by the \nreconstruction, which is flexible with respect to the degree of the derivative.\nSecond, by computing the derivative manually, resulting in generalized parameter-shift\nrules for quantum functions that is more efficient (regarding classical cost) than the \nautodifferentiation approach, but requires manual computations if we want to access\nhigher-order derivatives.\nAll we will need for the demo is the insight that these functions are Fourier series in their\nvariable, and the reconstruction itself is a\n`trigonometric interpolation <https://en.wikipedia.org/wiki/Trigonometric_interpolation>`_.\n\nA full description of the reconstruction, the technical derivation of the parameter-shift\nrules, and considerations for multivariate functions can be found in the paper\n`General parameter-shift rules for quantum gradients <https://arxiv.org/abs/2107.12390>`_\n[#GenPar]_.\nThe core idea to consider these quantum functions as Fourier series was first presented in\nthe preprint\n`Calculus on parameterized quantum circuits <https://arxiv.org/abs/1812.06323>`_ [#CalcPQC]_.\nWe will follow [#GenPar]_, but there also are two preprints discussing general parameter-shift\nrules: an algebraic approach in\n`Analytic gradients in variational quantum algorithms: Algebraic extensions of the parameter-shift rule to general unitary transformations <https://arxiv.org/abs/2107.08131>`_ [#AlgeShift]_\nand one focusing on special gates and spectral decompositions, namely\n`Generalized quantum circuit differentiation rules <https://arxiv.org/abs/2108.01218>`_\n[#GenDiffRules]_.\n\n|\n\n.. figure:: ../demonstrations/general_parshift/thumbnail_genpar.png\n    :align: center\n    :width: 50%\n    :target: javascript:void(0)\n\n    Function reconstruction and differentiation via parameter shifts.\n\n.. note ::\n\n    Before going through this tutorial, we recommend that readers refer to the\n    :doc:`Fourier series expressiveness tutorial </demos/tutorial_expressivity_fourier_series>`.\n    Additionally, having a basic understanding of the\n    :doc:`parameter-shift rule </glossary/parameter_shift>` might make this tutorial easier\n    to dive into.\n\n## Cost functions arising from quantum gates\nWe start our investigation by considering a cost function that arises from measuring the expectation\nvalue of an observable in a quantum state, created with a parametrized quantum operation\nthat depends on a single variational parameter $x$.\nThat is, the state may be prepared by any circuit, but we will only allow a single parameter\nin a single operation to enter the circuit.\nFor this we will use a handy gate structure that allows us to tune the complexity of the\noperation --- and thus of the cost function.\nMore concretely, we initialize a qubit register in a random state $|\\psi\\rangle$\nand apply a layer of Pauli-$Z$ rotations ``RZ`` to all qubits, where all rotations are parametrized by the *same* angle $x$.\nWe then measure the expectation value of a random Hermitian observable $B$ in the created\nstate, so that our cost function overall has the form\n\n.. math ::\n\n  E(x)=\\langle\\psi | U^\\dagger(x) B U(x)|\\psi\\rangle.\n\nHere, $U(x)$ consists of a layer of ``RZ`` gates,\n\n.. math ::\n\n  U(x)=\\prod_{a=1}^N R_Z^{(a)}(x) = \\prod_{a=1}^N \\exp\\left(-i\\frac{x}{2} Z_a\\right).\n\nLet's implement such a cost function using PennyLane.\nWe begin with functions that generate the random initial state $|\\psi\\rangle$\nand the random observable $B$ for a given number of qubits $N$ and a fixed seed:\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from scipy.stats import unitary_group\nimport numpy.random as rnd\n\n\ndef random_state(N, seed):\n    \"\"\"Create a random state on N qubits.\"\"\"\n    states = unitary_group.rvs(2 ** N, random_state=rnd.default_rng(seed))\n    return states[0]\n\n\ndef random_observable(N, seed):\n    \"\"\"Create a random observable on N qubits.\"\"\"\n    rnd.seed(seed)\n    # Generate real and imaginary part separately and (anti-)symmetrize them for Hermiticity\n    real_part, imag_part = rnd.random((2, 2 ** N, 2 ** N))\n    real_part += real_part.T\n    imag_part -= imag_part.T\n    return real_part + 1j * imag_part"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's set up a \"cost function generator\", namely a function that will create the\n``cost`` function we discussed above, using $|\\psi\\rangle$ as initial state and\nmeasuring the expectation value of $B$. This generator has the advantage that\nwe can quickly create the cost function for various numbers of qubits --- and therefore\ncost functions with different complexity.\n\nWe will use the default qubit simulator with its JAX backend and also will rely\non the NumPy implementation of JAX.\nTo obtain precise results, we enable 64-bit ``float`` precision via the JAX config.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nfrom jax import numpy as np\nimport pennylane as qml\n\ndef make_cost(N, seed):\n    \"\"\"Create a cost function on N qubits with N frequencies.\"\"\"\n    dev = qml.device(\"default.qubit\", wires=N)\n\n    @jax.jit\n    @qml.qnode(dev, interface=\"jax\")\n    def cost(x):\n        \"\"\"Cost function on N qubits with N frequencies.\"\"\"\n        qml.QubitStateVector(random_state(N, seed), wires=dev.wires)\n        for w in dev.wires:\n            qml.RZ(x, wires=w, id=\"x\")\n        return qml.expval(qml.Hermitian(random_observable(N, seed), wires=dev.wires))\n\n    return cost"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We also prepare some plotting functionalities and colors:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n\n# Set a plotting range on the x-axis\nxlim = (-np.pi, np.pi)\nX = np.linspace(*xlim, 60)\n# Colors\ngreen = \"#209494\"\norange = \"#ED7D31\"\nred = \"xkcd:brick red\"\nblue = \"xkcd:cerulean\"\npink = \"xkcd:bright pink\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we took care of these preparations, let's dive right into it:\nIt can be shown [#GenPar]_ that $E(x)$ takes the form of a\nFourier series in the variable $x$. That is to say that\n\n.. math ::\n\n  E(x) = a_0 + \\sum_{\\ell=1}^R a_{\\ell}\\cos(\\ell x)+b_{\\ell}\\sin(\\ell x).\n\nHere, $a_{\\ell}$ and $b_{\\ell}$ are the *Fourier coefficients*.\nIf you would like to understand this a bit better still, have a read of\n:mod:`~.pennylane.fourier` and remember to check out the\n:doc:`Fourier module tutorial </demos/tutorial_expressivity_fourier_series>`.\n\nDue to $B$ being Hermitian, $E(x)$ is a real-valued function, so \nonly positive frequencies and real coefficients appear in the Fourier series for $E(x)$.\nThis is true for any number of qubits (and therefore ``RZ`` gates) we use.\n\nUsing our function ``make_cost`` from above, we create the cost function for several\nnumbers of qubits and store both the function and its evaluations on the plotting range ``X``.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# Qubit numbers\nNs = [1, 2, 4, 5]\n# Fix a seed\nseed = 7658741\n\ncost_functions = []\nevaluated_cost = []\nfor N in Ns:\n    # Generate the cost function for N qubits and evaluate it\n    cost = make_cost(N, seed)\n    evaluated_cost.append([cost(x) for x in X])\n    cost_functions.append(cost)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's take a look at the created $E(x)$ for the various numbers of qubits:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# Figure with multiple axes\nfig, axs = plt.subplots(1, len(Ns), figsize=(12, 2))\n\nfor ax, N, E in zip(axs, Ns, evaluated_cost):\n    # Plot cost function evaluations\n    ax.plot(X, E, color=green)\n    # Axis and plot labels\n    ax.set_title(f\"{N} qubits\")\n    ax.set_xlabel(\"$x$\")\n\n_ = axs[0].set_ylabel(\"$E$\")\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["|\n\nIndeed we see that $E(x)$ is a periodic function whose complexity grows when increasing\n$N$ together with the number of ``RZ`` gates.\nTo take a look at the frequencies that are present in these functions, we may use\nPennyLane's :mod:`~.pennylane.fourier` module.\n\n.. note ::\n\n    The analysis tool :func:`~.pennylane.fourier.qnode_spectrum` makes use of the internal\n    structure of the :class:`~.pennylane.QNode` that encodes the cost function.\n    As we used the ``jax.jit`` decorator when defining the cost function above, we\n    here need to pass the wrapped function to ``qnode_spectrum``, which is stored in\n    ``cost_function.__wrapped__``.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from pennylane.fourier import qnode_spectrum\n\nspectra = []\nfor N, cost_function in zip(Ns, cost_functions):\n    # Compute spectrum with respect to parameter x\n    spec = qnode_spectrum(cost_function.__wrapped__)(X[0])[\"x\"][()]\n    print(f\"For {N} qubits the spectrum is {spec}.\")\n    # Store spectrum\n    spectra.append([freq for freq in spec if freq>0.0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The number of positive frequencies that appear in $E(x)$ is the same as the\nnumber of ``RZ`` gates we used in the circuit! Recall that we only need to consider\nthe positive frequencies because $E(x)$ is real-valued, and that we accounted for\nthe zero-frequency contribution in the coefficient $a_0$.\nIf you are interested why the number of gates coincides with the number of frequencies,\ncheck out the :doc:`Fourier module tutorial </demos/tutorial_expressivity_fourier_series>`.\n\nBefore moving on, let's also have a look at the Fourier coefficients in the functions\nwe created:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from pennylane.fourier.visualize import bar\n\nfig, axs = plt.subplots(2, len(Ns), figsize=(12, 4.5))\nfor i, (cost_function, spec) in enumerate(zip(cost_functions, spectra)):\n    # Compute the Fourier coefficients\n    coeffs = qml.fourier.coefficients(cost_function, 1, len(spec)+2)\n    # Show the Fourier coefficients\n    bar(coeffs, 1, axs[:, i], show_freqs=True, colour_dict={\"real\": green, \"imag\": orange})\n    axs[0, i].set_title(f\"{Ns[i]} qubits\")\n    # Set x-axis labels\n    axs[1, i].text(Ns[i] + 2, axs[1, i].get_ylim()[0], f\"Frequency\", ha=\"center\", va=\"top\")\n    # Clean up y-axis labels\n    if i == 0:\n        _ = [axs[j, i].set_ylabel(lab) for j, lab in enumerate([\"$a_\\ell/2$\", \"$b_\\ell/2$\"])]\n    else:\n        _ = [axs[j, i].set_ylabel(\"\") for j in [0, 1]]\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We find the real (imaginary) Fourier coefficients to be (anti-)symmetric.\nThis is expected because $E(x)$ is real-valued and we again see why it is enough\nto consider positive frequencies: the coefficients of the negative frequencies follow\nfrom those of the positive frequencies.\n\n## Determining the full dependence on $x$\n\nNext we will show how to determine the *full* dependence of the cost function on $x$,\ni.e., we will *reconstruct* $E(x)$.\nThe key idea is not new: Since $E(x)$ is periodic with known, integer frequencies, we can\nreconstruct it *exactly* by using trigonometric interpolation.\nFor this, we evaluate $E$ at shifted positions $x_\\mu$.\nWe will show the reconstruction both for *equidistant* and random shifts, corresponding to a\n`uniform <https://en.wikipedia.org/wiki/Discrete_Fourier_transform>`_ and a\n`non-uniform <https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform>`_\ndiscrete Fourier transform (DFT), respectively.\n\n### Equidistant shifts\n\nFor the equidistant case we can directly implement the trigonometric interpolation:\n\n.. math ::\n\n  x_\\mu &= \\frac{2\\mu\\pi}{2R+1}\\\\\n  E(x) &=\\sum_{\\mu=-R}^R E\\left(x_\\mu\\right) \\frac{\\sin\\left(\\frac{2R+1}{2}(x-x_\\mu)\\right)} {(2R+1)\\sin \\left(\\frac{1}{2} (x-x_\\mu)\\right)},\\\\\n\nwhere we reformulated $E$ in the second expression using the\n`sinc function <https://en.wikipedia.org/wiki/Sinc_function>`__ to enhance the numerical \nstability. Note that we have to take care of a rescaling factor of $\\pi$ between \nthis definition of $\\operatorname{sinc}$ and the NumPy implementation ``np.sinc``.\n\n.. note ::\n\n    When implementing $E$, we will replace\n\n    .. math ::\n\n        \\frac{\\sin\\left(\\frac{2R+1}{2}(x-x_\\mu)\\right)} {(2R+1)\\sin \\left(\\frac{1}{2} (x-x_\\mu)\\right)}\n\n    by \n\n    .. math ::\n\n        \\frac{\\operatorname{sinc}\\left(\\frac{2R+1}{2}(x-x_\\mu)\\right)} {\\operatorname{sinc} \\left(\\frac{1}{2} (x-x_\\mu)\\right)}\n\n    where the sinc function is defined as $\\operatorname{sinc}(x)=\\sin(x)/x$. \n    This enhances the numerical stability since $\\operatorname{sinc}(0)=1$, so that the\n    denominator does no longer vanish at the shifted points. \n    Note that we have to take care of a rescaling factor of $\\pi$\n    between this definition of $\\operatorname{sinc}$ and the NumPy implementation\n    ``np.sinc``.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["sinc = lambda x: np.sinc(x / np.pi)\n\ndef full_reconstruction_equ(fun, R):\n    \"\"\"Reconstruct a univariate function with up to R frequencies using equidistant shifts.\"\"\"\n    # Shift angles for the reconstruction\n    shifts = [2 * mu * np.pi / (2 * R + 1) for mu in range(-R, R + 1)]\n    # Shifted function evaluations\n    evals = np.array([fun(shift) for shift in shifts])\n\n    @jax.jit\n    def reconstruction(x):\n        \"\"\"Univariate reconstruction using equidistant shifts.\"\"\"\n        kernels = np.array(\n            [sinc((R + 0.5) * (x - shift)) / sinc(0.5 * (x - shift)) for shift in shifts]\n        )\n        return np.dot(evals, kernels)\n\n    return reconstruction\n\nreconstructions_equ = list(map(full_reconstruction_equ, cost_functions, Ns))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["So how is this reconstruction doing? We will plot it along with the original function\n$E$, mark the shifted evaluation points $x_\\mu$ (with crosses), and also show\nits deviation from $E(x)$ (lower plots).\nFor this, a function for the whole procedure of comparing the functions comes in handy, and\nwe will reuse it further below. For convenience, showing the deviation will be an optional\nfeature controled by the ``show_diff`` keyword argument.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def compare_functions(originals, reconstructions, Ns, shifts, show_diff=True):\n    \"\"\"Plot two sets of functions next to each other and show their difference (in pairs).\"\"\"\n    # Prepare the axes; we need fewer axes if we don't show the deviations\n    if show_diff:\n        fig, axs = plt.subplots(2, len(originals), figsize=(12, 4.5))\n    else:\n        fig, axs = plt.subplots(1, len(originals), figsize=(12, 2))\n    _axs = axs[0] if show_diff else axs\n\n    # Run over the functions and reconstructions\n    for i, (orig, recon, N, _shifts) in enumerate(zip(originals, reconstructions, Ns, shifts)):\n        # Evaluate the original function and its reconstruction over the plotting range\n        E = np.array(list(map(orig, X)))\n        E_rec = np.array(list(map(recon, X)))\n        # Evaluate the original function at the positions used in the reconstruction\n        E_shifts = np.array(list(map(orig, _shifts)))\n\n        # Show E, the reconstruction, and the shifts (top axes)\n        _axs[i].plot(X, E, lw=2, color=orange)\n        _axs[i].plot(X, E_rec, linestyle=\":\", lw=3, color=green)\n        _axs[i].plot(_shifts, E_shifts, ls=\"\", marker=\"x\", c=red)\n        # Manage plot titles and xticks\n        _axs[i].set_title(f\"{N} qubits\")\n\n        if show_diff:\n            # [Optional] Show the reconstruction deviation (bottom axes)\n            axs[1, i].plot(X, E - E_rec, color=blue)\n            axs[1, i].set_xlabel(\"$x$\")\n            # Hide the xticks of the top x-axes if we use the bottom axes\n            _axs[i].set_xticks([])\n\n    # Manage y-axis labels\n    _ = _axs[0].set_ylabel(\"$E$\")\n    if show_diff:\n        _ = axs[1, 0].set_ylabel(\"$E-E_{rec}$\")\n\n    return fig, axs\n\nequ_shifts = [[2 * mu * np.pi / (2 * N + 1) for mu in range(-N, N + 1)] for N in Ns]\nfig, axs = compare_functions(cost_functions, reconstructions_equ, Ns, equ_shifts)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["*It works!*\n\n### Non-equidistant shifts\n\nNow let's test the reconstruction with less regular sampling points on which to evaluate\n$E$. This means we can no longer use the closed-form expression from above, but switch\nto solving the set of equations\n\n.. math ::\n\n  E(x_\\mu) = a_0 + \\sum_{\\ell=1}^R a_{\\ell}\\cos(\\ell x_\\mu)+b_{\\ell}\\sin(\\ell x_\\mu)\n\nwith the---now irregular---sampling points $x_\\mu$.\nFor this, we set up the matrix\n\n.. math ::\n\n  C_{\\mu\\ell} = \\begin{cases}\n  1 &\\text{ if } \\ell=0\\\\\n  \\cos(\\ell x_\\mu) &\\text{ if } 1\\leq\\ell\\leq R\\\\\n  \\sin(\\ell x_\\mu) &\\text{ if } R<\\ell\\leq 2R,\n  \\end{cases}\n\ncollect the Fourier coefficients of $E$ into the vector\n$\\boldsymbol{W}=(a_0, \\boldsymbol{a}, \\boldsymbol{b})$, and the evaluations of $E$\ninto another vector called $\\boldsymbol{E}$ so that\n\n.. math ::\n\n  \\boldsymbol{E} = C \\boldsymbol{W} \\Rightarrow \\boldsymbol{W} = C^{-1}\\boldsymbol{E}.\n\nLet's implement this right away! We will take the function and the shifts $x_\\mu$ as\ninputs, inferring $R$ from the number of the provided shifts, which is $2R+1$.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def full_reconstruction_gen(fun, shifts):\n    \"\"\"Reconstruct a univariate trigonometric function using arbitrary shifts.\"\"\"\n    R = (len(shifts) - 1) // 2\n    frequencies = np.array(list(range(1, R + 1)))\n\n    # Construct the matrix C case by case\n    C1 = np.ones((2 * R + 1, 1))\n    C2 = np.cos(np.outer(shifts, frequencies))\n    C3 = np.sin(np.outer(shifts, frequencies))\n    C = np.hstack([C1, C2, C3])\n\n    # Evaluate the function to reconstruct at the shifted positions\n    evals = np.array(list(map(fun, shifts)))\n\n    # Solve the system of linear equations by inverting C\n    W = np.linalg.inv(C) @ evals\n\n    # Extract the Fourier coefficients\n    a0 = W[0]\n    a = W[1 : R + 1]\n    b = W[R + 1 :]\n\n    # Construct the Fourier series\n    @jax.jit\n    def reconstruction(x):\n        \"\"\"Univariate reconstruction based on arbitrary shifts.\"\"\"\n        return a0 + np.dot(a, np.cos(frequencies * x)) + np.dot(b, np.sin(frequencies * x))\n\n    return reconstruction"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To see this version of the reconstruction in action, we will sample the\nshifts $x_\\mu$ at random in $[-\\pi,\\pi)$:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["shifts = [rnd.random(2 * N + 1) * 2 * np.pi - np.pi for N in Ns]\nreconstructions_gen = list(map(full_reconstruction_gen, cost_functions, shifts))\nfig, axs = compare_functions(cost_functions, reconstructions_gen, Ns, shifts)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Again, we obtain a perfect reconstruction of $E(x)$ up to numerical errors.\nWe see that the deviation from the original cost function became larger than for equidistant\nshifts for some of the qubit numbers but it still remains much smaller than any energy scale of\nrelevance in applications.\nThe reason for these larger deviations is that some evaluation positions $x_\\mu$ were sampled\nvery close to each other, so that inverting the matrix $C$ becomes less stable numerically.\nConceptually, we see that the reconstruction does *not* rely on equidistant evaluations points.\n\n.. note ::\n\n    For some applications, the number of frequencies $R$ is not known exactly but an upper\n    bound for $R$ might be available. In this case, it is very useful that a reconstruction\n    that assumes *too many* frequencies in $E(x)$ works perfectly fine.\n    However, it has the disadvantage of spending too many evaluations on the reconstruction,\n    and the number of required measurements, which is meaningful for the (time)\n    complexity of quantum algorithms, does so as well!\n\n## Differentiation via reconstructions\n\nNext, we look at a modified reconstruction strategy that only obtains the odd or even part of\n$E(x)$. This can be done by slightly modifying the shifted positions at which we\nevaluate $E$ and the kernel functions.\n\nFrom a perspective of implementing the derivatives there are two approaches, differing in \nwhich parts we derive on paper and which we leave to the computer:\nIn the first approach, we perform a partial reconstruction using the evaluations of the\noriginal cost function $E$ on the quantum computer, as detailed below.\nThis gives us a function implemented in ``jax.numpy`` and we may afterwards apply \n``jax.grad`` to this function and obtain the derivative function. $E(0)$ then is only \none evaluation of this function away.\nIn the second approach, we compute the derivative of the partial reconstructions *manually* and \ndirectly implement the resulting shift rule that multiplies the quantum computer evaluations with\ncoefficients and sums them up. This means that the partial reconstruction is not performed at\nall by the classical computer, but only was used on paper to derive the formula for the \nderivative.\n\n*Why do we look at both approaches?*, you might ask. That is because neither of them is\nbetter than the other for *all* applications.\nThe first approach offers us derivatives of any order without additional manual work by\niteratively applying ``jax.grad``, which is very convenient. \nHowever, the automatic differentiation via JAX becomes increasingly expensive\nwith the order and we always reconstruct the *same* type of function, namely Fourier series,\nso that computing the respective derivatives once manually and coding up the resulting \ncoefficients of the parameter-shift rule pays off in the long run. This is the strength of the\nsecond approach.\nWe start with the first approach.\n\n### Automatically differentiated reconstructions\n\nWe implement the partial reconstruction method as a function; using PennyLane's\nautomatic differentiation backends, this then enables us to obtain the derivatives at the point\nof interest. For odd-order derivatives, we use the reconstruction of the odd part, for the\neven-order derivatives that of the even part.\n\nWe make use of modified `Dirichlet kernels <https://en.wikipedia.org/wiki/Dirichlet_kernel>`_\n$\\tilde{D}_\\mu(x)$ and equidistant shifts for this. For the odd reconstruction we have\n\n.. math ::\n\n  E_\\text{odd}(x) &= \\sum_{\\mu=1}^R E_\\text{odd}(x_\\mu) \\tilde{D}_\\mu(x)\\\\\n  \\tilde{D}_\\mu(x) &= \\frac{\\sin(R (x-x_\\mu))}{2R \\tan\\left(\\frac{1}{2} (x-x_\\mu)\\right)} - \\frac{\\sin(R (x+x_\\mu))}{2R \\tan\\left(\\frac{1}{2} (x+x_\\mu)\\right)},\n\nwhich we can implement using the reformulation\n\n.. math ::\n\n  \\frac{\\sin(X)}{\\tan(Y)}=\\frac{X}{Y}\\frac{\\operatorname{sinc}(X)}{\\operatorname{sinc}(Y)}\\cos(Y)\n\nfor the kernel.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["shifts_odd = lambda R: [(2 * mu - 1) * np.pi / (2 * R) for mu in range(1, R + 1)]\n# Odd linear combination of Dirichlet kernels\nD_odd = lambda x, R: np.array(\n    [\n        (\n            sinc(R * (x - shift)) / sinc(0.5 * (x - shift)) * np.cos(0.5 * (x - shift))\n            - sinc(R * (x + shift)) / sinc(0.5 * (x + shift)) * np.cos(0.5 * (x + shift))\n        )\n        for shift in shifts_odd(R)\n    ]\n)\n\n\ndef odd_reconstruction_equ(fun, R):\n    \"\"\"Reconstruct the odd part of an ``R``-frequency input function via equidistant shifts.\"\"\"\n    evaluations = np.array([(fun(shift) - fun(-shift)) / 2 for shift in shifts_odd(R)])\n\n    @jax.jit\n    def reconstruction(x):\n        \"\"\"Odd reconstruction based on equidistant shifts.\"\"\"\n        return np.dot(evaluations, D_odd(x, R))\n\n    return reconstruction\n\n\nodd_reconstructions = list(map(odd_reconstruction_equ, cost_functions, Ns))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The even part on the other hand takes the form\n\n.. math ::\n\n  E_\\text{even}(x) &= \\sum_{\\mu=0}^R E_\\text{even}(x_\\mu) \\hat{D}_\\mu(x)\\\\\n  \\hat{D}_\\mu(x) &=\n  \\begin{cases}\n     \\frac{\\sin(Rx)}{2R \\tan(x/2)} &\\text{if } \\mu = 0 \\\\[12pt]\n     \\frac{\\sin(R (x-x_\\mu))}{2R \\tan\\left(\\frac{1}{2} (x-x_\\mu)\\right)} + \\frac{\\sin(R (x+x_\\mu))}{2R \\tan\\left(\\frac{1}{2} (x+x_\\mu)\\right)} & \\text{if } \\mu \\in [R-1] \\\\[12pt]\n     \\frac{\\sin(R (x-\\pi))}{2R \\tan\\left(\\frac{1}{2} (x-\\pi)\\right)} & \\text{if } \\mu = R.\n  \\end{cases}\n\nNote that not only the kernels $\\hat{D}_\\mu(x)$ but also the shifted positions\n$\\{x_\\mu\\}$ differ between the odd and even case.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["shifts_even = lambda R: [mu * np.pi / R for mu in range(1, R)]\n# Even linear combination of Dirichlet kernels\nD_even = lambda x, R: np.array(\n    [\n        (\n            sinc(R * (x - shift)) / sinc(0.5 * (x - shift)) * np.cos(0.5 * (x - shift))\n            + sinc(R * (x + shift)) / sinc(0.5 * (x + shift)) * np.cos(0.5 * (x + shift))\n        )\n        for shift in shifts_even(R)\n    ]\n)\n# Special cases of even kernels\nD0 = lambda x, R: sinc(R * x) / (sinc(x / 2)) * np.cos(x / 2)\nDpi = lambda x, R: sinc(R * (x - np.pi)) / sinc((x - np.pi) / 2) * np.cos((x - np.pi) / 2)\n\n\ndef even_reconstruction_equ(fun, R):\n    \"\"\"Reconstruct the even part of ``R``-frequency input function via equidistant shifts.\"\"\"\n    _evaluations = np.array([(fun(shift) + fun(-shift)) / 2 for shift in shifts_even(R)])\n    evaluations = np.array([fun(0), *_evaluations, fun(np.pi)])\n    kernels = lambda x: np.array([D0(x, R), *D_even(x, R), Dpi(x, R)])\n\n    @jax.jit\n    def reconstruction(x):\n        \"\"\"Even reconstruction based on equidistant shifts.\"\"\"\n        return np.dot(evaluations, kernels(x))\n\n    return reconstruction\n\n\neven_reconstructions = list(map(even_reconstruction_equ, cost_functions, Ns))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We also set up a function that performs both partial reconstructions and sums the resulting\nfunctions to the full Fourier series.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def summed_reconstruction_equ(fun, R):\n    \"\"\"Sum an odd and an even reconstruction into the full function.\"\"\"\n    _odd_part = odd_reconstruction_equ(fun, R)\n    _even_part = even_reconstruction_equ(fun, R)\n\n    def reconstruction(x):\n        \"\"\"Full function based on separate odd/even reconstructions.\"\"\"\n        return _odd_part(x) + _even_part(x)\n\n    return reconstruction\n\n\nsummed_reconstructions = list(map(summed_reconstruction_equ, cost_functions, Ns))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We show these even (blue) and odd (red) reconstructions and how they indeed\nsum to the full function (orange, dashed).\nWe will again use the ``compare_functions`` utility from above for the comparison.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from matplotlib.lines import Line2D\n\n# Obtain the shifts for the reconstruction of both parts\nodd_and_even_shifts = [\n    (\n        shifts_odd(R)\n        + shifts_even(R)\n        + list(-1 * np.array(shifts_odd(R)))\n        + list(-1 * np.array(shifts_odd(R)))\n        + [0, np.pi]\n    )\n    for R in Ns\n]\n\n# Show the reconstructed parts and the sums\nfig, axs = compare_functions(cost_functions, summed_reconstructions, Ns, odd_and_even_shifts)\nfor i, (odd_recon, even_recon) in enumerate(zip(odd_reconstructions, even_reconstructions)):\n    # Odd part\n    E_odd = np.array(list(map(odd_recon, X)))\n    axs[0, i].plot(X, E_odd, color=red)\n    # Even part\n    E_even = np.array(list(map(even_recon, X)))\n    axs[0, i].plot(X, E_even, color=blue)\n    axs[0, i].set_title('')\n_ = axs[1, 0].set_ylabel(\"$E-(E_{odd}+E_{even})$\")\ncolors = [green, red, blue, orange]\nstyles = ['-', '-', '-', '--']\nhandles = [Line2D([0], [0], color=c, ls=ls, lw=1.2) for c, ls in zip(colors, styles)]\nlabels = ['Original', 'Odd reconstruction', 'Even reconstruction', 'Summed reconstruction']\n_ = fig.legend(handles, labels, bbox_to_anchor=(0.2, 0.89), loc='lower left', ncol=4)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Great! The even and odd part indeed sum to the correct function again. But what did we\ngain? \n\nNothing, actually, for the full reconstruction! Quite the opposite, we spent $2R$\nevaluations of $E$ on each part, that is $4R$ evaluations overall to obtain a\ndescription of the full function $E$! This is way more than the $2R+1$\nevaluations needed for the full reconstructions from the beginning.\n\nHowever, remember that we set out to compute derivatives of $E$ at $0$, so that\nfor derivatives of odd/even order only the odd/even reconstruction is required.\nUsing an autodifferentiation framework, e.g. JAX, we can easily compute such higher-order\nderivatives:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# An iterative function computing the ``order``th derivative of a function ``f`` with JAX\ngrad_gen = lambda f, order: grad_gen(jax.grad(f), order - 1) if order > 0 else f\n\n# Compute the first, second, and fourth derivative\nfor order, name in zip([1, 2, 4], [\"First\", \"Second\", \"4th\"]):\n    recons = odd_reconstructions if order % 2 else even_reconstructions\n    recon_name = \"odd \" if order % 2 else \"even\"\n    cost_grads = np.array([grad_gen(orig, order)(0.0) for orig in cost_functions])\n    recon_grads = np.array([grad_gen(recon, order)(0.0) for recon in recons])\n    all_equal = (\n        \"All entries match\" if np.allclose(cost_grads, recon_grads) else \"Some entries differ!\"\n    )\n    print(f\"{name} derivatives via jax: {all_equal}\")\n    print(\"From the cost functions:       \", np.round(np.array(cost_grads), 6))\n    print(f\"From the {recon_name} reconstructions: \", np.round(np.array(recon_grads), 6), \"\\n\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The derivatives coincide.\n\n.. note ::\n\n    While we used the $2R+1$ evaluations $x_\\mu=\\frac{2\\mu\\pi}{2R+1}$ for the full\n    reconstruction, derivatives only require $2R$ calls to the respective circuit.\n    Also note that the derivatives can be computed at any position $x_0$ other than\n    $0$ by simply reconstructing the function $E(x+x_0)$, which again will be\n    a Fourier series like $E(x)$.\n\n### Generalized parameter-shift rules\nThe second method is based on the previous one. Instead of consulting JAX, we may compute\nthe wanted derivative of the odd/even kernel function manually and thus derive general\nparameter-shift rules from this. We will leave the technical derivation of these rules\nto the paper [#GenPar]_. Start with the first derivative, which certainly is used the most:\n\n.. math ::\n\n  E'(0) = \\sum_{\\mu=1}^{2R} E\\left(\\frac{2\\mu-1}{2R}\\pi\\right) \\frac{(-1)^{\\mu-1}}{4R\\sin^2\\left(\\frac{2\\mu-1}{4R}\\pi\\right)},\n\nThis is straight-forward to implement by defining the coefficients and evaluating\n$E$ at the shifted positions $x_\\mu$:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def parameter_shift_first(fun, R):\n    \"\"\"Compute the first-order derivative of a function with R frequencies at 0.\"\"\"\n    shifts = (2 * np.arange(1, 2 * R + 1) - 1) * np.pi / (4 * R)\n    # Classically computed coefficients\n    coeffs = np.array(\n        [(-1) ** mu / (4 * R * np.sin(shift) ** 2) for mu, shift in enumerate(shifts)]\n    )\n    # Evaluations of the cost function E(x_mu)\n    evaluations = np.array(list(map(fun, 2 * shifts)))\n    # Contract coefficients with evaluations\n    return np.dot(coeffs, evaluations)\n\nps_der1 = list(map(parameter_shift_first, cost_functions, Ns))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The second-order derivative takes a similar form, but we have to take care of the evaluation at\n$0$ and the corresponding coefficient separately:\n\n.. math ::\n\n  E''(0) = -E(0)\\frac{2R^2+1}{6} - \\sum_{\\mu=1}^{2R-1} E\\left(\\frac{\\mu\\pi}{R}\\right)\\frac{(-1)^\\mu}{2\\sin^2 \\left(\\frac{\\mu\\pi}{2R}\\right)}.\n\nLet's code this up, again we only get slight complications from the special evaluation\nat $0$:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def parameter_shift_second(fun, R):\n    \"\"\"Compute the second-order derivative of a function with R frequencies at 0.\"\"\"\n    shifts = np.arange(1, 2 * R) * np.pi / (2 * R)\n    # Classically computed coefficients for the main sum\n    _coeffs = [(-1) ** mu / (2 * np.sin(shift) ** 2) for mu, shift in enumerate(shifts)]\n    # Include the coefficients for the \"special\" term E(0).\n    coeffs = np.array([-(2 * R ** 2 + 1) / 6] + _coeffs)\n    # Evaluate at the regularily shifted positions\n    _evaluations = list(map(fun, 2 * shifts))\n    # Include the \"special\" term E(0).\n    evaluations = np.array([fun(0.0)] + _evaluations)\n    # Contract coefficients with evaluations.\n    return np.dot(coeffs, evaluations)\n\nps_der2 = list(map(parameter_shift_second, cost_functions, Ns))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We will compare these two shift rules to the finite-difference derivative commonly used for\nnumerical differentiation. We choose a finite difference of $d_x=5\\times 10^{-5}$.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dx = 5e-5\n\ndef finite_diff_first(fun):\n    \"\"\"Compute the first order finite difference derivative.\"\"\"\n    return (fun(dx/2) - fun(-dx/2))/dx\n\nfd_der1 = list(map(finite_diff_first, cost_functions))\n\ndef finite_diff_second(fun):\n    \"\"\"Compute the second order finite difference derivative.\"\"\"\n    fun_p, fun_0, fun_m = fun(dx), fun(0.0), fun(-dx)\n    return ((fun_p - fun_0)/dx - (fun_0 - fun_m)/dx) /dx\n\nfd_der2 = list(map(finite_diff_second, cost_functions))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["All that is left is to compare the computed parameter-shift and finite-difference\nderivatives:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["print(\"Number of qubits/RZ gates:         \", *Ns, sep=\" \" * 9)\nprint(f\"First-order parameter-shift rule:  {np.round(np.array(ps_der1), 6)}\")\nprint(f\"First-order finite difference:     {np.round(np.array(fd_der1), 6)}\")\nprint(f\"Second-order parameter-shift rule: {np.round(np.array(ps_der2), 6)}\")\nprint(f\"Second-order finite difference:    {np.round(np.array(fd_der2), 6)}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The parameter-shift rules work as expected! And we were able to save\na circuit evaluation as compared to a full reconstruction.\n\nAnd this is all we want to show here about univariate function reconstructions and generalized\nparameter shift rules.\nNote that the techniques above can partially be extended to frequencies that are not\ninteger-valued, but many closed form expressions are no longer valid.\nFor the reconstruction, the approach via Dirichlet kernels no longer works in the general\ncase; instead, a system of equations has to be solved, but with generalized\nfrequencies $\\{\\Omega_\\ell\\}$ instead of $\\{\\ell\\}$ (see e.g. \nSections III A-C in [#GenPar]_)\n\n\n## References\n\n.. [#GenPar]\n\n    David Wierichs, Josh Izaac, Cody Wang, Cedric Yen-Yu Lin.\n    \"General parameter-shift rules for quantum gradients\".\n    `arXiv preprint arXiv:2107.12390 <https://arxiv.org/abs/2107.12390>`__.\n\n.. [#CalcPQC]\n\n    Javier Gil Vidal, Dirk Oliver Theis. \"Calculus on parameterized quantum circuits\".\n    `arXiv preprint arXiv:1812.06323 <https://arxiv.org/abs/1812.06323>`__.\n\n.. [#Rotosolve]\n\n    Mateusz Ostaszewski, Edward Grant, Marcello Benedetti.\n    \"Structure optimization for parameterized quantum circuits\".\n    `arXiv preprint arXiv:1905.09692 <https://arxiv.org/abs/1905.09692>`__.\n\n.. [#AlgeShift]\n\n    Artur F. Izmaylov, Robert A. Lang, Tzu-Ching Yen.\n    \"Analytic gradients in variational quantum algorithms: Algebraic extensions of the parameter-shift rule to general unitary transformations\".\n    `arXiv preprint arXiv:2107.08131 <https://arxiv.org/abs/2107.08131>`__.\n\n.. [#GenDiffRules]\n\n    Oleksandr Kyriienko, Vincent E. Elfving.\n    \"Generalized quantum circuit differentiation rules\".\n    `arXiv preprint arXiv:2108.01218 <https://arxiv.org/abs/2108.01218>`__.\n\n.. |brute| replace:: ``brute``\n\n.. |shgo| replace:: ``shgo``\n\n.. |Rotosolve_code| replace:: ``qml.RotosolveOptimizer``\n\n"]}], "metadata": {"kernelspec": {"display_name": "PennyLane", "language": "python", "name": "pennylane"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.10"}}, "nbformat": 4, "nbformat_minor": 0}