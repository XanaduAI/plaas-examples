{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Optimization using SPSA \n=======================\n\n\n\n\n\n*Author: PennyLane dev team. Posted: 19 Mar 2021. Last updated: 8 Apr\n2021.*\n\nIn this tutorial, we investigate using a gradient-free optimizer called\nthe Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm\nto optimize quantum circuits. SPSA is a technique that involves\napproximating the gradient of a quantum circuit without having to\ncompute it.\n\nThis demonstration shows how the SPSA optimizer performs:\n\n1.  A simple task on a sampling device,\n2.  The variational quantum eigensolver on a simulated hardware device.\n\nThroughout the demo, we show results obtained with SPSA and with\ngradient descent and also compare the number of device executions\nrequired to complete each optimization.\n\nBackground\n----------\n\nIn PennyLane, quantum gradients on hardware are commonly computed using\n[parameter-shift\nrules](https://pennylane.ai/qml/glossary/parameter_shift.html).\nComputing quantum gradients involves evaluating the partial derivative\nof the quantum function with respect to every free parameter. The\npartial derivatives are used to apply the product rule to compute the\ngradient of the quantum circuit. For qubit operations that are generated\nby one of the Pauli matrices, each partial derivative computation will\ninvolve two quantum circuit evaluations with a positive and a negative\nshift in the parameter values.\n\nAs there are two circuit evaluations for each free parameter, the number\nof overall quantum circuit executions for computing a quantum gradient\nis $O(p)$ as it scales linearly with the number of free parameters $p$.\nThis scaling can be very costly for optimization tasks with many free\nparameters. For the overall optimization this scaling means we need\n$O(pn)$ quantum circuit evaluations, where $n$ is the number of\noptimization steps taken.\n\nFortunately, there are certain optimization techniques that offer an\nalternative to computing the gradients of quantum circuits. One such\ntechnique is called the Simultaneous Perturbation Stochastic\nApproximation (SPSA) algorithm. SPSA is an optimization method that\ninvolves *approximating* the gradient of the cost function at each\niteration step. This technique requires only two quantum circuit\nexecutions per iteration step, regardless of the number of free\nparameters. Therefore the overall number of circuit executions would be\n$O(n')$ where $n'$ is the number of optimization steps taken when using\nSPSA. This technique is also considered robust against noise, making it\na great optimization method in the NISQ era.\n\nIn this demo, you\\'ll learn how the SPSA algorithm works, and how to\napply it in PennyLane to compute gradients of quantum circuits. You\\'ll\nalso see it in action using noisy quantum data!\n\nSimultaneous perturbation stochastic approximation (SPSA)\n---------------------------------------------------------\n\nSPSA is a general method for minimizing differentiable multivariate\nfunctions. It is particularly useful for functions for which evaluating\nthe gradient is not possible, or too resource intensive. SPSA provides a\nstochastic method for approximating the gradient of a multivariate\ndifferentiable cost function. To accomplish this the cost function is\nevaluated twice using perturbed parameter vectors: every component of\nthe original parameter vector is simultaneously shifted with a randomly\ngenerated value. This is in contrast to finite-differences methods where\nfor each evaluation only one component of the parameter vector is\nshifted at a time.\n\nSimilar to gradient-based approaches such as gradient descent, SPSA is\nan iterative optimization algorithm. Let\\'s consider a differentiable\ncost function $L(\\theta)$ where $\\theta$ is a $p$-dimensional vector and\nwhere the optimization problem can be translated into finding a\n$\\theta^*$ at which $\\frac{\\partial L}{\\partial \\theta} = 0$. It is\nassumed that measurements of $L(\\theta)$ are available at various values\nof $\\theta$\\-\\--this is exactly the problem that we\\'d consider when\noptimizing quantum functions!\n\nJust like with gradient-based methods, SPSA starts with an initial\nparameter vector $\\hat{\\theta}_{0}$. After $k$ iterations, the $(k+1)$\nth parameter iterates can be obtained as\n\n$$\\hat{\\theta}_{k+1} = \\hat{\\theta}_{k} - a_{k}\\hat{g}_{k}(\\hat{\\theta}_{k}),$$\n\nwhere $\\hat{g}_{k}$ is the estimate of the gradient $g(\\theta) =\n\\frac{ \\partial L}{\\partial \\theta}$ at the iterate $\\hat{\\theta}_{k}$\nbased on prior measurements of the cost function, and $a_{k}$ is a\npositive number.\n\nOne of the advantages of SPSA is that it is robust to any noise that may\noccur when measuring the function $L$. Therefore, let\\'s consider the\nfunction $y(\\theta)=L(\\theta) + \\varepsilon$, where $\\varepsilon$ is\nsome perturbation of the output. In SPSA, the estimated gradient at each\niteration step is expressed as\n\n$$\\hat{g}_{ki} (\\hat{\\theta}_{k}) = \\frac{y(\\hat{\\theta}_{k} +c_{k}\\Delta_{k})\n- y(\\hat{\\theta}_{k} -c_{k}\\Delta_{k})}{2c_{k}\\Delta_{ki}},$$\n\nwhere $c_{k}$ is a positive number and $\\Delta_{k} = (\\Delta_{k_1},\n\\Delta_{k_2}, ..., \\Delta_{k_p})^{T}$ is a perturbation vector. The\nstochasticity of the technique comes from the fact that for each\niteration step $k$ the components of the $\\Delta_{k}$ perturbation\nvector are randomly generated using a zero-mean distribution. In most\ncases, the Bernoulli distribution is used, meaning each parameter is\nsimultaneously perturbed by either $\\pm c_k$.\n\nIt is this perturbation that makes SPSA robust to noise \\-\\-- since\nevery parameter is already being shifted, additional shifts due to noise\nare less likely to hinder the optimization process. In a sense, noise\ngets \\\"absorbed\\\" into the already-stochastic process. This is\nhighlighted in the figure below, which portrays an example of the type\nof path SPSA takes through the space of the function, compared to a\nstandard gradient-based optimizer.\n\n![..](https://pennylane.ai/qml/_images/spsa_mntn.png)\n\nA schematic of the search paths used by gradient descent with\nparameter-shift and SPSA.\n\nNow that we have explored how SPSA works, let\\'s see how it performs in\npractice!\n\nOptimization on a sampling device\n---------------------------------\n\n> **Important**\n> \n> To run this demo locally, you\\'ll need to install the\n> [noisyopt](https://github.com/andim/noisyopt) library. This library\n> contains a straightforward implementation of SPSA that can be used in\n> the same way as the optimizers available in [SciPy\\'s minimize\n> method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html).\n\n\n\nFirst, let\\'s consider a simple quantum circuit on a sampling device.\nFor this, we\\'ll be using a device from the [PennyLane-Qiskit\nplugin](https://pennylaneqiskit.readthedocs.io/en/latest/) that samples\nquantum circuits to get measurement outcomes and later post-processes\nthese outcomes to compute statistics like expectation values.\n\n> **Note**\n> \n> Just as with other PennyLane devices, the number of samples taken for a\n> device execution can be specified using the `shots` keyword argument of\n> the device.\n\n\n\nOnce we have a device selected, we just need a couple of other\ningredients for the pieces of an example optimization to come together:\n\n-   a circuit ansatz:\n    `StronglyEntanglingLayers()`,\n-   initial parameters: the correct shape can be computed by\n    `shape()`,\n-   an observable: $\\bigotimes_{i=0}^{N-1}\\sigma_z^i$, where $N$ stands\n    for the number of qubits.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import pennylane as qml\nfrom pennylane import numpy as np\n\nnum_wires = 4\nnum_layers = 5\n\ndev_sampler_spsa = qml.device(\"qiskit.aer\", wires=num_wires, shots=1000)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We seed so that we can simulate the same circuit every time.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["np.random.seed(50)\n\nall_pauliz_tensor_prod = qml.operation.Tensor(*[qml.PauliZ(i) for i in range(num_wires)])\n\n\ndef circuit(params):\n    qml.StronglyEntanglingLayers(params, wires=list(range(num_wires)))\n    return qml.expval(all_pauliz_tensor_prod)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["After this, we\\'ll initialize the parameters in a way that is compatible\nwith the `noisyopt` package. The `noisyopt` package requires the\ntrainable parameters be a flattened array. As a result, our cost\nfunction must accept a flat array of parameters to be optimized.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["flat_shape = num_layers * num_wires * 3\nparam_shape = qml.templates.StronglyEntanglingLayers.shape(n_wires=num_wires, n_layers=num_layers)\ninit_params = np.random.normal(scale=0.1, size=param_shape, requires_grad=True)\n\ninit_params_spsa = init_params.reshape(flat_shape)\n\nqnode_spsa = qml.QNode(circuit, dev_sampler_spsa)\n\n\ndef cost_spsa(params):\n    return qnode_spsa(params.reshape(num_layers, num_wires, 3))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Once we have defined each piece of the optimization, there\\'s only one\nremaining component required: the *SPSA optimizer*. We\\'ll use the SPSA\noptimizer provided by the `noisyopt` package. Once imported, we can\ninitialize parts of the optimization such as the number of iterations, a\ncollection to store the cost values, and a callback function. Once the\noptimization has concluded, we save the number of device executions\nrequired for completion using the callback function. This will be an\ninteresting quantity!\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from noisyopt import minimizeSPSA\n\nniter_spsa = 200\n\n# Evaluate the initial cost\ncost_store_spsa = [cost_spsa(init_params_spsa)]\ndevice_execs_spsa = [0]\n\n\ndef callback_fn(xk):\n    cost_val = cost_spsa(xk)\n    cost_store_spsa.append(cost_val)\n\n    # We've evaluated the cost function, let's make up for that\n    num_executions = int(dev_sampler_spsa.num_executions / 2)\n    device_execs_spsa.append(num_executions)\n\n    iteration_num = len(cost_store_spsa)\n    if iteration_num % 10 == 0:\n        print(\n            f\"Iteration = {iteration_num}, \"\n            f\"Number of device executions = {num_executions}, \"\n            f\"Cost = {cost_val}\"\n        )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Choosing the hyperparameters\n============================\n\nThe `noisyopt` package allows us to choose the initial value of two\nhyperparameters for SPSA: the $c$ and $a$ coefficients. Recall from\nabove that the $c$ values control the amount of random shift when\nevaluating the cost function, while the $a$ coefficients are analogous\nto a learning rate and affect the degree to which the parameters change\nat each update step.\n\nWith stochastic approximation, specifying such hyperparameters\nsignificantly influences the convergence of the optimization for a given\nproblem. Although there is no universal recipe for selecting these\nvalues (as they depend strongly on the specific problem), includes\nguidelines for the selection. In our case, the initial values for $c$\nand $a$ were selected as a result of a grid search to ensure a fast\nconvergence. We further note that apart from $c$ and $a$, there are\nfurther coefficients that are initialized in the `noisyopt` package\nusing the previously mentioned guidelines.\n\nOur cost function does not take a seed as a keyword argument (which\nwould be the default behaviour for `minimizeSPSA`), so we set\n`paired=False`.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["res = minimizeSPSA(\n    cost_spsa,\n    x0=init_params_spsa.copy(),\n    niter=niter_spsa,\n    paired=False,\n    c=0.15,\n    a=0.2,\n    callback=callback_fn,\n)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Out:\n\n``` {.none}\nIteration = 10, Number of device executions = 14, Cost = 0.09\nIteration = 20, Number of device executions = 29, Cost = -0.638\nIteration = 30, Number of device executions = 44, Cost = -0.842\nIteration = 40, Number of device executions = 59, Cost = -0.926\nIteration = 50, Number of device executions = 74, Cost = -0.938\nIteration = 60, Number of device executions = 89, Cost = -0.94\nIteration = 70, Number of device executions = 104, Cost = -0.962\nIteration = 80, Number of device executions = 119, Cost = -0.938\nIteration = 90, Number of device executions = 134, Cost = -0.946\nIteration = 100, Number of device executions = 149, Cost = -0.966\nIteration = 110, Number of device executions = 164, Cost = -0.954\nIteration = 120, Number of device executions = 179, Cost = -0.964\nIteration = 130, Number of device executions = 194, Cost = -0.952\nIteration = 140, Number of device executions = 209, Cost = -0.958\nIteration = 150, Number of device executions = 224, Cost = -0.968\nIteration = 160, Number of device executions = 239, Cost = -0.948\nIteration = 170, Number of device executions = 254, Cost = -0.974\nIteration = 180, Number of device executions = 269, Cost = -0.962\nIteration = 190, Number of device executions = 284, Cost = -0.988\nIteration = 200, Number of device executions = 299, Cost = -0.964\n```\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let\\'s perform the same optimization using gradient descent. We set\nthe step size according to a favourable value found after grid search\nfor fast convergence. Note that we also create a new device in order to\nreset the execution count to 0.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["opt = qml.GradientDescentOptimizer(stepsize=0.3)\n\n# Create a device, qnode and cost function specific to gradient descent\ndev_sampler_gd = qml.device(\"qiskit.aer\", wires=num_wires, shots=1000)\nqnode_gd = qml.QNode(circuit, dev_sampler_gd)\n\n\ndef cost_gd(params):\n    return qnode_gd(params)\n\n\nsteps = 20\nparams = init_params.copy()\n\ndevice_execs_grad = [0]\ncost_store_grad = []\n\nfor k in range(steps):\n    params, val = opt.step_and_cost(cost_gd, params)\n    device_execs_grad.append(dev_sampler_gd.num_executions)\n    cost_store_grad.append(val)\n    print(\n        f\"Iteration = {k}, \"\n        f\"Number of device executions = {dev_sampler_gd.num_executions}, \"\n        f\"Cost = {val}\"\n    )\n\n# The step_and_cost function gives us the cost at the previous step, so to find\n# the cost at the final parameter values we have to compute it manually\ncost_store_grad.append(cost_gd(params))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Out:\n\n``` {.none}\nIteration = 0, Number of device executions = 121, Cost = 0.904\nIteration = 1, Number of device executions = 242, Cost = 0.758\nIteration = 2, Number of device executions = 363, Cost = 0.284\nIteration = 3, Number of device executions = 484, Cost = -0.416\nIteration = 4, Number of device executions = 605, Cost = -0.836\nIteration = 5, Number of device executions = 726, Cost = -0.964\nIteration = 6, Number of device executions = 847, Cost = -0.992\nIteration = 7, Number of device executions = 968, Cost = -0.994\nIteration = 8, Number of device executions = 1089, Cost = -0.992\nIteration = 9, Number of device executions = 1210, Cost = -0.994\nIteration = 10, Number of device executions = 1331, Cost = -0.998\nIteration = 11, Number of device executions = 1452, Cost = -0.992\nIteration = 12, Number of device executions = 1573, Cost = -0.994\nIteration = 13, Number of device executions = 1694, Cost = -1.0\nIteration = 14, Number of device executions = 1815, Cost = -0.996\nIteration = 15, Number of device executions = 1936, Cost = -0.996\nIteration = 16, Number of device executions = 2057, Cost = -0.998\nIteration = 17, Number of device executions = 2178, Cost = -0.996\nIteration = 18, Number of device executions = 2299, Cost = -0.996\nIteration = 19, Number of device executions = 2420, Cost = -0.996\n```\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["SPSA and gradient descent comparison\n====================================\n\nAt this point, nothing else remains but to check which of these\napproaches did better!\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\n\nplt.plot(device_execs_grad, cost_store_grad, label=\"Gradient descent\")\nplt.plot(device_execs_spsa, cost_store_spsa, label=\"SPSA\")\n\nplt.xlabel(\"Number of device executions\", fontsize=14)\nplt.ylabel(\"Cost function value\", fontsize=14)\nplt.grid()\n\nplt.title(\"Gradient descent vs. SPSA for simple optimization\", fontsize=16)\nplt.legend(fontsize=14)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![](https://pennylane.ai/qml/_images/first_comparison.png)\n\nIt seems that SPSA performs great and it does so with a significant\nsavings when compared to gradient descent!\n\nLet\\'s take a deeper dive to see how much better it actually is.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["grad_desc_exec_min = device_execs_grad[np.argmin(cost_store_grad)]\nspsa_exec_min = device_execs_spsa[np.argmin(cost_store_spsa)]\nprint(f\"Device execution ratio: {grad_desc_exec_min/spsa_exec_min}.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Out:\n\n``` {.none}\nDevice execution ratio: 4.161375661375661\n```\n\nThis means that SPSA can potentially find the minimum of a cost function\nby using over 4 times fewer device executions than gradient descent!\nThat\\'s a huge saving, especially in cases such as running on actual\nquantum hardware.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["SPSA and the variational quantum eigensolver\n============================================\n\nNow that we\\'ve explored the theoretical underpinnings of SPSA, let\\'s\nuse it to optimize a real chemical system, that of the hydrogen molecule\n$H_2$. This molecule was studied previously in the [introductory\nvariational quantum eigensolver (VQE) demo](/demos/tutorial_vqe), and so\nwe will reuse some of that machinery below to set up the problem.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from pennylane import qchem\n\nsymbols = [\"H\", \"H\"]\ncoordinates = np.array([0.0, 0.0, -0.6614, 0.0, 0.0, 0.6614])\nh2_ham, num_qubits = qchem.molecular_hamiltonian(symbols, coordinates)\n\n# Variational ansatz for H_2 - see Intro VQE demo for more details\ndef circuit(params, wires):\n    qml.BasisState(np.array([1, 1, 0, 0]), wires=wires)\n    for i in wires:\n        qml.Rot(*params[i], wires=i)\n    qml.CNOT(wires=[2, 3])\n    qml.CNOT(wires=[2, 0])\n    qml.CNOT(wires=[3, 1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The $H_2$ Hamiltonian uses 4 qubits, contains 15 terms, and has a ground\nstate energy of $-1.136189454088$ Hartree.\n\nSince SPSA is robust to noise, let\\'s see how it fares compared to\ngradient descent when run on noisy hardware. For this, we will set up\nand use a simulated version of IBM Q\\'s Melbourne hardware.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from qiskit import IBMQ\nfrom qiskit.providers.aer import noise\n\n# Note: you will need to be authenticated to IBMQ to run the following code.\n# Do not run the simulation on this device, as it will send it to real hardware\n# For access to IBMQ, the following statements will be useful:\n# IBMQ.save_account(TOKEN)\n# IBMQ.load_account() # Load account from disk\n# List the providers to pick an available backend:\n# IBMQ.providers()    # List all available providers\n\ndev_melbourne = qml.device(\n    \"qiskit.ibmq\", wires=num_qubits, backend=\"ibmq_16_melbourne\"\n)\nnoise_model = noise.NoiseModel.from_backend(dev_melbourne.backend.properties())\ndev_noisy = qml.device(\n    \"qiskit.aer\", wires=dev_melbourne.num_wires, shots=1000, noise_model=noise_model\n)\n\ndef exp_val_circuit(params):\n    circuit(params, range(dev_melbourne.num_wires))\n    return qml.expval(h2_ham)\n\n# Initialize the optimizer - optimal step size was found through a grid search\nopt = qml.GradientDescentOptimizer(stepsize=2.2)\ncost = qml.QNode(exp_val_circuit, dev_noisy)\n\n# This random seed was used in the original VQE demo and is known to allow the\n# algorithm to converge to the global minimum.\nnp.random.seed(0)\ninit_params = np.random.normal(0, np.pi, (num_qubits, 3), requires_grad=True)\nparams = init_params.copy()\n\nh2_grad_device_executions_melbourne = [0]\nh2_grad_energies_melbourne = []\n\nmax_iterations = 20\n\n# Run the gradient descent algorithm\nfor n in range(max_iterations):\n    params, energy = opt.step_and_cost(cost, params)\n    h2_grad_device_executions_melbourne.append(dev_noisy.num_executions)\n    h2_grad_energies_melbourne.append(energy)\n\n    if n % 5 == 0:\n        print(\n            f\"Iteration = {n}, \"\n            f\"Number of device executions = {dev_noisy.num_executions},  \"\n            f\"Energy = {energy:.8f} Ha\"\n        )\n\nh2_grad_energies_melbourne.append(cost(params))\n\ntrue_energy = -1.136189454088\n\nprint()\nprint(f\"Final estimated value of the ground-state energy = {energy:.8f} Ha\")\nprint(\n    f\"Accuracy with respect to the true energy: {np.abs(energy - true_energy):.8f} Ha\"\n)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Out:\n\n``` {.none}\nIteration = 0, Number of device executions = 333,  Energy = -0.66345346 Ha\nIteration = 5, Number of device executions = 1998,  Energy = -0.99124272 Ha\nIteration = 10, Number of device executions = 3663,  Energy = -1.00105536 Ha\nIteration = 15, Number of device executions = 5328,  Energy = -0.99592924 Ha\n\nFinal estimated value of the ground-state energy = -0.98134253 Ha\nAccuracy with respect to the true energy: 0.15484692 Ha\n```\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.figure(figsize=(10, 6))\n\nplt.plot(\n    h2_grad_device_executions_melbourne,\n    h2_grad_energies_melbourne,\n    label=\"Gradient descent, Melbourne sim.\",\n)\n\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.xlabel(\"Device executions\", fontsize=14)\nplt.ylabel(\"Energy (Ha)\", fontsize=14)\nplt.grid()\n\nplt.axhline(y=true_energy, color=\"black\", linestyle=\"dashed\", label=\"True energy\")\n\nplt.legend(fontsize=14)\n\nplt.title(\"H2 energy from the VQE using gradient descent\", fontsize=16)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![](https://pennylane.ai/qml/_images/h2_vqe_noisy_shots_melbourne.png)\n\nOn noisy hardware, the energy never quite reaches its true value, no\nmatter how many iterations are used. In order to reach the true value,\nwe would have to incorporate error mitigation techniques.\n\nVQE with SPSA\n=============\n\nNow let\\'s perform the same experiment using SPSA instead of the VQE.\nSPSA should use only 2 device executions per term in the expectation\nvalue. Since there are 15 terms, and 200 iterations, we expect 6000\ntotal device executions.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev_noisy_spsa = qml.device(\n    \"qiskit.aer\", wires=dev_melbourne.num_wires, shots=1000, noise_model=noise_model\n)\ncost_spsa = qml.QNode(exp_val_circuit, dev_noisy_spsa)\n\n# Wrapping the cost function and flattening the parameters to be compatible\n# with noisyopt which assumes a flat array of input parameters\ndef wrapped_cost(params):\n    return cost_spsa(params.reshape(num_qubits, num_params))\n\n\nnum_qubits = 4\nnum_params = 3\n\nparams = init_params.copy().reshape(num_qubits * num_params)\n\nh2_spsa_device_executions_melbourne = [0]\nh2_spsa_energies_melbourne = [wrapped_cost(params)]\n\n\ndef callback_fn(xk):\n    cost_val = wrapped_cost(xk)\n    h2_spsa_energies_melbourne.append(cost_val)\n\n    # We have evaluated every term twice, so we need to make up for this\n    num_executions = int(dev_noisy_spsa.num_executions / 2)\n    h2_spsa_device_executions_melbourne.append(num_executions)\n\n    iteration_num = len(h2_spsa_energies_melbourne)\n    if iteration_num % 10 == 0:\n        print(\n            f\"Iteration = {iteration_num}, \"\n            f\"Number of device executions = {num_executions},  \"\n            f\"Energy = {cost_val:.8f} Ha\"\n        )\n\n\nres = minimizeSPSA(\n    # Hyperparameters chosen based on grid search\n    wrapped_cost,\n    x0=params,\n    niter=niter_spsa,\n    paired=False,\n    c=0.3,\n    a=1.5,\n    callback=callback_fn,\n)\n\nprint()\nprint(f\"Final estimated value of the ground-state energy = {energy:.8f} Ha\")\nprint(\n    f\"Accuracy with respect to the true energy: {np.abs(energy - true_energy):.8f} Ha\"\n)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Out:\n\n``` {.none}\nIteration = 10, Number of device executions = 210,  Energy = -0.93065488 Ha\nIteration = 20, Number of device executions = 435,  Energy = -0.97890496 Ha\nIteration = 30, Number of device executions = 660,  Energy = -0.96639933 Ha\nIteration = 40, Number of device executions = 885,  Energy = -0.96915750 Ha\nIteration = 50, Number of device executions = 1110,  Energy = -0.96290227 Ha\nIteration = 60, Number of device executions = 1335,  Energy = -0.98274165 Ha\nIteration = 70, Number of device executions = 1560,  Energy = -0.98002812 Ha\nIteration = 80, Number of device executions = 1785,  Energy = -0.98027459 Ha\nIteration = 90, Number of device executions = 2010,  Energy = -0.99295116 Ha\nIteration = 100, Number of device executions = 2235,  Energy = -0.96745352 Ha\nIteration = 110, Number of device executions = 2460,  Energy = -0.96522842 Ha\nIteration = 120, Number of device executions = 2685,  Energy = -0.98482781 Ha\nIteration = 130, Number of device executions = 2910,  Energy = -0.98701641 Ha\nIteration = 140, Number of device executions = 3135,  Energy = -0.97656477 Ha\nIteration = 150, Number of device executions = 3360,  Energy = -0.98735587 Ha\nIteration = 160, Number of device executions = 3585,  Energy = -0.98969587 Ha\nIteration = 170, Number of device executions = 3810,  Energy = -0.96972110 Ha\nIteration = 180, Number of device executions = 4035,  Energy = -0.98354804 Ha\nIteration = 190, Number of device executions = 4260,  Energy = -0.96640637 Ha\nIteration = 200, Number of device executions = 4485,  Energy = -0.98526135 Ha\n\nFinal estimated value of the ground-state energy = -0.98134253 Ha\nAccuracy with respect to the true energy: 0.15484692 Ha\n```\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.figure(figsize=(10, 6))\n\nplt.plot(\n    h2_grad_device_executions_melbourne,\n    h2_grad_energies_melbourne,\n    label=\"Gradient descent, Melbourne sim.\",\n)\n\nplt.plot(\n    h2_spsa_device_executions_melbourne,\n    h2_spsa_energies_melbourne,\n    label=\"SPSA, Melbourne sim.\",\n)\n\nplt.title(\"H2 energy from the VQE using gradient descent vs. SPSA\", fontsize=16)\nplt.xlabel(\"Number of device executions\", fontsize=14)\nplt.ylabel(\"Energy (Ha)\", fontsize=14)\nplt.grid()\n\nplt.legend(fontsize=14)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![](https://pennylane.ai/qml/_images/h2_vqe_noisy_spsa.png)\n\nWe observe here that the SPSA optimizer again converges in fewer device\nexecutions than the gradient descent optimizer. \ud83c\udf89\n\nDue to the (simulated) hardware noise, however, the obtained energies\nare higher than the true energy, and the output still bounces around (in\nSPSA this is expected due to the inherently stochastic nature of the\nalgorithm).\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Conclusion\n==========\n\nSPSA is a useful optimization technique that may be particularly\nbeneficial on near-term quantum hardware. It uses significantly fewer\niterations to achieve comparable result quality as gradient-based\nmethods, giving it the potential to save time and resources. It can be a\ngood alternative to gradient-based methods when the optimization problem\ninvolves executing quantum circuits with many free parameters.\n\nThere are also extensions to SPSA that could be interesting to explore\nin this context. One, in particular, uses an adaptive technique to\napproximate the *Hessian* matrix during optimization to effectively\nincrease the convergence rate of SPSA. The proposed technique can also\nbe applied in cases where there is direct access to the gradient of the\ncost function.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["References\n==========\n"]}]}