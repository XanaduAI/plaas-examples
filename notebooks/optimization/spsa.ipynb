{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Optimization using SPSA\n\n.. meta::\n    :property=\"og:description\": Use the simultaneous perturbation stochastic\n        approximation algorithm to optimize variational circuits in PennyLane.\n    :property=\"og:image\": https://pennylane.ai/qml/_images/spsa_mntn.png\n\n.. related::\n\n   tutorial_vqe A brief overview of VQE\n   tutorial_vqe_qng Accelerating VQE with the QNG\n\n*Author: PennyLane dev team. Posted: 19 Mar 2021. Last updated: 8 Apr 2021.*\n\nIn this tutorial, we investigate using a gradient-free optimizer called\nthe Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm to optimize quantum\ncircuits. SPSA is a technique that involves approximating the gradient of a\nquantum circuit without having to compute it.\n\nThis demonstration shows how the SPSA optimizer performs:\n\n1. A simple task on a sampling device,\n2. The variational quantum eigensolver on a simulated hardware device.\n\nThroughout the demo, we show results obtained with SPSA and with gradient\ndescent and also compare the number of device executions required to complete\neach optimization.\n\n## Background\n\nIn PennyLane, quantum gradients on hardware are commonly computed using\n`parameter-shift rules\n<https://pennylane.ai/qml/glossary/parameter_shift.html>`_. Computing quantum\ngradients involves evaluating the partial derivative of the quantum function\nwith respect to every free parameter. The partial derivatives are used to apply\nthe product rule to compute the gradient of the quantum circuit. For qubit\noperations that are generated by one of the Pauli matrices, each partial\nderivative computation will involve two quantum circuit evaluations with a\npositive and a negative shift in the parameter values.\n\nAs there are two circuit evaluations for each free parameter, the number of\noverall quantum circuit executions for computing a quantum gradient is\n$O(p)$ as it scales linearly with the number of free parameters\n$p$. This scaling can be very costly for optimization tasks with many\nfree parameters.  For the overall optimization this scaling means we need\n$O(pn)$ quantum circuit evaluations, where $n$ is the number of\noptimization steps taken.\n\nFortunately, there are certain optimization techniques that offer an\nalternative to computing the gradients of quantum circuits. One such technique\nis called the Simultaneous Perturbation Stochastic Approximation (SPSA)\nalgorithm [#spall_overview]_. SPSA is an optimization method that involves\n*approximating* the gradient of the cost function at each iteration step. This\ntechnique requires only two quantum circuit executions per iteration step,\nregardless of the number of free parameters. Therefore the overall number of\ncircuit executions would be $O(n')$ where $n'$ is the number of\noptimization steps taken when using SPSA. This technique is also considered\nrobust against noise, making it a great optimization method in the NISQ era.\n\nIn this demo, you'll learn how the SPSA algorithm works, and how to apply it in\nPennyLane to compute gradients of quantum circuits. You'll also see it in action\nusing noisy quantum data!\n\n## Simultaneous perturbation stochastic approximation (SPSA)\n\nSPSA is a general method for minimizing differentiable multivariate functions.\nIt is particularly useful for functions for which evaluating the gradient is not\npossible, or too resource intensive. SPSA provides a stochastic method for\napproximating the gradient of a multivariate differentiable cost function. To\naccomplish this the cost function is evaluated twice using perturbed parameter\nvectors: every component of the original parameter vector is simultaneously\nshifted with a randomly generated value. This is in contrast to\nfinite-differences methods where for each evaluation only one component of the\nparameter vector is shifted at a time.\n\nSimilar to gradient-based approaches such as gradient descent, SPSA is an\niterative optimization algorithm. Let's consider a differentiable cost function\n$L(\\theta)$ where $\\theta$ is a $p$-dimensional vector and\nwhere the optimization problem can be translated into finding a $\\theta^*$\nat which $\\frac{\\partial L}{\\partial \\theta} = 0$.  It is assumed that\nmeasurements of $L(\\theta)$ are available at various values of\n$\\theta$---this is exactly the problem that we'd consider when optimizing\nquantum functions!\n\nJust like with gradient-based methods, SPSA starts with an initial parameter\nvector $\\hat{\\theta}_{0}$. After $k$ iterations, the $(k+1)$ th\nparameter iterates can be obtained as\n\n\\begin{align}\\hat{\\theta}_{k+1} = \\hat{\\theta}_{k} - a_{k}\\hat{g}_{k}(\\hat{\\theta}_{k}),\\end{align}\n\nwhere $\\hat{g}_{k}$ is the estimate of the gradient $g(\\theta) =\n\\frac{ \\partial L}{\\partial \\theta}$ at the iterate $\\hat{\\theta}_{k}$\nbased on prior measurements of the cost function, and $a_{k}$ is a\npositive number [#spall_overview]_.\n\nOne of the advantages of SPSA is that it is robust to any noise that may occur\nwhen measuring the function $L$. Therefore, let's consider the function\n$y(\\theta)=L(\\theta) + \\varepsilon$, where $\\varepsilon$ is some\nperturbation of the output. In SPSA, the estimated gradient at each iteration\nstep is expressed as\n\n\\begin{align}\\hat{g}_{ki} (\\hat{\\theta}_{k}) = \\frac{y(\\hat{\\theta}_{k} +c_{k}\\Delta_{k})\n    - y(\\hat{\\theta}_{k} -c_{k}\\Delta_{k})}{2c_{k}\\Delta_{ki}},\\end{align}\n\nwhere $c_{k}$ is a positive number and $\\Delta_{k} = (\\Delta_{k_1},\n\\Delta_{k_2}, ..., \\Delta_{k_p})^{T}$ is a perturbation vector. The\nstochasticity of the technique comes from the fact that for each iteration step\n$k$ the components of the $\\Delta_{k}$ perturbation vector are\nrandomly generated using a zero-mean distribution. In most cases, the Bernoulli\ndistribution is used, meaning each parameter is simultaneously perturbed by\neither $\\pm c_k$.\n\nIt is this perturbation that makes SPSA robust to noise --- since every\nparameter is already being shifted, additional shifts due to noise are less\nlikely to hinder the optimization process. In a sense, noise gets \"absorbed\"\ninto the already-stochastic process. This is highlighted in the figure below,\nwhich portrays an example of the type of path SPSA takes through the space of\nthe function, compared to a standard gradient-based optimizer.\n\n.. figure:: ../demonstrations/spsa/spsa_mntn.png\n   :align: center\n   :width: 60%\n\n   ..\n\n   A schematic of the search paths used by gradient descent with\n   parameter-shift and SPSA.\n\nNow that we have explored how SPSA works, let's see how it performs in practice!\n\n## Optimization on a sampling device\n\n.. important::\n\n    To run this demo locally, you'll need to install the `noisyopt\n    <https://github.com/andim/noisyopt>`_ library. This library contains a\n    straightforward implementation of SPSA that can be used in the same way as the\n    optimizers available in `SciPy's minimize method\n    <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`_.\n\nFirst, let's consider a simple quantum circuit on a sampling device. For this,\nwe'll be using a device from the `PennyLane-Qiskit plugin\n<https://pennylaneqiskit.readthedocs.io/en/latest/>`_ that samples quantum\ncircuits to get measurement outcomes and later post-processes these outcomes to\ncompute statistics like expectation values.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Just as with other PennyLane devices, the number of samples taken for a device\n    execution can be specified using the ``shots`` keyword argument of the\n    device.</p></div>\n\nOnce we have a device selected, we just need a couple of other ingredients for\nthe pieces of an example optimization to come together:\n\n* a circuit ansatz: :func:`~.pennylane.templates.layers.StronglyEntanglingLayers`,\n* initial parameters: the correct shape can be computed by :func:`~.pennylane.templates.layers.StronglyEntanglingLayers.shape`,\n* an observable: $\\bigotimes_{i=0}^{N-1}\\sigma_z^i$, where $N$ stands\n  for the number of qubits.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import pennylane as qml\nfrom pennylane import numpy as np\n\nnum_wires = 4\nnum_layers = 5\n\ndev_sampler_spsa = qml.device(\"qiskit.aer\", wires=num_wires, shots=1000)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We seed so that we can simulate the same circuit every time.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["np.random.seed(50)\n\nall_pauliz_tensor_prod = qml.operation.Tensor(*[qml.PauliZ(i) for i in range(num_wires)])\n\n\ndef circuit(params):\n    qml.StronglyEntanglingLayers(params, wires=list(range(num_wires)))\n    return qml.expval(all_pauliz_tensor_prod)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["After this, we'll initialize the parameters in a way that is compatible with\nthe ``noisyopt`` package. The ``noisyopt`` package requires the trainable parameters\nbe a flattened array. As a result, our cost function must accept a flat array of parameters\nto be optimized.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["flat_shape = num_layers * num_wires * 3\nparam_shape = qml.templates.StronglyEntanglingLayers.shape(n_wires=num_wires, n_layers=num_layers)\ninit_params = np.random.normal(scale=0.1, size=param_shape, requires_grad=True)\n\ninit_params_spsa = init_params.reshape(flat_shape)\n\nqnode_spsa = qml.QNode(circuit, dev_sampler_spsa)\n\n\ndef cost_spsa(params):\n    return qnode_spsa(params.reshape(num_layers, num_wires, 3))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Once we have defined each piece of the optimization, there's only one\nremaining component required: the *SPSA optimizer*.\nWe'll use the SPSA optimizer provided by the ``noisyopt`` package. Once\nimported, we can initialize parts of the optimization such as the number of\niterations, a collection to store the cost values, and a callback function.\nOnce the optimization has concluded, we save the number of device executions\nrequired for completion using the callback function. This will be an\ninteresting quantity!\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from noisyopt import minimizeSPSA\n\nniter_spsa = 200\n\n# Evaluate the initial cost\ncost_store_spsa = [cost_spsa(init_params_spsa)]\ndevice_execs_spsa = [0]\n\n\ndef callback_fn(xk):\n    cost_val = cost_spsa(xk)\n    cost_store_spsa.append(cost_val)\n\n    # We've evaluated the cost function, let's make up for that\n    num_executions = int(dev_sampler_spsa.num_executions / 2)\n    device_execs_spsa.append(num_executions)\n\n    iteration_num = len(cost_store_spsa)\n    if iteration_num % 10 == 0:\n        print(\n            f\"Iteration = {iteration_num}, \"\n            f\"Number of device executions = {num_executions}, \"\n            f\"Cost = {cost_val}\"\n        )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Choosing the hyperparameters\n\nThe ``noisyopt`` package allows us to choose the initial value of two\nhyperparameters for SPSA: the $c$ and $a$ coefficients. Recall\nfrom above that the $c$ values control the amount of random shift when\nevaluating the cost function, while the $a$ coefficients are analogous to a learning\nrate and affect the degree to which the parameters change at each update\nstep.\n\nWith stochastic approximation, specifying such hyperparameters significantly\ninfluences the convergence of the optimization for a given problem. Although\nthere is no universal recipe for selecting these values (as they depend\nstrongly on the specific problem), [#spall_implementation]_ includes\nguidelines for the selection. In our case, the initial values for $c$\nand $a$ were selected as a result of a grid search to ensure a fast\nconvergence.  We further note that apart from $c$ and $a$, there\nare further coefficients that are initialized in the ``noisyopt`` package\nusing the\npreviously mentioned guidelines.\n\nOur cost function does not take a seed as a keyword argument (which would be\nthe default behaviour for ``minimizeSPSA``), so we set ``paired=False``.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["res = minimizeSPSA(\n    cost_spsa,\n    x0=init_params_spsa.copy(),\n    niter=niter_spsa,\n    paired=False,\n    c=0.15,\n    a=0.2,\n    callback=callback_fn,\n)"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n    Iteration = 10, Number of device executions = 14, Cost = 0.09\n    Iteration = 20, Number of device executions = 29, Cost = -0.638\n    Iteration = 30, Number of device executions = 44, Cost = -0.842\n    Iteration = 40, Number of device executions = 59, Cost = -0.926\n    Iteration = 50, Number of device executions = 74, Cost = -0.938\n    Iteration = 60, Number of device executions = 89, Cost = -0.94\n    Iteration = 70, Number of device executions = 104, Cost = -0.962\n    Iteration = 80, Number of device executions = 119, Cost = -0.938\n    Iteration = 90, Number of device executions = 134, Cost = -0.946\n    Iteration = 100, Number of device executions = 149, Cost = -0.966\n    Iteration = 110, Number of device executions = 164, Cost = -0.954\n    Iteration = 120, Number of device executions = 179, Cost = -0.964\n    Iteration = 130, Number of device executions = 194, Cost = -0.952\n    Iteration = 140, Number of device executions = 209, Cost = -0.958\n    Iteration = 150, Number of device executions = 224, Cost = -0.968\n    Iteration = 160, Number of device executions = 239, Cost = -0.948\n    Iteration = 170, Number of device executions = 254, Cost = -0.974\n    Iteration = 180, Number of device executions = 269, Cost = -0.962\n    Iteration = 190, Number of device executions = 284, Cost = -0.988\n    Iteration = 200, Number of device executions = 299, Cost = -0.964\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's perform the same optimization using gradient descent. We set the\nstep size according to a favourable value found after grid search for fast\nconvergence. Note that we also create a new device in order to reset the execution count to 0.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["opt = qml.GradientDescentOptimizer(stepsize=0.3)\n\n# Create a device, qnode and cost function specific to gradient descent\ndev_sampler_gd = qml.device(\"qiskit.aer\", wires=num_wires, shots=1000)\nqnode_gd = qml.QNode(circuit, dev_sampler_gd)\n\n\ndef cost_gd(params):\n    return qnode_gd(params)\n\n\nsteps = 20\nparams = init_params.copy()\n\ndevice_execs_grad = [0]\ncost_store_grad = []\n\nfor k in range(steps):\n    params, val = opt.step_and_cost(cost_gd, params)\n    device_execs_grad.append(dev_sampler_gd.num_executions)\n    cost_store_grad.append(val)\n    print(\n        f\"Iteration = {k}, \"\n        f\"Number of device executions = {dev_sampler_gd.num_executions}, \"\n        f\"Cost = {val}\"\n    )\n\n# The step_and_cost function gives us the cost at the previous step, so to find\n# the cost at the final parameter values we have to compute it manually\ncost_store_grad.append(cost_gd(params))"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n    Iteration = 0, Number of device executions = 121, Cost = 0.904\n    Iteration = 1, Number of device executions = 242, Cost = 0.758\n    Iteration = 2, Number of device executions = 363, Cost = 0.284\n    Iteration = 3, Number of device executions = 484, Cost = -0.416\n    Iteration = 4, Number of device executions = 605, Cost = -0.836\n    Iteration = 5, Number of device executions = 726, Cost = -0.964\n    Iteration = 6, Number of device executions = 847, Cost = -0.992\n    Iteration = 7, Number of device executions = 968, Cost = -0.994\n    Iteration = 8, Number of device executions = 1089, Cost = -0.992\n    Iteration = 9, Number of device executions = 1210, Cost = -0.994\n    Iteration = 10, Number of device executions = 1331, Cost = -0.998\n    Iteration = 11, Number of device executions = 1452, Cost = -0.992\n    Iteration = 12, Number of device executions = 1573, Cost = -0.994\n    Iteration = 13, Number of device executions = 1694, Cost = -1.0\n    Iteration = 14, Number of device executions = 1815, Cost = -0.996\n    Iteration = 15, Number of device executions = 1936, Cost = -0.996\n    Iteration = 16, Number of device executions = 2057, Cost = -0.998\n    Iteration = 17, Number of device executions = 2178, Cost = -0.996\n    Iteration = 18, Number of device executions = 2299, Cost = -0.996\n    Iteration = 19, Number of device executions = 2420, Cost = -0.996\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### SPSA and gradient descent comparison\n\nAt this point, nothing else remains but to check which of these approaches did\nbetter!\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\n\nplt.plot(device_execs_grad, cost_store_grad, label=\"Gradient descent\")\nplt.plot(device_execs_spsa, cost_store_spsa, label=\"SPSA\")\n\nplt.xlabel(\"Number of device executions\", fontsize=14)\nplt.ylabel(\"Cost function value\", fontsize=14)\nplt.grid()\n\nplt.title(\"Gradient descent vs. SPSA for simple optimization\", fontsize=16)\nplt.legend(fontsize=14)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. figure:: ../demonstrations/spsa/first_comparison.png\n    :align: center\n    :width: 75%\n\nIt seems that SPSA performs great and it does so with a significant savings when\ncompared to gradient descent!\n\nLet's take a deeper dive to see how much better it actually is.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["grad_desc_exec_min = device_execs_grad[np.argmin(cost_store_grad)]\nspsa_exec_min = device_execs_spsa[np.argmin(cost_store_spsa)]\nprint(f\"Device execution ratio: {grad_desc_exec_min/spsa_exec_min}.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n    Device execution ratio: 4.161375661375661\n\nThis means that SPSA can potentially find the minimum of a cost function by\nusing over 4 times fewer device executions than gradient descent! That's a huge\nsaving, especially in cases such as running on actual quantum hardware.\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## SPSA and the variational quantum eigensolver\n\nNow that we've explored the theoretical underpinnings of SPSA, let's use it\nto optimize a real chemical system, that of the hydrogen molecule $H_2$.\nThis molecule was studied previously in the `introductory variational quantum\neigensolver (VQE) demo </demos/tutorial_vqe>`_, and so we will reuse some of\nthat machinery below to set up the problem.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from pennylane import qchem\n\nsymbols = [\"H\", \"H\"]\ncoordinates = np.array([0.0, 0.0, -0.6614, 0.0, 0.0, 0.6614])\nh2_ham, num_qubits = qchem.molecular_hamiltonian(symbols, coordinates)\n\n# Variational ansatz for H_2 - see Intro VQE demo for more details\ndef circuit(params, wires):\n    qml.BasisState(np.array([1, 1, 0, 0]), wires=wires)\n    for i in wires:\n        qml.Rot(*params[i], wires=i)\n    qml.CNOT(wires=[2, 3])\n    qml.CNOT(wires=[2, 0])\n    qml.CNOT(wires=[3, 1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The $H_2$ Hamiltonian uses 4 qubits, contains 15 terms, and has a ground\nstate energy of $-1.136189454088$ Hartree.\n\nSince SPSA is robust to noise, let's see how it fares compared to gradient\ndescent when run on noisy hardware. For this, we will set up and use a simulated\nversion of IBM Q's hardware.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from qiskit import IBMQ\nfrom qiskit.providers.aer import noise\n\n# Note: you will need to be authenticated to IBMQ to run the following code.\n# Do not run the simulation on this device, as it will send it to real hardware\n# For access to IBMQ, the following statements will be useful:\n# IBMQ.load_account() # Load account from disk\n# List the providers to pick an available backend:\n# IBMQ.providers()    # List all available providers\n\ndev = qml.device(\n    \"qiskit.ibmq\", wires=num_qubits, backend=\"ibmq_lima\"\n)\nnoise_model = noise.NoiseModel.from_backend(dev.backend)\ndev_noisy = qml.device(\n    \"qiskit.aer\", wires=dev.num_wires, shots=1000, noise_model=noise_model\n)\n\ndef exp_val_circuit(params):\n    circuit(params, range(dev.num_wires))\n    return qml.expval(h2_ham)\n\n# Initialize the optimizer - optimal step size was found through a grid search\nopt = qml.GradientDescentOptimizer(stepsize=2.2)\ncost = qml.QNode(exp_val_circuit, dev_noisy)\n\n# This random seed was used in the original VQE demo and is known to allow the\n# algorithm to converge to the global minimum.\nnp.random.seed(0)\ninit_params = np.random.normal(0, np.pi, (num_qubits, 3), requires_grad=True)\nparams = init_params.copy()\n\nh2_grad_device_executions_ibm = [0]\nh2_grad_energies_ibm = []\n\nmax_iterations = 20\n\n# Run the gradient descent algorithm\nfor n in range(max_iterations):\n    params, energy = opt.step_and_cost(cost, params)\n    h2_grad_device_executions_ibm.append(dev_noisy.num_executions)\n    h2_grad_energies_ibm.append(energy)\n\n    if n % 5 == 0:\n        print(\n            f\"Iteration = {n}, \"\n            f\"Number of device executions = {dev_noisy.num_executions},  \"\n            f\"Energy = {energy:.8f} Ha\"\n        )\n\nh2_grad_energies_ibm.append(cost(params))\n\ntrue_energy = -1.136189454088\n\nprint()\nprint(f\"Final estimated value of the ground-state energy = {energy:.8f} Ha\")\nprint(\n    f\"Accuracy with respect to the true energy: {np.abs(energy - true_energy):.8f} Ha\"\n)"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n    Iteration = 0, Number of device executions = 333,  Energy = -0.66345346 Ha\n    Iteration = 5, Number of device executions = 1998,  Energy = -0.99124272 Ha\n    Iteration = 10, Number of device executions = 3663,  Energy = -1.00105536 Ha\n    Iteration = 15, Number of device executions = 5328,  Energy = -0.99592924 Ha\n\n    Final estimated value of the ground-state energy = -0.98134253 Ha\n    Accuracy with respect to the true energy: 0.15484692 Ha\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.figure(figsize=(10, 6))\n\nplt.plot(\n    h2_grad_device_executions_ibm,\n    h2_grad_energies_ibm,\n    label=\"Gradient descent, IBM sim.\",\n)\n\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.xlabel(\"Device executions\", fontsize=14)\nplt.ylabel(\"Energy (Ha)\", fontsize=14)\nplt.grid()\n\nplt.axhline(y=true_energy, color=\"black\", linestyle=\"dashed\", label=\"True energy\")\n\nplt.legend(fontsize=14)\n\nplt.title(\"H2 energy from the VQE using gradient descent\", fontsize=16)"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. figure:: ../demonstrations/spsa/h2_vqe_noisy_shots_ibm.png\n    :align: center\n    :width: 90%\n\nOn noisy hardware, the energy never quite reaches its true value, no matter\nhow many iterations are used. In order to reach the true value, we would have\nto incorporate error mitigation techniques.\n\n### VQE with SPSA\n\nNow let's perform the same experiment using SPSA instead of the VQE.\nSPSA should use only 2 device executions per term in the expectation value.\nSince there are 15 terms, and 200 iterations, we expect 6000 total device\nexecutions.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev_noisy_spsa = qml.device(\n    \"qiskit.aer\", wires=dev.num_wires, shots=1000, noise_model=noise_model\n)\ncost_spsa = qml.QNode(exp_val_circuit, dev_noisy_spsa)\n\n# Wrapping the cost function and flattening the parameters to be compatible\n# with noisyopt which assumes a flat array of input parameters\ndef wrapped_cost(params):\n    return cost_spsa(params.reshape(num_qubits, num_params))\n\n\nnum_qubits = 4\nnum_params = 3\n\nparams = init_params.copy().reshape(num_qubits * num_params)\n\nh2_spsa_device_executions_ibm = [0]\nh2_spsa_energies_ibm = [wrapped_cost(params)]\n\n\ndef callback_fn(xk):\n    cost_val = wrapped_cost(xk)\n    h2_spsa_energies_ibm.append(cost_val)\n\n    # We have evaluated every term twice, so we need to make up for this\n    num_executions = int(dev_noisy_spsa.num_executions / 2)\n    h2_spsa_device_executions_ibm.append(num_executions)\n\n    iteration_num = len(h2_spsa_energies_ibm)\n    if iteration_num % 10 == 0:\n        print(\n            f\"Iteration = {iteration_num}, \"\n            f\"Number of device executions = {num_executions},  \"\n            f\"Energy = {cost_val:.8f} Ha\"\n        )\n\n\nres = minimizeSPSA(\n    # Hyperparameters chosen based on grid search\n    wrapped_cost,\n    x0=params,\n    niter=niter_spsa,\n    paired=False,\n    c=0.3,\n    a=1.5,\n    callback=callback_fn,\n)\n\nprint()\nprint(f\"Final estimated value of the ground-state energy = {energy:.8f} Ha\")\nprint(\n    f\"Accuracy with respect to the true energy: {np.abs(energy - true_energy):.8f} Ha\"\n)"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n    Iteration = 10, Number of device executions = 210,  Energy = -0.93065488 Ha\n    Iteration = 20, Number of device executions = 435,  Energy = -0.97890496 Ha\n    Iteration = 30, Number of device executions = 660,  Energy = -0.96639933 Ha\n    Iteration = 40, Number of device executions = 885,  Energy = -0.96915750 Ha\n    Iteration = 50, Number of device executions = 1110,  Energy = -0.96290227 Ha\n    Iteration = 60, Number of device executions = 1335,  Energy = -0.98274165 Ha\n    Iteration = 70, Number of device executions = 1560,  Energy = -0.98002812 Ha\n    Iteration = 80, Number of device executions = 1785,  Energy = -0.98027459 Ha\n    Iteration = 90, Number of device executions = 2010,  Energy = -0.99295116 Ha\n    Iteration = 100, Number of device executions = 2235,  Energy = -0.96745352 Ha\n    Iteration = 110, Number of device executions = 2460,  Energy = -0.96522842 Ha\n    Iteration = 120, Number of device executions = 2685,  Energy = -0.98482781 Ha\n    Iteration = 130, Number of device executions = 2910,  Energy = -0.98701641 Ha\n    Iteration = 140, Number of device executions = 3135,  Energy = -0.97656477 Ha\n    Iteration = 150, Number of device executions = 3360,  Energy = -0.98735587 Ha\n    Iteration = 160, Number of device executions = 3585,  Energy = -0.98969587 Ha\n    Iteration = 170, Number of device executions = 3810,  Energy = -0.96972110 Ha\n    Iteration = 180, Number of device executions = 4035,  Energy = -0.98354804 Ha\n    Iteration = 190, Number of device executions = 4260,  Energy = -0.96640637 Ha\n    Iteration = 200, Number of device executions = 4485,  Energy = -0.98526135 Ha\n\n    Final estimated value of the ground-state energy = -0.98134253 Ha\n    Accuracy with respect to the true energy: 0.15484692 Ha\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.figure(figsize=(10, 6))\n\nplt.plot(\n    h2_grad_device_executions_ibm,\n    h2_grad_energies_ibm,\n    label=\"Gradient descent, IBM sim.\",\n)\n\nplt.plot(\n    h2_spsa_device_executions_ibm,\n    h2_spsa_energies_ibm,\n    label=\"SPSA, IBM sim.\",\n)\n\nplt.title(\"H2 energy from the VQE using gradient descent vs. SPSA\", fontsize=16)\nplt.xlabel(\"Number of device executions\", fontsize=14)\nplt.ylabel(\"Energy (Ha)\", fontsize=14)\nplt.grid()\n\nplt.legend(fontsize=14)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. figure:: ../demonstrations/spsa/h2_vqe_noisy_spsa.png\n    :align: center\n    :width: 90%\n\nWe observe here that the SPSA optimizer again converges in fewer device\nexecutions than the gradient descent optimizer. \ud83c\udf89\n\nDue to the (simulated) hardware noise, however, the obtained energies are\nhigher than the true energy, and the output still bounces around (in SPSA\nthis is expected due to the inherently stochastic nature of the algorithm).\n\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Conclusion\n\nSPSA is a useful optimization technique that may be particularly beneficial on\nnear-term quantum hardware. It uses significantly fewer iterations to achieve\ncomparable result quality as gradient-based methods, giving it the potential\nto save time and resources. It can be a good alternative to\ngradient-based methods when the optimization problem involves executing\nquantum circuits with many free parameters.\n\nThere are also extensions to SPSA that could be interesting to explore in\nthis context. One, in particular, uses an adaptive technique to approximate\nthe *Hessian* matrix during optimization to effectively increase the\nconvergence rate of SPSA [#spall_overview]_. The proposed technique can also\nbe applied in cases where there is direct access to the gradient of the cost\nfunction.\n\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## References\n\n.. [#spall_overview]\n\n   James C. Spall, \"`An Overview of the Simultaneous Perturbation Method\n   for Efficient Optimization <https://www.jhuapl.edu/SPSA/PDF-SPSA/Spall_An_Overview.PDF>`__\", 1998\n\n.. [#spall_implementation]\n\n   J. C. Spall, \"Implementation of the simultaneous perturbation algorithm\n   for stochastic optimization,\" in IEEE Transactions on Aerospace and\n   Electronic Systems, vol. 34, no. 3, pp. 817-823, July 1998, doi:\n   10.1109/7.705889.\n\n.. [#spall_hessian]\n\n   J. C. Spall, \"Adaptive stochastic approximation by the simultaneous\n   perturbation method,\" in IEEE Transactions on Automatic Control,\n   vol. 45, no. 10, pp. 1839-1853, Oct 2020, doi:\n   10.1109/TAC.2000.880982.\n\n\n\n\n"]}], "metadata": {"kernelspec": {"display_name": "PennyLane", "language": "python", "name": "pennylane"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.10"}}, "nbformat": 4, "nbformat_minor": 0}