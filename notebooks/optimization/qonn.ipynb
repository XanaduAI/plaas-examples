{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n\nOptimizing a quantum optical neural network\n===========================================\n\n.. meta::\n    :property=\"og:description\": Optimizing a quantum optical neural network using PennyLane.\n    :property=\"og:image\": https://pennylane.ai/qml/_images/qonn_thumbnail.png\n\n.. related::\n\n   quantum_neural_net Function fitting with a photonic QNN\n\n*Author: PennyLane dev team. Last updated: 8 Mar 2022.*\n\nThis tutorial is based on a paper from `Steinbrecher et al. (2019)\n<https://www.nature.com/articles/s41534-019-0174-7>`__ which explores a Quantum Optical Neural\nNetwork (QONN) based on Fock states. Similar to the continuous-variable :doc:`quantum neural network\n</demos/quantum_neural_net>` (CV QNN) model described by `Killoran et al. (2018)\n<https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.1.033063>`__, the QONN\nattempts to apply neural networks and deep learning theory to the quantum case, using quantum data\nas well as a quantum hardware-based architecture.\n\nWe will focus on constructing a QONN as described in Steinbrecher et al. and training it to work as\na basic CNOT gate using a \"dual-rail\" state encoding. This tutorial also provides a working example\nof how to use third-party optimization libraries with PennyLane; in this case, `NLopt\n<https://nlopt.readthedocs.io/en/latest/>`__ will be used.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. figure:: ../demonstrations/qonn/qonn_thumbnail.png\n    :width: 100%\n    :align: center\n\n    A quantum optical neural network using the Reck encoding (green)\n    with a Kerr non-linear layer (red)\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Background\n----------\n\nThe QONN is an optical architecture consisting of layers of linear\nunitaries, using the encoding described in `Reck et\nal. (1994) <https://dx.doi.org/10.1103/PhysRevLett.73.58>`__, and Kerr\nnon-linearities applied on all involved optical modes. This setup can be\nconstructed using arrays of beamsplitters and programmable phase shifts\nalong with some form of Kerr non-linear material.\n\nBy constructing a cost function based on the input-output relationship\nof the QONN, using the programmable phase-shift variables as\noptimization parameters, it can be trained to both act as an arbitrary\nquantum gate or to be able to generalize on previously unseen data. This\nis very similar to classical neural networks, and many classical machine\nlearning task can in fact also be solved by these types of quantum deep\nneural networks.\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Code and simulations\n--------------------\n\nThe first thing we need to do is to import PennyLane, NumPy, as well as an\noptimizer. Here we use a wrapped version of NumPy supplied by PennyLane\nwhich uses Autograd to wrap essential functions to support automatic\ndifferentiation.\n\nThere are many optimizers to choose from. We could either use an\noptimizer from the ``pennylane.optimize`` module or we could use a\nthird-party optimizer. In this case we will use the Nlopt library\nwhich has several fast implementations of both gradient-free and\ngradient-based optimizers.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import pennylane as qml\nfrom pennylane import numpy as np\n\nimport nlopt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We create a Strawberry Fields simulator device with as many quantum modes\n(or wires) as we want our quantum-optical neural network to have. Four\nmodes are used for this demonstration, due to the use of a dual-rail encoding. The\ncutoff dimension is set to the same value as the number of wires (a\nlower cutoff value will cause loss of information, while a higher value\nmight use unnecessary resources without any improvement).\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev = qml.device(\"strawberryfields.fock\", wires=4, cutoff_dim=4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\"><h4>Note</h4><p>You will need to have `Strawberry Fields <https://strawberryfields.ai/>`__ as well as the\n    `Strawberry Fields plugin <https://pennylane-sf.readthedocs.io/en/latest/>`__ for PennyLane\n    installed for this tutorial to work.</p></div>\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Creating the QONN\n~~~~~~~~~~~~~~~~~\n\nCreate a layer function which defines one layer of the QONN, consisting of a linear\n`interferometer\n<https://pennylane.readthedocs.io/en/stable/code/api/pennylane.templates.subroutines.Interferometer.html>`__\n(i.e., an array of beamsplitters and phase shifts) and a non-linear Kerr interaction layer. Both\nthe interferometer and the non-linear layer are applied to all modes. The triangular mesh scheme,\ndescribed in `Reck et al. (1994) <https://dx.doi.org/10.1103/PhysRevLett.73.58>`__ is chosen here\ndue to its use in the paper from Steinbrecher et al., although any other interferometer scheme\nshould work equally well. Some might even be slightly faster than the one we use here.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def layer(theta, phi, wires):\n    M = len(wires)\n    phi_nonlinear = np.pi / 2\n\n    qml.Interferometer(\n        theta, phi, np.zeros(M), wires=wires, mesh=\"triangular\",\n    )\n\n    for i in wires:\n        qml.Kerr(phi_nonlinear, wires=i)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, we define the full QONN by building each layer one-by-one and then\nmeasuring the mean photon number of each mode. The parameters to be\noptimized are all contained in ``var``, where each element in ``var`` is\na list of parameters ``theta`` and ``phi`` for a specific layer.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["@qml.qnode(dev)\ndef quantum_neural_net(var, x):\n    wires = list(range(len(x)))\n\n    # Encode input x into a sequence of quantum fock states\n    for i in wires:\n        qml.FockState(x[i], wires=i)\n\n    # \"layer\" subcircuits\n    for i, v in enumerate(var):\n        layer(v[: len(v) // 2], v[len(v) // 2 :], wires)\n\n    return [qml.expval(qml.NumberOperator(w)) for w in wires]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Defining the cost function\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA helper function is needed to calculate the normalized square loss of\ntwo vectors. The square loss function returns a value between 0 and 1,\nwhere 0 means that ``labels`` and ``predictions`` are equal and 1 means\nthat the vectors are fully orthogonal.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def square_loss(labels, predictions):\n    term = 0\n    for l, p in zip(labels, predictions):\n        lnorm = l / np.linalg.norm(l)\n        pnorm = p / np.linalg.norm(p)\n\n        term = term + np.abs(np.dot(lnorm, pnorm.T)) ** 2\n\n    return 1 - term / len(labels)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, we define the cost function to be used during optimization. It\ncollects the outputs from the QONN (``predictions``) for each input\n(``data_inputs``) and then calculates the square loss between the\npredictions and the true outputs (``labels``).\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def cost(var, data_input, labels):\n    predictions = np.array([quantum_neural_net(var, x) for x in data_input])\n    sl = square_loss(labels, predictions)\n\n    return sl"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optimizing for the CNOT gate\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFor this tutorial we will train the network to function as a CNOT gate.\nThat is, it should transform the input states in the following way:\n\n.. figure:: ../demonstrations/qonn/cnot.png\n    :width: 30%\n    :align: center\n\n|\n\nWe need to choose the inputs ``X`` and the corresponding labels ``Y``. They are\ndefined using the dual-rail encoding, meaning that $|0\\rangle = [1, 0]$\n(as a vector in the Fock basis of a single mode), and\n$|1\\rangle = [0, 1]$. So a CNOT transformation of $|1\\rangle|0\\rangle = |10\\rangle = [0, 1, 1, 0]$\nwould give $|11\\rangle = [0, 1, 0, 1]$.\n\nFurthermore, we want to make sure that the gradient isn't calculated with regards\nto the inputs or the labels. We can do this by marking them with `requires_grad=False`.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# Define the CNOT input-output states (dual-rail encoding) and initialize\n# them as non-differentiable.\n\nX = np.array([[1, 0, 1, 0],\n              [1, 0, 0, 1],\n              [0, 1, 1, 0],\n              [0, 1, 0, 1]], requires_grad=False)\n\nY = np.array([[1, 0, 1, 0],\n              [1, 0, 0, 1],\n              [0, 1, 0, 1],\n              [0, 1, 1, 0]], requires_grad=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["At this stage we could play around with other input-output\ncombinations; just keep in mind that the input states should contain the\nsame total number of photons as the output, since we want to use\nthe dual-rail encoding. Also, since the QONN will\nact upon the states as a unitary operator, there must be a bijection\nbetween the inputs and the outputs, i.e., two different inputs must have\ntwo different outputs, and vice versa.\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\"><h4>Note</h4><p>Other example gates we could use include the dual-rail encoded SWAP gate,\n\n    .. code:: python\n\n        X = np.array([[1, 0, 1, 0],\n                      [1, 0, 0, 1],\n                      [0, 1, 1, 0],\n                      [0, 1, 0, 1]])\n\n        Y = np.array([[1, 0, 1, 0],\n                      [0, 1, 1, 0],\n                      [0, 0, 0, 1],\n                      [0, 1, 0, 1]])\n\n    the single-rail encoded SWAP gate (remember to change the number of\n    modes to 2 in the device initialization above),\n\n    .. code:: python\n\n        X = np.array([[0, 1], [1, 0]])\n        Y = np.array([[1, 0], [0, 1]])\n\n    or the single 6-photon GHZ state (which needs 6 modes, and thus might be\n    very heavy on both memory and CPU):\n\n    .. code:: python\n\n        X = np.array([1, 0, 1, 0, 1, 0])\n        Y = (np.array([1, 0, 1, 0, 1, 0]) + np.array([1, 0, 1, 0, 1, 0])) / 2</p></div>\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, we must set the number of layers to use and then calculate the\ncorresponding number of initial parameter values, initializing them with\na random value between $-2\\pi$ and $2\\pi$. For the CNOT gate two layers is\nenough, although for more complex optimization tasks, many more layers\nmight be needed. Generally, the more layers there are, the richer the\nrepresentational capabilities of the neural network, and the better it\nwill be at finding a good fit.\n\nThe number of variables corresponds to the number of transmittivity\nangles $\\theta$ and phase angles $\\phi$, while the Kerr\nnon-linearity is set to a fixed strength.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["num_layers = 2\nM = len(X[0])\nnum_variables_per_layer = M * (M - 1)\n\nrng = np.random.default_rng(seed=1234)\nvar_init = (4 * rng.random(size=(num_layers, num_variables_per_layer), requires_grad=True) - 2) * np.pi\nprint(var_init)"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n       [[ 5.99038594 -1.50550479  5.31866903 -2.99466132 -2.27329341 -4.79920711\n       -3.24506046 -2.2803699   5.83179179 -2.97006415 -0.74133893  1.38067731]\n       [ 4.56939998  4.5711137   2.1976234   2.00904031  2.96261861 -3.48398028\n       -4.12093786  4.65477183 -5.52746064  2.30830291  2.15184041  1.3950931 ]]\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The NLopt library is used for optimizing the QONN. For using\ngradient-based methods the cost function must be wrapped so that NLopt\ncan access its gradients. This is done by calculating the gradient using\nautograd and then saving it in the ``grad[:]`` variable inside of the\noptimization function. The variables are flattened to conform to the\nrequirements of both NLopt and the above-defined cost function.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["cost_grad = qml.grad(cost)\n\nprint_every = 1\n\n# Wrap the cost so that NLopt can use it for gradient-based optimizations\nevals = 0\ndef cost_wrapper(var, grad=[]):\n    global evals\n    evals += 1\n\n    if grad.size > 0:\n        # Get the gradient for `var` by first \"unflattening\" it\n        var = var.reshape((num_layers, num_variables_per_layer))\n        var = np.array(var, requires_grad=True)\n        var_grad = cost_grad(var, X, Y)\n        grad[:] = var_grad.flatten()\n    cost_val = cost(var.reshape((num_layers, num_variables_per_layer)), X, Y)\n\n    if evals % print_every == 0:\n        print(f\"Iter: {evals:4d}    Cost: {cost_val:.4e}\")\n\n    return float(cost_val)\n\n\n# Choose an algorithm\nopt_algorithm = nlopt.LD_LBFGS  # Gradient-based\n# opt_algorithm = nlopt.LN_BOBYQA  # Gradient-free\n\nopt = nlopt.opt(opt_algorithm, num_layers*num_variables_per_layer)\n\nopt.set_min_objective(cost_wrapper)\n\nopt.set_lower_bounds(-2*np.pi * np.ones(num_layers*num_variables_per_layer))\nopt.set_upper_bounds(2*np.pi * np.ones(num_layers*num_variables_per_layer))\n\nvar = opt.optimize(var_init.flatten())\nvar = var.reshape(var_init.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n       Iter:    1    Cost: 5.2344e-01\n       Iter:    2    Cost: 4.6269e-01\n       Iter:    3    Cost: 3.3963e-01\n       Iter:    4    Cost: 3.0214e-01\n       Iter:    5    Cost: 2.7352e-01\n       Iter:    6    Cost: 1.9481e-01\n       Iter:    7    Cost: 2.6425e-01\n       Iter:    8    Cost: 8.8005e-02\n       Iter:    9    Cost: 1.3520e-01\n       Iter:   10    Cost: 6.9529e-02\n       Iter:   11    Cost: 2.2332e-02\n       Iter:   12    Cost: 5.4051e-03\n       Iter:   13    Cost: 1.7288e-03\n       Iter:   14    Cost: 5.7472e-04\n       Iter:   15    Cost: 2.1946e-04\n       Iter:   16    Cost: 8.5438e-05\n       Iter:   17    Cost: 3.9276e-05\n       Iter:   18    Cost: 1.8697e-05\n       Iter:   19    Cost: 8.7004e-06\n       Iter:   20    Cost: 3.7786e-06\n       Iter:   21    Cost: 1.5192e-06\n       Iter:   22    Cost: 7.0577e-07\n       Iter:   23    Cost: 3.1065e-07\n       Iter:   24    Cost: 1.4212e-07\n       Iter:   25    Cost: 6.3160e-08\n       Iter:   26    Cost: 2.5086e-08\n       Iter:   27    Cost: 1.2039e-08\n       Iter:   28    Cost: 4.6965e-09\n       Iter:   29    Cost: 1.6962e-09\n       Iter:   30    Cost: 6.1205e-10\n       Iter:   31    Cost: 2.4764e-10\n       Iter:   32    Cost: 1.2485e-10\n       Iter:   33    Cost: 8.3915e-11\n       Iter:   34    Cost: 6.1669e-11\n       Iter:   35    Cost: 5.1633e-11\n       Iter:   36    Cost: 4.8152e-11\n       Iter:   37    Cost: 3.9745e-11\n       Iter:   38    Cost: 3.2651e-11\n       Iter:   39    Cost: 1.9693e-11\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\"><h4>Note</h4><p>It\u2019s also possible to use any of PennyLane\u2019s built-in\n    gradient-based optimizers:\n\n    .. code:: python\n\n        from pennylane.optimize import AdamOptimizer\n\n        opt = AdamOptimizer(0.01, beta1=0.9, beta2=0.999)\n\n        var = var_init\n        for it in range(200):\n            var = opt.step(lambda v: cost(v, X, Y), var)\n\n            if (it+1) % 20 == 0:\n                print(f\"Iter: {it+1:5d} | Cost: {cost(var, X, Y):0.7f} \")</p></div>\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, we print the results.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["print(f\"The optimized parameters (layers, parameters):\\n {var}\\n\")\n\nY_pred = np.array([quantum_neural_net(var, x) for x in X])\nfor i, x in enumerate(X):\n    print(f\"{x} --> {Y_pred[i].round(2)}, should be {Y[i]}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n       The optimized parameters (layers, parameters):\n       [[ 5.59646472 -0.76686269  6.28318531 -3.2286718  -1.61696115 -4.79794955\n       -3.44889052 -2.68088816  5.65397191 -2.81207159 -0.59737994  1.39431044]\n       [ 4.71056381  5.24800052  3.14152765  3.13959016  2.78451845 -3.92895253\n       -4.38654718  4.65891554 -5.34964081  2.607051    2.40425267  1.39415476]]\n\n       [1 0 1 0] --> [1. 0. 1. 0.], should be [1 0 1 0]\n       [1 0 0 1] --> [1. 0. 0. 1.], should be [1 0 0 1]\n       [0 1 1 0] --> [0. 1. 0. 1.], should be [0 1 0 1]\n       [0 1 0 1] --> [0. 1. 1. 0.], should be [0 1 1 0]\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also print the circuit to see how the final network looks.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["print(qml.draw(quantum_neural_net)(var_init, X[0]))"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n       0: \u2500\u2500|1\u27e9\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256dBS(5.32,5.83)\u2500\u2500\u2500\u2500R(0.00)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500Kerr(1.57)\u2500\u2500\u2500\u2500\n       1: \u2500\u2500|0\u27e9\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256dBS(-1.51,-2.28)\u2500\u2570BS(5.32,5.83)\u2500\u2500\u2500\u256dBS(-2.27,-0.74)\u2500\u2500R(0.00)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       2: \u2500\u2500|1\u27e9\u2500\u256dBS(5.99,-3.25)\u2500\u2570BS(-1.51,-2.28)\u2500\u256dBS(-2.99,-2.97)\u2500\u2570BS(-2.27,-0.74)\u2500\u256dBS(-4.80,1.38)\n       3: \u2500\u2500|0\u27e9\u2500\u2570BS(5.99,-3.25)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570BS(-2.99,-2.97)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570BS(-4.80,1.38)\n\n       \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256dBS(2.20,-5.53)\u2500\u2500R(0.00)\u2500\u2500\u2500\u2500\u2500\u2500\n       \u2500\u2500\u2500Kerr(1.57)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256dBS(4.57,4.65)\u2500\u2570BS(2.20,-5.53)\u2500\u256dBS(2.96,2.15)\n       \u2500\u2500\u2500R(0.00)\u2500\u2500\u2500\u2500\u2500Kerr(1.57)\u2500\u256dBS(4.57,-4.12)\u2500\u2570BS(4.57,4.65)\u2500\u256dBS(2.01,2.31)\u2500\u2500\u2570BS(2.96,2.15)\n       \u2500\u2500\u2500R(0.00)\u2500\u2500\u2500\u2500\u2500Kerr(1.57)\u2500\u2570BS(4.57,-4.12)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2570BS(2.01,2.31)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n       \u2500\u2500\u2500Kerr(1.57)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  <n>\n       \u2500\u2500\u2500R(0.00)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500Kerr(1.57)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  <n>\n       \u2500\u2500\u256dBS(-3.48,1.40)\u2500\u2500R(0.00)\u2500\u2500\u2500\u2500\u2500Kerr(1.57)\u2500\u2524  <n>\n       \u2500\u2500\u2570BS(-3.48,1.40)\u2500\u2500R(0.00)\u2500\u2500\u2500\u2500\u2500Kerr(1.57)\u2500\u2524  <n>\n\n\n"]}]}