{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n\nAdjoint Differentiation\n=======================\n\n.. meta::\n    :property=\"og:description\": Learn how to use the adjoint method to compute gradients of quantum circuits.\"\n    :property=\"og:image\": https://pennylane.ai/qml/_images/icon.png\n\n.. related::\n\n   tutorial_backprop  Quantum gradients with backpropagation\n   tutorial_quantum_natural_gradient Quantum natural gradient\n   tutorial_general_parshift Generalized parameter-shift rules\n   tutorial_stochastic_parameter_shift The Stochastic Parameter-Shift Rule\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["*Author: PennyLane dev team. Posted: 23 Nov 2021. Last updated: 23 Nov 2021.*\n\n`Classical automatic differentiation <https://en.wikipedia.org/wiki/Automatic_differentiation#The_chain_rule,_forward_and_reverse_accumulation>`__\nhas two methods of calculation: forward and reverse.\nThe optimal choice of method depends on the structure of the problem; is the function \nmany-to-one or one-to-many? We use the properties of the problem to optimize how we \ncalculate derivatives.\n\nMost methods for calculating the derivatives of quantum circuits are either direct applications\nof classical gradient methods to quantum simulations, or quantum hardware methods like parameter-shift\nwhere we can only extract restricted pieces of information.\n\nAdjoint differentiation straddles these two strategies, taking benefits from each.\nOn simulators, we can examine and modify the state vector at any point. At the same time, we know our\nquantum circuit holds specific properties not present in an arbitrary classical computation. \n\nQuantum circuits only involve:\n\n1) initialization,\n\n\\begin{align}|0\\rangle,\\end{align}\n\n2) application of unitary operators,\n\n\\begin{align}|\\Psi\\rangle = U_{n} U_{n-1} \\dots U_0 |0\\rangle,\\end{align}\n\n3) measurement, such as estimating an expectation value of a Hermitian operator,\n\n\\begin{align}\\langle M \\rangle = \\langle \\Psi | M | \\Psi \\rangle.\\end{align}\n\nSince all our operators are unitary, we can easily \"undo\" or \"erase\" them by applying their adjoint:\n\n\\begin{align}U^{\\dagger} U | \\phi \\rangle = |\\phi\\rangle.\\end{align}\n\nThe **adjoint differentiation method** takes advantage of the ability to erase, creating a time- and\nmemory-efficient method for computing quantum gradients on state vector simulators. Tyson Jones and Julien Gacon describe this\nalgorithm in their paper\n`\"Efficient calculation of gradients in classical simulations of variational quantum algorithms\" <https://arxiv.org/abs/2009.02823>`__ .\n\nIn this demo, you will learn how adjoint differentiation works and how to request it\nfor your PennyLane QNode. We will also look at the performance benefits.\n\nTime for some code\n------------------\n\nSo how does it work? Instead of jumping straight to the algorithm, let's explore the above equations\nand their implementation in a bit more detail.\n\nTo start, we import PennyLane and PennyLane's numpy:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import pennylane as qml\nfrom pennylane import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We also need a circuit to simulate:\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev = qml.device('default.qubit', wires=2)\n\nx = np.array([0.1, 0.2, 0.3])\n\n@qml.qnode(dev, diff_method=\"adjoint\")\ndef circuit(a):\n    qml.RX(a[0], wires=0)\n    qml.CNOT(wires=(0,1))\n    qml.RY(a[1], wires=1)\n    qml.RZ(a[2], wires=1)\n    return qml.expval(qml.PauliX(wires=1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The fast c++ simulator device ``\"lightning.qubit\"`` also supports adjoint differentiation,\nbut here we want to quickly prototype a minimal version to illustrate how the algorithm works.\nWe recommend performing\nadjoint differentiation on ``\"lightning.qubit\"`` for substantial performance increases.\n\nWe will use the ``circuit`` QNode just for comparison purposes.  Throughout this\ndemo, we will instead use a list of its operations ``ops`` and a single observable ``M``.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["n_gates = 4\nn_params = 3\n\nops = [\n    qml.RX(x[0], wires=0),\n    qml.CNOT(wires=(0,1)),\n    qml.RY(x[1], wires=1),\n    qml.RZ(x[2], wires=1)\n]\nM = qml.PauliX(wires=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We create our state by using the ``\"default.qubit\"`` methods ``_create_basis_state``\nand ``_apply_operation``.\n\nThese are private methods that you shouldn't typically use and are subject to change\nwithout a deprecation period,\nbut we use them here to illustrate the algorithm.\n\nInternally, the device uses a 2x2x2x... array to represent the state, whereas\nthe measurement ``qml.state()`` and the device attribute ``dev.state``\nflatten this internal representation.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["state = dev._create_basis_state(0)\n\nfor op in ops:\n    state = dev._apply_operation(state, op)\n    \nprint(state)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can think of the expectation $\\langle M \\rangle$ as an inner product between a bra and a ket:\n\n\\begin{align}\\langle M \\rangle = \\langle b | k \\rangle = \\langle \\Psi | M | \\Psi \\rangle,\\end{align}\n\nwhere\n\n\\begin{align}\\langle b | = \\langle \\Psi| M = \\langle 0 | U_0^{\\dagger} \\dots U_n^{\\dagger} M,\\end{align}\n\n\\begin{align}| k \\rangle =  |\\Psi \\rangle = U_n U_{n-1} \\dots U_0 |0\\rangle.\\end{align}\n\nWe could have attached $M$, a Hermitian observable ($M^{\\dagger}=M$), to either the\nbra or the ket, but attaching it to the bra side will be useful later.\n\nUsing the ``state`` calculated above, we can create these $|b\\rangle$ and $|k\\rangle$\nvectors.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["bra = dev._apply_operation(state, M)\nket = state"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we use ``np.vdot`` to take their inner product.  ``np.vdot`` sums over all dimensions\nand takes the complex conjugate of the first input.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["M_expval = np.vdot(bra, ket)\nprint(\"vdot  : \", M_expval)\nprint(\"QNode : \", circuit(x))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We got the same result via both methods! This validates our use of ``vdot`` and\ndevice methods.\n\nBut the dividing line between what makes the \"bra\" and \"ket\" vector is actually\nfairly arbitrary.  We can divide the two vectors at any point from one $\\langle 0 |$\nto the other $|0\\rangle$. For example, we could have used\n\n\\begin{align}\\langle b_n | = \\langle 0 | U_1^{\\dagger} \\dots  U_n^{\\dagger} M U_n,\\end{align}\n\n\\begin{align}|k_n \\rangle = U_{n-1} \\dots U_1 |0\\rangle,\\end{align}\n\nand gotten the exact same results.  Here, the subscript $n$ is used to indicate that $U_n$\nwas moved to the bra side of the expression.  Let's calculate that instead:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["bra_n = dev._create_basis_state(0)\n\nfor op in ops:\n    bra_n = dev._apply_operation(bra_n, op)\nbra_n = dev._apply_operation(bra_n, M)\nbra_n = dev._apply_operation(bra_n, ops[-1].inv())\n\nops[-1].inv() # returning the operation to an uninverted state\n\nket_n = dev._create_basis_state(0)\n\nfor op in ops[:-1]: # don't apply last operation\n    ket_n = dev._apply_operation(ket_n, op)\n\nM_expval_n = np.vdot(bra_n, ket_n)\nprint(M_expval_n)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Same answer!\n\nWe can calculate this in a more efficient way if we already have the\ninitial ``state`` $| \\Psi \\rangle$. To shift the splitting point, we don't\nhave to recalculate everything from scratch. We just remove the operation from\nthe ket and add it to the bra:\n\n\\begin{align}\\langle b_n | = \\langle b | U_n,\\end{align}\n\n\\begin{align}|k_n\\rangle = U_n^{\\dagger} |k\\rangle .\\end{align}\n\nFor the ket vector, you can think of $U_n^{\\dagger}$ as \"eating\" its\ncorresponding unitary from the vector, erasing it from the list of operations.\n\nOf course, we actually work with the conjugate transpose of $\\langle b_n |$,\n\n\\begin{align}|b_n\\rangle = U_n^{\\dagger} | b \\rangle.\\end{align}\n\nOnce we write it in this form, we see that the adjoint of the operation $U_n^{\\dagger}$\noperates on both $|k_n\\rangle$ and $|b_n\\rangle$ to move the splitting point right.\n\nLet's call this the \"version 2\" method.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["bra_n_v2 = dev._apply_operation(state, M)\nket_n_v2 = state\n\nops[-1].inv()\n\nbra_n_v2 = dev._apply_operation(bra_n_v2, ops[-1])\nket_n_v2 = dev._apply_operation(ket_n_v2, ops[-1])\n\nops[-1].inv()\n\nM_expval_n_v2 = np.vdot(bra_n_v2, ket_n_v2)\nprint(M_expval_n_v2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Much simpler!\n\nWe can easily iterate over all the operations to show that the same result occurs\nno matter where you split the operations:\n\n\\begin{align}\\langle b_i | = \\langle b_{i+1}| U_{i},\\end{align}\n\n\\begin{align}|k_{i+1} \\rangle = U_{i} |k_{i}\\rangle.\\end{align}\n\nRewritten, we have our iteration formulas\n\n\\begin{align}| b_i \\rangle = U_i^{\\dagger} |b_{i+1}\\rangle,\\end{align}\n\n\\begin{align}| k_i \\rangle  = U_i^{\\dagger} |k_{i+1}\\rangle.\\end{align}\n\nFor each iteration, we move an operation from the ket side to the bra side.\nWe start near the center at $U_n$ and reverse through the operations list until we reach $U_0$.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["bra_loop = dev._apply_operation(state, M)\nket_loop = state\n\nfor op in reversed(ops):\n    op.inv()\n    bra_loop = dev._apply_operation(bra_loop, op)\n    ket_loop = dev._apply_operation(ket_loop, op)\n    op.inv()\n    print(np.vdot(bra_loop, ket_loop))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally to Derivatives!\n-----------------------\n\nWe showed how to calculate the same thing a bunch of different ways. Why is this useful? \nWherever we cut, we can stick additional things in the middle. What are we sticking in the middle?\nThe derivative of a gate.\n\nFor simplicity's sake, assume each unitary operation $U_i$ is a function of a single\nparameter $\\theta_i$.  For non-parametrized gates like CNOT, we say its derivative is zero.\nWe can also generalize the algorithm to multi-parameter gates, but we leave those out for now.\n\nRemember that each parameter occurs twice in $\\langle M \\rangle$: once in the bra and once in\nthe ket. Therefore, we use the product rule to take the derivative with respect to both locations:\n\n\\begin{align}\\frac{\\partial \\langle M \\rangle}{\\partial \\theta_i} = \n      \\langle 0 | U_1^{\\dagger} \\dots \\frac{\\text{d} U_i^{\\dagger}}{\\text{d} \\theta_i} \\dots M \\dots U_i \\dots U_1 | 0\\rangle.\\end{align}\n\n\n\\begin{align}+ \\langle 0 | U_1^{\\dagger} \\dots U_i^{\\dagger} \\dots M \\dots \\frac{\\text{d} U_i}{\\text{d} \\theta_i}  \\dots U_1 |0\\rangle\\end{align}\n\nWe can now notice that those two components are complex conjugates of each other, so we can\nfurther simplify.  Note that each term is not an expectation value of a Hermitian observable,\nand therefore not guaranteed to be real.\nWhen we add them together, the imaginary part cancels out, and we obtain twice the\nvalue of the real part:\n\n\\begin{align}= 2 \\cdot \\text{Re}\\left( \\langle 0 | U_1^{\\dagger} \\dots U_i^{\\dagger} \\dots M \\dots \\frac{\\text{d} U_i}{\\text{d} \\theta_i}  \\dots U_1 |0\\rangle \\right).\\end{align}\n\nWe can take that formula and break it into its \"bra\" and \"ket\" halves for a derivative at the $i$ th position:\n\n\\begin{align}\\frac{\\partial \\langle M \\rangle }{\\partial \\theta_i } = \n   2 \\text{Re} \\left( \\langle b_i | \\frac{\\text{d} U_i }{\\text{d} \\theta_i} | k_i \\rangle \\right)\\end{align}\n\nwhere\n\n\\begin{align}\\langle b_i | = \\langle 0 | U_1^{\\dagger} \\dots U_n^{\\dagger} M U_n \\dots U_{i+1},\\end{align}\n\n\n.. math :: |k_i \\rangle = U_{i-1} \\dots U_1 |0\\rangle.\n\nNotice that $U_i$ does not appear in either the bra or the ket in the above equations.\nThese formulas differ from the ones we used when just calculating the expectation value.\nFor the actual derivative calculation, we use a temporary version of the bra,\n\n\\begin{align}\\langle \\tilde{b}_i | = \\langle b_i | \\frac{\\text{d} U_i}{\\text{d} \\theta_i},\\end{align}\n\nand use these to get the derivative\n\n\\begin{align}\\frac{\\partial \\langle M \\rangle}{\\partial \\theta_i} = 2 \\text{Re}\\left( \\langle \\tilde{b}_i | k_i \\rangle \\right).\\end{align}\n\nBoth the bra and the ket can be calculated recursively:\n\n\\begin{align}| b_{i} \\rangle = U^{\\dagger}_{i+1} |b_{i+1}\\rangle,\\end{align}\n\n\\begin{align}| k_{i} \\rangle = U^{\\dagger}_{i} |k_{i+1}\\rangle.\\end{align}\n\nWe can iterate through the operations starting at $n$ and ending at $1$.\n\nWe do have to calculate initial state first, the \"forward\" pass:\n\n\\begin{align}|\\Psi\\rangle = U_{n} U_{n-1} \\dots U_0 |0\\rangle.\\end{align}\n\nOnce we have that, we only have about the same amount of work to calculate all the derivatives, \ninstead of quadratically more work.\n\nDerivative of an Operator\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nOne final thing before we get back to coding: how do we get the derivative of an operator?\n\nMost parametrized gates can be represented in terms of Pauli Rotations, which can be written as\n\n\\begin{align}U = e^{i c \\hat{G} \\theta}\\end{align}\n\nfor a Pauli matrix $\\hat{G}$, a constant $c$, and the parameter $\\theta$.\nThus we can easily calculate their derivatives:\n\n\\begin{align}\\frac{\\text{d} U}{\\text{d} \\theta} = i c \\hat{G} e^{i c \\hat{G} \\theta} = i c \\hat{G} U .\\end{align}\n\nLuckily, PennyLane already has a built-in function for calculating this.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["grad_op0 = qml.operation.operation_derivative(ops[0])\nprint(grad_op0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now for calculating the full derivative using the adjoint method!\n\nWe loop over the reversed operations, just as before.  But if the operation has a parameter,\nwe calculate its derivative and append it to a list before moving on. Since the ``operation_derivative``\nfunction spits back out a matrix instead of an operation,\nwe have to use ``dev._apply_unitary`` instead to create $|\\tilde{b}_i\\rangle$.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["bra = dev._apply_operation(state, M)\nket = state\n\ngrads = []\n\nfor op in reversed(ops):\n    op.inv()\n    ket = dev._apply_operation(ket, op)\n    \n    # Calculating the derivative\n    if op.num_params != 0:\n        dU = qml.operation.operation_derivative(op)\n        \n        bra_temp = dev._apply_unitary(bra, dU, op.wires)\n        \n        dM = 2 * np.real(np.vdot(bra_temp, ket))\n        grads.append(dM)\n    \n    bra = dev._apply_operation(bra, op)\n    op.inv()\n\n\n# Finally reverse the order of the gradients\n# since we calculated them in reverse\ngrads = grads[::-1]\n\nprint(\"our calculation: \", grads)\n\ngrad_compare = qml.grad(circuit)(x)\nprint(\"comparison: \", grad_compare)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It matches!!!\n\nIf you want to use adjoint differentiation without having to code up your own\nmethod that can support arbitrary circuits, you can use ``diff_method=\"adjoint\"`` in PennyLane with \n``\"default.qubit\"`` or PennyLane's fast C++ simulator ``\"lightning.qubit\"``.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev_lightning = qml.device('lightning.qubit', wires=2)\n\n@qml.qnode(dev_lightning, diff_method=\"adjoint\")\ndef circuit_adjoint(a):\n    qml.RX(a[0], wires=0)\n    qml.CNOT(wires=(0,1))\n    qml.RY(a[1], wires=1)\n    qml.RZ(a[2], wires=1)\n    return qml.expval(M)\n\nqml.grad(circuit_adjoint)(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Performance\n--------------\n\nThe algorithm gives us the correct answers, but is it worth using? Parameter-shift\ngradients require at least two executions per parameter, so that method gets more\nand more expensive with the size of the circuit, especially on simulators. \nBackpropagation demonstrates decent time scaling, but requires more and more \nmemory as the circuit gets larger.  Simulation of large circuits is already \nRAM-limited, and backpropagation constrains the size of possible circuits even more.\nPennyLane also achieves backpropagation derivatives from a Python simulator and\ninterface-specific functions. The ``\"lightning.qubit\"`` device does not support \nbackpropagation, so backpropagation derivatives lose the speedup from an optimized\nsimulator.\n\nWith adjoint differentiation on ``\"lightning.qubit\"``, you can get the best of both worlds: fast and \nmemory efficient.\n\nBut how fast? The provided script `here <https://pennylane.ai/qml/demos/adjoint_diff_benchmarking.html>`__ \ngenerated the following images on a mid-range laptop. \nThe backpropagation times were produced with the Python simulator ``\"default.qubit\"``, while parameter-shift\nand adjoint differentiation times were calculated with ``\"lightning.qubit\"``.\nThe adjoint method clearly wins out for performance.\n\n.. figure:: ../demonstrations/adjoint_diff/scaling.png\n    :width: 80%\n    :align: center\n\n\nConclusions\n-----------\n\nSo what have we learned? Adjoint differentiation is an efficient method for differentiating\nquantum circuits with state vector simulation.  It scales nicely in time without \nexcessive memory requirements. Now that you've seen how the algorithm works, you can\nbetter understand what is happening when you select adjoint differentiation from one\nof PennyLane's simulators.\n\n\nBibliography\n-------------\n\nJones and Gacon. Efficient calculation of gradients in classical simulations of variational quantum algorithms.\n`https://arxiv.org/abs/2009.02823 <https://arxiv.org/abs/2009.02823>`__\n\nXiu-Zhe Luo, Jin-Guo Liu, Pan Zhang, and Lei Wang. Yao.jl: `Extensible, efficient framework for quantum\nalgorithm design <https://quantum-journal.org/papers/q-2020-10-11-341/>`__ , 2019\n\n\n"]}]}