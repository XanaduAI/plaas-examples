{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nQuantum gradients with backpropagation\n======================================\n\n.. meta::\n    :property=\"og:description\": Using backpropagation can speed up training of quantum circuits compared to the parameter-shift rule\u2014if you are using a simulator.\n\n    :property=\"og:image\": https://pennylane.ai/qml/_images/sphx_glr_tutorial_backprop_002.png\n\n.. related::\n\n   tutorial_quantum_natural_gradient Quantum natural gradient\n\n*Author: PennyLane dev team. Last updated: 31 Jan 2021.*\n\nIn PennyLane, any quantum device, whether a hardware device or a simulator, can be\ntrained using the :doc:`parameter-shift rule </glossary/parameter_shift>` to compute quantum\ngradients. Indeed, the parameter-shift rule is ideally suited to hardware devices, as it does\nnot require any knowledge about the internal workings of the device; it is sufficient to treat\nthe device as a 'black box', and to query it with different input values in order to determine the gradient.\n\nWhen working with simulators, however, we *do* have access to the internal (classical)\ncomputations being performed. This allows us to take advantage of other methods of computing the\ngradient, such as backpropagation, which may be advantageous in certain regimes. In this tutorial,\nwe will compare and contrast the parameter-shift rule against backpropagation, using\nthe PennyLane :class:`default.qubit <pennylane.devices.default_qubit>`\ndevice.\n\nThe parameter-shift rule\n------------------------\n\nThe parameter-shift rule states that, given a variational quantum circuit $U(\\boldsymbol\n\\theta)$ composed of parametrized Pauli rotations, and some measured observable $\\hat{B}$, the\nderivative of the expectation value\n\n\\begin{align}\\langle \\hat{B} \\rangle (\\boldsymbol\\theta) =\n    \\langle 0 \\vert U(\\boldsymbol\\theta)^\\dagger \\hat{B} U(\\boldsymbol\\theta) \\vert 0\\rangle\\end{align}\n\nwith respect to the input circuit parameters $\\boldsymbol{\\theta}$ is given by\n\n\\begin{align}\\nabla_{\\theta_i}\\langle \\hat{B} \\rangle(\\boldsymbol\\theta)\n      =  \\frac{1}{2}\n            \\left[\n                \\langle \\hat{B} \\rangle\\left(\\boldsymbol\\theta + \\frac{\\pi}{2}\\hat{\\mathbf{e}}_i\\right)\n              - \\langle \\hat{B} \\rangle\\left(\\boldsymbol\\theta - \\frac{\\pi}{2}\\hat{\\mathbf{e}}_i\\right)\n            \\right].\\end{align}\n\nThus, the gradient of the expectation value can be calculated by evaluating the same variational\nquantum circuit, but with shifted parameter values (hence the name, parameter-shift rule!).\n\nLet's have a go implementing the parameter-shift rule manually in PennyLane.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import pennylane as qml\nfrom pennylane import numpy as np\nfrom matplotlib import pyplot as plt\n\n# set the random seed\nnp.random.seed(42)\n\n# create a device to execute the circuit on\ndev = qml.device(\"default.qubit\", wires=3)\n\n@qml.qnode(dev, diff_method=\"parameter-shift\")\ndef circuit(params):\n    qml.RX(params[0], wires=0)\n    qml.RY(params[1], wires=1)\n    qml.RZ(params[2], wires=2)\n\n    qml.broadcast(qml.CNOT, wires=[0, 1, 2], pattern=\"ring\")\n\n    qml.RX(params[3], wires=0)\n    qml.RY(params[4], wires=1)\n    qml.RZ(params[5], wires=2)\n\n    qml.broadcast(qml.CNOT, wires=[0, 1, 2], pattern=\"ring\")\n    return qml.expval(qml.PauliY(0) @ qml.PauliZ(2))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's test the variational circuit evaluation with some parameter input:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# initial parameters\nparams = np.random.random([6], requires_grad=True)\n\nprint(\"Parameters:\", params)\nprint(\"Expectation value:\", circuit(params))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also draw the executed quantum circuit:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["fig, ax = qml.draw_mpl(circuit, decimals=2)(params)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we have defined our variational circuit QNode, we can construct\na function that computes the gradient of the $i\\text{th}$ parameter\nusing the parameter-shift rule.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def parameter_shift_term(qnode, params, i):\n    shifted = params.copy()\n    shifted[i] += np.pi/2\n    forward = qnode(shifted)  # forward evaluation\n\n    shifted[i] -= np.pi\n    backward = qnode(shifted) # backward evaluation\n\n    return 0.5 * (forward - backward)\n\n# gradient with respect to the first parameter\nprint(parameter_shift_term(circuit, params, 0))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In order to compute the gradient with respect to *all* parameters, we need\nto loop over the index ``i``:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def parameter_shift(qnode, params):\n    gradients = np.zeros([len(params)])\n\n    for i in range(len(params)):\n        gradients[i] = parameter_shift_term(qnode, params, i)\n\n    return gradients\n\nprint(parameter_shift(circuit, params))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can compare this to PennyLane's *built-in* quantum gradient support by using\nthe :func:`qml.grad <pennylane.grad>` function, which allows us to compute gradients\nof hybrid quantum-classical cost functions. Remember, when we defined the\nQNode, we specified that we wanted it to be differentiable using the parameter-shift\nmethod (``diff_method=\"parameter-shift\"``).\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["grad_function = qml.grad(circuit)\nprint(grad_function(params)[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Alternatively, we can directly compute quantum gradients of QNodes using\nPennyLane's built in :mod:`qml.gradients <pennylane.gradients>` module:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["print(qml.gradients.param_shift(circuit)(params))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If you count the number of quantum evaluations, you will notice that we had to evaluate the circuit\n``2*len(params)`` number of times in order to compute the quantum gradient with respect to all\nparameters. While reasonably fast for a small number of parameters, as the number of parameters in\nour quantum circuit grows, so does both\n\n1. the circuit depth (and thus the time taken to evaluate each expectation value or 'forward' pass), and\n\n2. the number of parameter-shift evaluations required.\n\nBoth of these factors increase the time taken to compute the gradient with\nrespect to all parameters.\n\nBenchmarking\n~~~~~~~~~~~~\n\nLet's consider an example with a significantly larger number of parameters.\nWe'll make use of the :class:`~pennylane.StronglyEntanglingLayers` template\nto make a more complicated QNode.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev = qml.device(\"default.qubit\", wires=4)\n\n@qml.qnode(dev, diff_method=\"parameter-shift\")\ndef circuit(params):\n    qml.StronglyEntanglingLayers(params, wires=[0, 1, 2, 3])\n    return qml.expval(qml.PauliZ(0) @ qml.PauliZ(1) @ qml.PauliZ(2) @ qml.PauliZ(3))\n\n# initialize circuit parameters\nparam_shape = qml.StronglyEntanglingLayers.shape(n_wires=4, n_layers=15)\nparams = np.random.normal(scale=0.1, size=param_shape, requires_grad=True)\nprint(params.size)\nprint(circuit(params))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This circuit has 180 parameters. Let's see how long it takes to perform a forward\npass of the circuit.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import timeit\n\nreps = 3\nnum = 10\ntimes = timeit.repeat(\"circuit(params)\", globals=globals(), number=num, repeat=reps)\nforward_time = min(times) / num\n\nprint(f\"Forward pass (best of {reps}): {forward_time} sec per loop\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can now estimate the time taken to compute the full gradient vector,\nand see how this compares.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# create the gradient function\ngrad_fn = qml.grad(circuit)\n\ntimes = timeit.repeat(\"grad_fn(params)\", globals=globals(), number=num, repeat=reps)\nbackward_time = min(times) / num\n\nprint(f\"Gradient computation (best of {reps}): {backward_time} sec per loop\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Based on the parameter-shift rule, we expect that the amount of time to compute the quantum\ngradients should be approximately $2p\\Delta t_{f}$ where $p$ is the number of\nparameters and $\\Delta t_{f}$ if the time taken for the forward pass. Let's verify this:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["print(2 * forward_time * params.size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Backpropagation\n---------------\n\nAn alternative to the parameter-shift rule for computing gradients is\n`reverse-mode autodifferentiation <https://en.wikipedia.org/wiki/Reverse_accumulation>`__.\nUnlike the parameter-shift method, which requires $2p$ circuit evaluations for\n$p$ parameters, reverse-mode requires only a *single* forward pass of the\ndifferentiable function to compute\nthe gradient of all variables, at the expense of increased memory usage.\nDuring the forward pass, the results of all intermediate subexpressions are stored;\nthe computation is then traversed *in reverse*, with the gradient computed by repeatedly\napplying the chain rule.\nIn most classical machine learning settings (where we are training scalar loss functions\nconsisting of a large number of parameters),\nreverse-mode autodifferentiation is the\npreferred method of autodifferentiation---the reduction in computational time enables larger and\nmore complex models to be successfully trained. The backpropagation algorithm is a particular\nspecial-case of reverse-mode autodifferentiation, which has helped lead to the machine learning\nexplosion we see today.\n\nIn quantum machine learning, however, the inability to store and utilize the results of\n*intermediate* quantum operations on hardware remains a barrier to using backprop;\nwhile reverse-mode\nautodifferentiation works fine for small quantum simulations, only the\nparameter-shift rule can be used to compute gradients on quantum hardware directly. Nevertheless,\nwhen training quantum models via classical simulation, it's useful to explore the regimes where\nreverse-mode differentiation may be a better choice than the parameter-shift rule.\n\nBenchmarking\n~~~~~~~~~~~~\n\nWhen creating a QNode, :doc:`PennyLane supports various methods of differentiation\n<code/api/pennylane.qnode>`, including ``\"parameter-shift\"`` (which we used previously),\n``\"finite-diff\"``, ``\"reversible\"``, and ``\"backprop\"``. While ``\"parameter-shift\"`` works with all devices\n(simulator or hardware), ``\"backprop\"`` will only work for specific simulator devices that are\ndesigned to support backpropagation.\n\nOne such device is :class:`default.qubit <pennylane.devices.DefaultQubit>`. It\nhas backends written using TensorFlow, JAX, and Autograd, so when used with the\nTensorFlow, JAX, and Autograd interfaces respectively, supports backpropagation.\nIn this demo, we will use the default Autograd interface.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev = qml.device(\"default.qubit\", wires=4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When defining the QNode, we specify ``diff_method=\"backprop\"`` to ensure that\nwe are using backpropagation mode. Note that this is the *default differentiation\nmode* for the ``default.qubit`` device.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["@qml.qnode(dev, diff_method=\"backprop\")\ndef circuit(params):\n    qml.StronglyEntanglingLayers(params, wires=[0, 1, 2, 3])\n    return qml.expval(qml.PauliZ(0) @ qml.PauliZ(1) @ qml.PauliZ(2) @ qml.PauliZ(3))\n\n# initialize circuit parameters\nparam_shape = qml.StronglyEntanglingLayers.shape(n_wires=4, n_layers=15)\nparams = np.random.normal(scale=0.1, size=param_shape, requires_grad=True)\nprint(circuit(params))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's see how long it takes to perform a forward pass of the circuit.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import timeit\n\nreps = 3\nnum = 10\ntimes = timeit.repeat(\"circuit(params)\", globals=globals(), number=num, repeat=reps)\nforward_time = min(times) / num\nprint(f\"Forward pass (best of {reps}): {forward_time} sec per loop\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comparing this to the forward pass from ``default.qubit``, we note that there is some potential\noverhead from using backpropagation. We can now time how long it takes to perform a\ngradient computation via backpropagation:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["times = timeit.repeat(\"qml.grad(circuit)(params)\", globals=globals(), number=num, repeat=reps)\nbackward_time = min(times) / num\nprint(f\"Backward pass (best of {reps}): {backward_time} sec per loop\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Unlike with the parameter-shift rule, the time taken to perform the backwards pass appears\nof the order of a single forward pass! The can significantly speed up training of simulated\ncircuits with many parameters.\n\nTime comparison\n---------------\n\nLet's compare the two differentiation approaches as the number of trainable parameters\nin the variational circuit increases, by timing both the forward pass and the gradient\ncomputation as the number of layers is allowed to increase.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev = qml.device(\"default.qubit\", wires=4)\n\ndef circuit(params):\n    qml.StronglyEntanglingLayers(params, wires=[0, 1, 2, 3])\n    return qml.expval(qml.PauliZ(0) @ qml.PauliZ(1) @ qml.PauliZ(2) @ qml.PauliZ(3))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll continue to use the same ansatz as before, but to reduce the time taken\nto collect the data, we'll reduce the number and repetitions of timings per data\npoint. Below, we loop over a variational circuit depth ranging from 0 (no gates/\ntrainable parameters) to 20. Each layer will contain $3N$ parameters, where\n$N$ is the number of wires (in this case, we have $N=4$).\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["reps = 2\nnum = 3\n\nforward_shift = []\ngradient_shift = []\nforward_backprop = []\ngradient_backprop = []\n\nfor depth in range(0, 21):\n    param_shape = qml.StronglyEntanglingLayers.shape(n_wires=4, n_layers=depth)\n    params = np.random.normal(scale=0.1, size=param_shape, requires_grad=True)\n    num_params = params.size\n\n    # forward pass timing\n    # ===================\n\n    qnode_shift = qml.QNode(circuit, dev, diff_method=\"parameter-shift\")\n    qnode_backprop = qml.QNode(circuit, dev, diff_method=\"backprop\")\n\n    # parameter-shift\n    t = timeit.repeat(\"qnode_shift(params)\", globals=globals(), number=num, repeat=reps)\n    forward_shift.append([num_params, min(t) / num])\n\n    # backprop\n    t = timeit.repeat(\"qnode_backprop(params)\", globals=globals(), number=num, repeat=reps)\n    forward_backprop.append([num_params, min(t) / num])\n\n    if num_params == 0:\n        continue\n\n    # Gradient timing\n    # ===============\n\n    qnode_shift = qml.QNode(circuit, dev, diff_method=\"parameter-shift\")\n    qnode_backprop = qml.QNode(circuit, dev, diff_method=\"backprop\")\n\n    # parameter-shift\n    t = timeit.repeat(\"qml.grad(qnode_shift)(params)\", globals=globals(), number=num, repeat=reps)\n    gradient_shift.append([num_params, min(t) / num])\n\n    # backprop\n    t = timeit.repeat(\"qml.grad(qnode_backprop)(params)\", globals=globals(), number=num, repeat=reps)\n    gradient_backprop.append([num_params, min(t) / num])\n\ngradient_shift = np.array(gradient_shift).T\ngradient_backprop = np.array(gradient_backprop).T\nforward_shift = np.array(forward_shift).T\nforward_backprop = np.array(forward_backprop).T"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We now import matplotlib, and plot the results.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.style.use(\"bmh\")\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(*gradient_shift, '.-', label=\"Parameter-shift\")\nax.plot(*gradient_backprop, '.-', label=\"Backprop\")\nax.set_ylabel(\"Time (s)\")\nax.set_xlabel(\"Number of parameters\")\nax.legend()\n\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. raw:: html\n\n    <br>\n\nWe can see that the computational time for the parameter-shift rule increases with\nincreasing number of parameters, as expected, whereas the computational time\nfor backpropagation appears much more constant, with perhaps a minute linear increase\nwith $p$. Note that the plots are not perfectly linear, with some 'bumpiness' or\nnoisiness. This is likely due to low-level operating system jitter, and\nother environmental fluctuations---increasing the number of repeats can help smooth\nout the plot.\n\nFor a better comparison, we can scale the time required for computing the quantum\ngradients against the time taken for the corresponding forward pass:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["gradient_shift[1] /= forward_shift[1, 1:]\ngradient_backprop[1] /= forward_backprop[1, 1:]\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(*gradient_shift, '.-', label=\"Parameter-shift\")\nax.plot(*gradient_backprop, '.-', label=\"Backprop\")\n\n# perform a least squares regression to determine the linear best fit/gradient\n# for the normalized time vs. number of parameters\nx = gradient_shift[0]\nm_shift, c_shift = np.polyfit(*gradient_shift, deg=1)\nm_back, c_back = np.polyfit(*gradient_backprop, deg=1)\n\nax.plot(x, m_shift * x + c_shift, '--', label=f\"{m_shift:.2f}p{c_shift:+.2f}\")\nax.plot(x, m_back * x + c_back, '--', label=f\"{m_back:.2f}p{c_back:+.2f}\")\n\nax.set_ylabel(\"Normalized time\")\nax.set_xlabel(\"Number of parameters\")\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")\nax.legend()\n\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. raw:: html\n\n    <br>\n\nWe can now see clearly that there is constant overhead for backpropagation with\n``default.qubit``, but the parameter-shift rule scales as $\\sim 2p$.\n\n"]}]}