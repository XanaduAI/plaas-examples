{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n\n# Quantum GANs\n\n.. meta::\n    :property=\"og:description\": Explore quantum GANs to generate hand-written digits of zero\n    :property=\"og:image\": https://pennylane.ai/qml/_images/patch.jpeg\n\n.. related::\n    tutorial_QGAN QGANs with Cirq + TensorFlow\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["*Author: James Ellis. Contact: jamesellis9570@yahoo.co.uk. Last updated:\n27 Jan 2022.*\n\nIn this tutorial, we will explore quantum GANs to generate hand-written\ndigits of zero. We will first cover the theory of the classical case,\nthen extend to a quantum method recently proposed in the literature. If\nyou have no experience with GANs, particularly in PyTorch, you might\nfind `PyTorch's\ntutorial <https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html>`__\nuseful since it serves as the foundation for what is to follow.\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Generative Adversarial Networks (GANs)\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The goal of generative adversarial networks (GANs) [1] is to generate\ndata that resembles the original data used in training. To achieve this,\nwe train two neural networks simulatenously: a generator and a\ndiscriminator. The job of the generator is to create fake data which\nimitates the real training dataset. On the otherhand, the discriminator\nacts like a detective trying to discern real from fake data. During the\ntraining process, both players iteratively improve with one another. By\nthe end, the generator should hopefully generate new data very similar\nto the training dataset.\n\nSpecifically, the training dataset represents samples drawn from some\nunknown data distribution $P_{data}$, and the generator has the\njob of trying to capture this distribution. The generator, $G$,\nstarts from some initial latent distribution, $P_z$, and maps it\nto $P_g = G(P_z)$. The best solution would be for\n$P_g = P_{data}$. However, this point is rarely achieved in\npractice apart from in the most simple tasks.\n\nBoth the discriminator, $D$, and generator, $G$, play in a\n2-player minimax game. The discriminator tries to maximise the\nprobability of discerning real from fake data, while the generator tries\nto minimise the same probability. The value function for the game is\nsummarised by,\n\n\\begin{align}\\begin{align}\n   \\min_G \\max_D V(D,G) &= \\mathbb{E}_{\\boldsymbol{x}\\sim p_{data}}[\\log D(\\boldsymbol{x})] \\\\\n       & ~~ + \\mathbb{E}_{\\boldsymbol{z}\\sim p_{\\boldsymbol{z}}}[\\log(1 - D(G(\\boldsymbol{z}))]\n   \\end{align}\\end{align}\n\n-  $\\boldsymbol{x}$: real data sample\n-  $\\boldsymbol{z}$: latent vector\n-  $D(\\boldsymbol{x})$: probability of the discriminator\n   classifying real data as real\n-  $G(\\boldsymbol{z})$: fake data\n-  $D(G(\\boldsymbol{z}))$: probability of discriminator\n   classifying fake data as real\n\nIn practice, the two networks are trained iteratively, each with\na separate loss function to be minimised,\n\n\\begin{align}L_D = -[y \\cdot \\log(D(x)) + (1-y)\\cdot \\log(1-D(G(z)))]\\end{align}\n\n\\begin{align}L_G = [(1-y) \\cdot \\log(1-D(G(z)))]\\end{align}\n\nwhere $y$ is a binary label for real ($y=1$) or fake\n($y=0$) data. In practice, generator training is shown to be more\nstable [1] when made to maximise $\\log(D(G(z)))$ instead of\nminimising $\\log(1-D(G(z)))$. Hence, the generator loss function to\nbe minimised becomes,\n\n\\begin{align}L_G = -[(1-y) \\cdot \\log(D(G(z)))]\\end{align}\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Quantum GANs: The Patch Method\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this tutorial, we re-create one of the quantum GAN methods presented\nby Huang et al.[2]: the patch method. This method uses several quantum\ngenerators, with each sub-generator, $G^{(i)}$, responsible for\nconstructing a small patch of the final image. The final image is\ncontructed by concatenting all of the patches together as shown below.\n\n.. figure:: ../demonstrations/quantum_gans/patch.jpeg\n  :width: 90% \n  :alt: quantum_patch_method\n  :align: center\n\nThe main advantage of this method is that it is particulary suited to\nsituations where the number of available qubits are limited. The same\nquantum device can be used for each sub-generator in an iterative\nfashion, or execution of the generators can be parallelised across\nmultiple devices.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In this tutorial, parenthesised superscripts are used to denote\n   individual objects as part of a collection.</p></div>\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Module Imports\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# Library imports\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport pennylane as qml\n\n# Pytorch imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set the random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Data\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As mentioned in the introduction, we will use a `small\ndataset <https://archive.ics.uci.edu/ml/datasets/optical+recognition+of+handwritten+digits>`__\nof handwritten zeros. First, we need to create a custom dataloader for\nthis dataset.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["class DigitsDataset(Dataset):\n    \"\"\"Pytorch dataloader for the Optical Recognition of Handwritten Digits Data Set\"\"\"\n\n    def __init__(self, csv_file, label=0, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.csv_file = csv_file\n        self.transform = transform\n        self.df = self.filter_by_label(label)\n\n    def filter_by_label(self, label):\n        # Use pandas to return a dataframe of only zeros\n        df = pd.read_csv(self.csv_file)\n        df = df.loc[df.iloc[:, -1] == label]\n        return df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        image = self.df.iloc[idx, :-1] / 16\n        image = np.array(image)\n        image = image.astype(np.float32).reshape(8, 8)\n\n        if self.transform:\n            image = self.transform(image)\n\n        # Return image and label\n        return image, 0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next we define some variables and create the dataloader instance.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["image_size = 8  # Height / width of the square images\nbatch_size = 1\n\ntransform = transforms.Compose([transforms.ToTensor()])\ndataset = DigitsDataset(csv_file=\"quantum_gans/optdigits.tra\", transform=transform)\ndataloader = torch.utils.data.DataLoader(\n    dataset, batch_size=batch_size, shuffle=True, drop_last=True\n)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's visualize some of the data.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.figure(figsize=(8,2))\n\nfor i in range(8):\n    image = dataset[i][0].reshape(image_size,image_size)\n    plt.subplot(1,8,i+1)\n    plt.axis('off')\n    plt.imshow(image.numpy(), cmap='gray')\n    \nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Implementing the Discriminator\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For the discriminator, we use a fully connected neural network with two\nhidden layers. A single output is sufficient to represent the\nprobability of an input being classified as real.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["class Discriminator(nn.Module):\n    \"\"\"Fully connected classical discriminator\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        self.model = nn.Sequential(\n            # Inputs to first hidden layer (num_input_features -> 64)\n            nn.Linear(image_size * image_size, 64),\n            nn.ReLU(),\n            # First hidden layer (64 -> 16)\n            nn.Linear(64, 16),\n            nn.ReLU(),\n            # Second hidden layer (16 -> output)\n            nn.Linear(16, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.model(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Implementing the Generator\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Each sub-generator, $G^{(i)}$, shares the same circuit\narchitecture as shown below. The overall quantum generator consists of\n$N_G$ sub-generators, each consisting of $N$ qubits. The\nprocess from latent vector input to image output can be split into four\ndistinct sections: state embedding, parameterisation, non-linear\ntransformation, and post-processing. Each of the following sections\nbelow refer to a single iteration of the training process to simplify\nthe discussion.\n\n.. figure:: ../demonstrations/quantum_gans/qcircuit.jpeg\n  :width: 90% \n  :alt: quantum_circuit\n  :align: center\n\n**1) State Embedding**\n\n\nA latent vector, $\\boldsymbol{z}\\in\\mathbb{R}^N$, is sampled from\na uniform distribution in the interval $[0,\\pi/2)$. All\nsub-generators receive the same latent vector which is then embedded\nusing RY gates.\n\n**2) Parameterised Layers**\n\n\nThe parameterised layer consists of parameterised RY gates followed by\ncontrol Z gates. This layer is repeated $D$ times in total.\n\n**3) Non-Linear Transform**\n\n\nQuantum gates in the circuit model are unitary which, by definition,\nlinearly transform the quantum state. A linear mapping between the\nlatent and generator distribution would be suffice for only the most\nsimple generative tasks, hence we need non-linear transformations. We\nwill use ancillary qubits to help.\n\nFor a given sub-generator, the pre-measurement quantum state is given\nby,\n\n\\begin{align}|\\Psi(z)\\rangle = U_{G}(\\theta)|\\boldsymbol{z}\\rangle\\end{align}\n\nwhere $U_{G}(\\theta)$ represents the overall unitary of the\nparameterised layers. Let us inspect the state when we take a partial\nmeasurment, $\\Pi$, and trace out the ancillary subsystem,\n$\\mathcal{A}$,\n\n\\begin{align}\\rho(\\boldsymbol{z}) = \\frac{\\text{Tr}_{\\mathcal{A}}(\\Pi \\otimes \\mathbb{I} |\\Psi(z)\\rangle \\langle \\Psi(\\boldsymbol{z})|) }{\\text{Tr}(\\Pi \\otimes \\mathbb{I} |\\Psi(\\boldsymbol{z})\\rangle \\langle \\Psi(\\boldsymbol{z})|))} = \\frac{\\text{Tr}_{\\mathcal{A}}(\\Pi \\otimes \\mathbb{I} |\\Psi(\\boldsymbol{z})\\rangle \\langle \\Psi(\\boldsymbol{z})|) }{\\langle \\Psi(\\boldsymbol{z})| \\Pi \\otimes \\mathbb{I} |\\Psi(\\boldsymbol{z})\\rangle}\\end{align}\n\nThe post-measurement state, $\\rho(\\boldsymbol{z})$, is dependent\non $\\boldsymbol{z}$ in both the numerator and denominator. This\nmeans the state has been non-linearly transformed! For this tutorial,\n$\\Pi = (|0\\rangle \\langle0|)^{\\otimes N_A}$, where $N_A$ \nis the number of ancillary qubits in the system.\n\nWith the remaining data qubits, we measure the probability of\n$\\rho(\\boldsymbol{z})$ in each computational basis state,\n$P(j)$, to obtain the sub-generator output,\n$\\boldsymbol{g}^{(i)}$,\n\n\\begin{align}\\boldsymbol{g}^{(i)} = [P(0), P(1), ... ,P(2^{N-N_A} - 1)]\\end{align}\n\n**4) Post Processing**\n\n\nDue to the normalisation constraint of the measurment, all elements in \n$\\boldsymbol{g}^{(i)}$ must sum to one. This is\na problem if we are to use $\\boldsymbol{g}^{(i)}$ as the pixel\nintensity values for our patch. For example, imagine a hypothetical\nsituation where a patch of full intensity pixels was the target. The\nbest patch a sub-generator could produce would be a patch of pixels all\nat a magnitude of $\\frac{1}{2^{N-N_A}}$. To alleviate this\nconstraint, we apply a post-processing technique to each patch,\n\n\\begin{align}\\boldsymbol{\\tilde{x}^{(i)}} = \\frac{\\boldsymbol{g}^{(i)}}{\\max_{k}\\boldsymbol{g}_k^{(i)}}\\end{align}\n\nTherefore, the final image, $\\boldsymbol{\\tilde{x}}$, is given by\n\n\\begin{align}\\boldsymbol{\\tilde{x}} = [\\boldsymbol{\\tilde{x}^{(1)}}, ... ,\\boldsymbol{\\tilde{x}^{(N_G)}}]\\end{align}\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# Quantum variables\nn_qubits = 5  # Total number of qubits / N\nn_a_qubits = 1  # Number of ancillary qubits / N_A\nq_depth = 6  # Depth of the parameterised quantum circuit / D\nn_generators = 4  # Number of subgenerators for the patch method / N_G"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we define the quantum device we want to use, along with any\navailable CUDA GPUs (if available).\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# Quantum simulator\ndev = qml.device(\"lightning.qubit\", wires=n_qubits)\n# Enable CUDA device if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, we define the quantum circuit and measurement process described above.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["@qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\ndef quantum_circuit(noise, weights):\n\n    weights = weights.reshape(q_depth, n_qubits)\n\n    # Initialise latent vectors\n    for i in range(n_qubits):\n        qml.RY(noise[i], wires=i)\n\n    # Repeated layer\n    for i in range(q_depth):\n        # Parameterised layer\n        for y in range(n_qubits):\n            qml.RY(weights[i][y], wires=y)\n\n        # Control Z gates\n        for y in range(n_qubits - 1):\n            qml.CZ(wires=[y, y + 1])\n\n    return qml.probs(wires=list(range(n_qubits)))\n\n\n# For further info on how the non-linear transform is implemented in Pennylane\n# https://discuss.pennylane.ai/t/ancillary-subsystem-measurement-then-trace-out/1532\ndef partial_measure(noise, weights):\n    # Non-linear Transform\n    probs = quantum_circuit(noise, weights)\n    probsgiven0 = probs[: (2 ** (n_qubits - n_a_qubits))]\n    probsgiven0 /= torch.sum(probs)\n\n    # Post-Processing\n    probsgiven = probsgiven0 / torch.max(probsgiven0)\n    return probsgiven"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we create a quantum generator class to use during training.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["class PatchQuantumGenerator(nn.Module):\n    \"\"\"Quantum generator class for the patch method\"\"\"\n\n    def __init__(self, n_generators, q_delta=1):\n        \"\"\"\n        Args:\n            n_generators (int): Number of sub-generators to be used in the patch method.\n            q_delta (float, optional): Spread of the random distribution for parameter initialisation.\n        \"\"\"\n\n        super().__init__()\n\n        self.q_params = nn.ParameterList(\n            [\n                nn.Parameter(q_delta * torch.rand(q_depth * n_qubits), requires_grad=True)\n                for _ in range(n_generators)\n            ]\n        )\n        self.n_generators = n_generators\n\n    def forward(self, x):\n        # Size of each sub-generator output\n        patch_size = 2 ** (n_qubits - n_a_qubits)\n\n        # Create a Tensor to 'catch' a batch of images from the for loop. x.size(0) is the batch size.\n        images = torch.Tensor(x.size(0), 0).to(device)\n\n        # Iterate over all sub-generators\n        for params in self.q_params:\n\n            # Create a Tensor to 'catch' a batch of the patches from a single sub-generator\n            patches = torch.Tensor(0, patch_size).to(device)\n            for elem in x:\n                q_out = partial_measure(elem, params).float().unsqueeze(0)\n                patches = torch.cat((patches, q_out))\n\n            # Each batch of patches is concatenated with each other to create a batch of images\n            images = torch.cat((images, patches), 1)\n\n        return images"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's define learning rates and number of iterations for the training process.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["lrG = 0.3  # Learning rate for the generator\nlrD = 0.01  # Learning rate for the discriminator\nnum_iter = 500  # Number of training iterations"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now putting everything together and executing the training process.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["discriminator = Discriminator().to(device)\ngenerator = PatchQuantumGenerator(n_generators).to(device)\n\n# Binary cross entropy\ncriterion = nn.BCELoss()\n\n# Optimisers\noptD = optim.SGD(discriminator.parameters(), lr=lrD)\noptG = optim.SGD(generator.parameters(), lr=lrG)\n\nreal_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\nfake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\n\n# Fixed noise allows us to visually track the generated images throughout training\nfixed_noise = torch.rand(8, n_qubits, device=device) * math.pi / 2\n\n# Iteration counter\ncounter = 0\n\n# Collect images for plotting later\nresults = []\n\nwhile True:\n    for i, (data, _) in enumerate(dataloader):\n\n        # Data for training the discriminator\n        data = data.reshape(-1, image_size * image_size)\n        real_data = data.to(device)\n\n        # Noise follwing a uniform distribution in range [0,pi/2)\n        noise = torch.rand(batch_size, n_qubits, device=device) * math.pi / 2\n        fake_data = generator(noise)\n\n        # Training the discriminator\n        discriminator.zero_grad()\n        outD_real = discriminator(real_data).view(-1)\n        outD_fake = discriminator(fake_data.detach()).view(-1)\n\n        errD_real = criterion(outD_real, real_labels)\n        errD_fake = criterion(outD_fake, fake_labels)\n        # Propagate gradients\n        errD_real.backward()\n        errD_fake.backward()\n\n        errD = errD_real + errD_fake\n        optD.step()\n\n        # Training the generator\n        generator.zero_grad()\n        outD_fake = discriminator(fake_data).view(-1)\n        errG = criterion(outD_fake, real_labels)\n        errG.backward()\n        optG.step()\n\n        counter += 1\n\n        # Show loss values         \n        if counter % 10 == 0:\n            print(f'Iteration: {counter}, Discriminator Loss: {errD:0.3f}, Generator Loss: {errG:0.3f}')\n            test_images = generator(fixed_noise).view(8,1,image_size,image_size).cpu().detach()\n            \n            # Save images every 50 iterations\n            if counter % 50 == 0:\n                results.append(test_images)\n\n        if counter == num_iter:\n            break\n    if counter == num_iter:\n        break"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\"><h4>Note</h4><p>You may have noticed ``errG = criterion(outD_fake, real_labels)`` and\n    wondered why we don\u2019t use ``fake_labels`` instead of ``real_labels``.\n    However, this is simply a trick to be able to use the same\n    ``criterion`` function for both the generator and discriminator.\n    Using ``real_labels`` forces the generator loss function to use the\n    $\\log(D(G(z))$ term instead of the $\\log(1 - D(G(z))$ term\n    of the binary cross entropy loss function.</p></div>\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, we plot how the generated images evolved throughout training.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["fig = plt.figure(figsize=(10, 5))\nouter = gridspec.GridSpec(5, 2, wspace=0.1)\n\nfor i, images in enumerate(results):\n    inner = gridspec.GridSpecFromSubplotSpec(1, images.size(0),\n                    subplot_spec=outer[i])\n    \n    images = torch.squeeze(images, dim=1)\n    for j, im in enumerate(images):\n\n        ax = plt.Subplot(fig, inner[j])\n        ax.imshow(im.numpy(), cmap=\"gray\")\n        ax.set_xticks([])\n        ax.set_yticks([])\n        if j==0:\n            ax.set_title(f'Iteration {50+i*50}', loc='left')\n        fig.add_subplot(ax)\n\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Acknowledgements\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Many thanks to Karolis \u0160pukas who I co-developed much of the code with.\nI also extend my thanks to Dr.\u00a0Yuxuan Du for answering my questions\nregarding his paper. I am also indebited to the Pennylane community for\ntheir help over the past few years.\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## References\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["[1] Ian J. Goodfellow et al.\u00a0*Generative Adversarial Networks*.\n`arXiv:1406.2661 <https://arxiv.org/abs/1406.2661>`__ (2014).\n\n[2] He-Liang Huang et al.\u00a0*Experimental Quantum Generative Adversarial\nNetworks for Image Generation*. \n`arXiv:2010.06201 <https://arxiv.org/abs/2010.06201>`__ (2020).\n\n\n"]}], "metadata": {"kernelspec": {"display_name": "PennyLane", "language": "python", "name": "pennylane"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.10"}}, "nbformat": 4, "nbformat_minor": 0}