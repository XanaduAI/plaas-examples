{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n\n# Quanvolutional Neural Networks\n\n.. meta::\n    :property=\"og:description\": Train a quantum convolutional neural network\n        to classify MNIST images.\n    :property=\"og:image\": https://pennylane.ai/qml/_images/circuit.png\n\n*Author: Andrea Mari. Last updated: 15 Jan 2021.*\n\nIn this demo we implement the *Quanvolutional Neural Network*, a quantum\nmachine learning model originally introduced in\n`Henderson et al. (2019) <https://arxiv.org/abs/1904.04767>`_.\n\n.. figure:: ../demonstrations/quanvolution/circuit.png\n    :align: center\n    :width: 90%\n    :target: javascript:void(0)\n\n## Introduction\n\n### Classical convolution\nThe *convolutional neural network* (CNN) is a standard model in classical machine learning which is particularly\nsuitable for processing images.\nThe model is based on the idea of a *convolution layer* where, instead of processing the full input data with a global function,\na local convolution is applied.\n\nIf the input is an image, small local regions are sequentially processed with the same kernel. The results obtained for each region are usually associated to different channels\nof a single output pixel. The union of all the output pixels produces a new image-like object, which can be further processed by\nadditional layers.\n\n\n### Quantum convolution\nOne can extend the same idea also to the context of quantum variational circuits. A possible approach is given\nby the following procedure which is very similar to the one used in Ref. [1]. The scheme is also represented in the\nfigure at the top of this tutorial.\n\n\n1.  A small region of the input image, in our example a $2 \\times 2$ square, is embedded into a quantum circuit.\n    In this demo, this is achieved with parametrized rotations applied to the qubits initialized in the ground state.\n\n2.  A quantum computation, associated to a unitary $U$, is performed on the system.\n    The unitary could be generated by a variational quantum circuit or, more simply, by a random circuit as\n    proposed in Ref. [1].\n\n3.  The quantum system is finally measured, obtaining a list of classical expectation values.\n    The measurement results could also be classically post-processed as proposed in Ref. [1] but, for simplicity, in this\n    demo we directly use the raw expectation values.\n\n4.  Analogously to a classical convolution layer, each expectation value is mapped to a different channel of a\n    single output pixel.\n\n5.  Iterating the same procedure over different regions, one can scan the full input image,\n    producing an output object which will be structured as a multi-channel image.\n\n6.  The quantum convolution can be followed by further quantum layers or by classical layers.\n\n\nThe main difference with respect to a classical convolution is that a quantum circuit can\ngenerate highly complex kernels whose computation could be, at least in principle, classically intractable.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In this tutorial we follow the approach of Ref. [1] in which a fixed non-trainable quantum\n    circuit is used as a \"quanvolution\" kernel, while the subsequent classical layers\n    are trained for the classification problem of interest.\n    However, by leveraging the ability of PennyLane to evaluate gradients of\n    quantum circuits, the quantum kernel could also be trained.</p></div>\n\n\n## General setup\nThis Python code requires *PennyLane* with the *TensorFlow* interface and the plotting library *matplotlib*.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane.templates import RandomLayers\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Setting of the main hyper-parameters of the model\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["n_epochs = 30   # Number of optimization epochs\nn_layers = 1    # Number of random layers\nn_train = 50    # Size of the train dataset\nn_test = 30     # Size of the test dataset\n\nSAVE_PATH = \"quanvolution/\" # Data saving folder\nPREPROCESS = True           # If False, skip quantum processing and load data from SAVE_PATH\nnp.random.seed(0)           # Seed for NumPy random number generator\ntf.random.set_seed(0)       # Seed for TensorFlow random number generator"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Loading of the MNIST dataset\nWe import the MNIST dataset from *Keras*. To speedup the evaluation of this demo\nwe use only a small number of training and test images. Obviously, better\nresults are achievable when using the full dataset.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["mnist_dataset = keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n\n# Reduce dataset size\ntrain_images = train_images[:n_train]\ntrain_labels = train_labels[:n_train]\ntest_images = test_images[:n_test]\ntest_labels = test_labels[:n_test]\n\n# Normalize pixel values within 0 and 1\ntrain_images = train_images / 255\ntest_images = test_images / 255\n\n# Add extra dimension for convolution channels\ntrain_images = np.array(train_images[..., tf.newaxis], requires_grad=False)\ntest_images = np.array(test_images[..., tf.newaxis], requires_grad=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Quantum circuit as a convolution kernel\n\nWe follow the scheme described in the introduction and represented in the figure at the top\nof this demo.\n\nWe initialize a PennyLane ``default.qubit`` device, simulating a system of $4$ qubits.\nThe associated ``qnode`` represents the quantum circuit consisting of:\n\n1. an embedding layer of local $R_y$ rotations (with angles scaled by a factor of $\\pi$);\n\n2. a random circuit of ``n_layers``;\n\n3. a final measurement in the computational basis, estimating $4$ expectation values.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev = qml.device(\"default.qubit\", wires=4)\n# Random circuit parameters\nrand_params = np.random.uniform(high=2 * np.pi, size=(n_layers, 4))\n\n@qml.qnode(dev)\ndef circuit(phi):\n    # Encoding of 4 classical input values\n    for j in range(4):\n        qml.RY(np.pi * phi[j], wires=j)\n\n    # Random quantum circuit\n    RandomLayers(rand_params, wires=list(range(4)))\n\n    # Measurement producing 4 classical output values\n    return [qml.expval(qml.PauliZ(j)) for j in range(4)]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The next function defines the convolution scheme:\n\n1. the image is divided into squares of $2 \\times 2$ pixels;\n\n2. each square is processed by the quantum circuit;\n\n3. the $4$ expectation values are mapped into $4$ different\n   channels of a single output pixel.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This process halves the resolution of the input image. In the\n      standard language of CNN, this would correspond to a convolution\n      with a $2 \\times 2$ *kernel* and a *stride* equal to $2$.</p></div>\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def quanv(image):\n    \"\"\"Convolves the input image with many applications of the same quantum circuit.\"\"\"\n    out = np.zeros((14, 14, 4))\n\n    # Loop over the coordinates of the top-left pixel of 2X2 squares\n    for j in range(0, 28, 2):\n        for k in range(0, 28, 2):\n            # Process a squared 2x2 region of the image with a quantum circuit\n            q_results = circuit(\n                [\n                    image[j, k, 0],\n                    image[j, k + 1, 0],\n                    image[j + 1, k, 0],\n                    image[j + 1, k + 1, 0]\n                ]\n            )\n            # Assign expectation values to different channels of the output pixel (j/2, k/2)\n            for c in range(4):\n                out[j // 2, k // 2, c] = q_results[c]\n    return out"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Quantum pre-processing of the dataset\n\nSince we are not going to train the quantum convolution layer, it is more\nefficient to apply it as a \"pre-processing\" layer to all the images of our dataset.\nLater an entirely classical model will be directly trained and tested on the\npre-processed dataset, avoiding unnecessary repetitions of quantum computations.\n\nThe pre-processed images will be saved in the folder ``SAVE_PATH``.\nOnce saved, they can be directly loaded by setting ``PREPROCESS = False``,\notherwise the quantum convolution is evaluated at each run of the code.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["if PREPROCESS == True:\n    q_train_images = []\n    print(\"Quantum pre-processing of train images:\")\n    for idx, img in enumerate(train_images):\n        print(\"{}/{}        \".format(idx + 1, n_train), end=\"\\r\")\n        q_train_images.append(quanv(img))\n    q_train_images = np.asarray(q_train_images)\n\n    q_test_images = []\n    print(\"\\nQuantum pre-processing of test images:\")\n    for idx, img in enumerate(test_images):\n        print(\"{}/{}        \".format(idx + 1, n_test), end=\"\\r\")\n        q_test_images.append(quanv(img))\n    q_test_images = np.asarray(q_test_images)\n\n    # Save pre-processed images\n    np.save(SAVE_PATH + \"q_train_images.npy\", q_train_images)\n    np.save(SAVE_PATH + \"q_test_images.npy\", q_test_images)\n\n\n# Load pre-processed images\nq_train_images = np.load(SAVE_PATH + \"q_train_images.npy\")\nq_test_images = np.load(SAVE_PATH + \"q_test_images.npy\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let us visualize the effect of the quantum convolution\nlayer on a batch of samples:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["n_samples = 4\nn_channels = 4\nfig, axes = plt.subplots(1 + n_channels, n_samples, figsize=(10, 10))\nfor k in range(n_samples):\n    axes[0, 0].set_ylabel(\"Input\")\n    if k != 0:\n        axes[0, k].yaxis.set_visible(False)\n    axes[0, k].imshow(train_images[k, :, :, 0], cmap=\"gray\")\n\n    # Plot all output channels\n    for c in range(n_channels):\n        axes[c + 1, 0].set_ylabel(\"Output [ch. {}]\".format(c))\n        if k != 0:\n            axes[c, k].yaxis.set_visible(False)\n        axes[c + 1, k].imshow(q_train_images[k, :, :, c], cmap=\"gray\")\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Below each input image, the $4$ output channels generated by the\nquantum convolution are visualized in gray scale.\n\nOne can clearly notice the downsampling of the resolution and\nsome local distortion introduced by the quantum kernel.\nOn the other hand the global shape of the image is preserved,\nas expected for a convolution layer.\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Hybrid quantum-classical model\n\nAfter the application of the quantum convolution layer we feed the resulting\nfeatures into a classical neural network that will be trained to classify\nthe $10$ different digits of the MNIST dataset.\n\nWe use a very simple model: just a fully connected layer with\n10 output nodes with a final *softmax* activation function.\n\nThe model is compiled with a *stochastic-gradient-descent* optimizer,\nand a *cross-entropy* loss function.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def MyModel():\n    \"\"\"Initializes and returns a custom Keras model\n    which is ready to be trained.\"\"\"\n    model = keras.models.Sequential([\n        keras.layers.Flatten(),\n        keras.layers.Dense(10, activation=\"softmax\")\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Training\n\nWe first initialize an instance of the model, then we train and validate\nit with the dataset that has been already pre-processed by a quantum convolution.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["q_model = MyModel()\n\nq_history = q_model.fit(\n    q_train_images,\n    train_labels,\n    validation_data=(q_test_images, test_labels),\n    batch_size=4,\n    epochs=n_epochs,\n    verbose=2,\n)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In order to compare the results achievable with and without the quantum convolution layer,\nwe initialize also a \"classical\" instance of the model that will be directly trained\nand validated with the raw MNIST images (i.e., without quantum pre-processing).\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["c_model = MyModel()\n\nc_history = c_model.fit(\n    train_images,\n    train_labels,\n    validation_data=(test_images, test_labels),\n    batch_size=4,\n    epochs=n_epochs,\n    verbose=2,\n)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Results\n\nWe can finally plot the test accuracy and the test loss with respect to the\nnumber of training epochs.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n\nplt.style.use(\"seaborn\")\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 9))\n\nax1.plot(q_history.history[\"val_accuracy\"], \"-ob\", label=\"With quantum layer\")\nax1.plot(c_history.history[\"val_accuracy\"], \"-og\", label=\"Without quantum layer\")\nax1.set_ylabel(\"Accuracy\")\nax1.set_ylim([0, 1])\nax1.set_xlabel(\"Epoch\")\nax1.legend()\n\nax2.plot(q_history.history[\"val_loss\"], \"-ob\", label=\"With quantum layer\")\nax2.plot(c_history.history[\"val_loss\"], \"-og\", label=\"Without quantum layer\")\nax2.set_ylabel(\"Loss\")\nax2.set_ylim(top=2.5)\nax2.set_xlabel(\"Epoch\")\nax2.legend()\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## References\n\n1. Maxwell Henderson, Samriddhi Shakya, Shashindra Pradhan, Tristan Cook.\n   \"Quanvolutional Neural Networks: Powering Image Recognition with Quantum Circuits.\"\n   `arXiv:1904.04767 <https://arxiv.org/abs/1904.04767>`__, 2019.\n\n\n"]}], "metadata": {"kernelspec": {"display_name": "PennyLane", "language": "python", "name": "pennylane"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.10"}}, "nbformat": 4, "nbformat_minor": 0}