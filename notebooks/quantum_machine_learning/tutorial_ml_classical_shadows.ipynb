{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Machine learning for quantum many-body problems\n\n.. meta::\n    :property=\"og:description\": Machine learning for many-body problems\n    :property=\"og:image\": https://pennylane.ai/qml/_images/ml_classical_shadow.png\n\n.. related::\n    tutorial_classical_shadows Classical Shadows\n    tutorial_kernel_based_training Kernel-based training with scikit-learn\n    tutorial_kernels_module Training and evaluating quantum kernels\n\n\n*Author: Utkarsh Azad. Posted: 2 May 2022. Last Updated: 9 May 2022*\n\n\nStoring and processing a complete description of an $n$-qubit quantum mechanical\nsystem is challenging because the amount of memory required generally scales exponentially\nwith the number of qubits. The quantum community has recently addressed this challenge by using\nthe :doc:`classical shadow <tutorial_classical_shadows>` formalism, which allows us to build more concise classical descriptions\nof quantum states using randomized single-qubit measurements. It was argued in Ref. [#preskill]_\nthat combining classical shadows with classical machine learning enables using learning models\nthat efficiently predict properties of the quantum systems, such as the expectation value of a\nHamiltonian, correlation functions, and entanglement entropies.\n\n.. figure:: /demonstrations/ml_classical_shadows/class_shadow_ml.png\n   :align: center\n   :width: 80 %\n   :alt: Combining ML with Classical Shadow\n\n   Combining machine learning and classical shadows\n\n\nIn this demo, we describe one of the ideas presented in Ref. [#preskill]_ for using classical\nshadow formalism and machine learning to predict the ground-state properties of the\n2D antiferromagnetic Heisenberg model. We begin by learning how to build the Heisenberg model,\ncalculate its ground-state properties, and compute its classical shadow. Finally, we demonstrate\nhow to use :doc:`kernel-based learning models <tutorial_kernels_module>` to predict ground-state properties from the learned\nclassical shadows. So let's get started!\n\n## Building the 2D Heisenberg Model\n\nWe define a two-dimensional antiferromagnetic `Heisenberg model\n<https://en.wikipedia.org/wiki/Quantum_Heisenberg_model>`__ as a square\nlattice, where a spin-1/2 particle occupies each site. The antiferromagnetic\nnature and the overall physics of this model depend on the couplings\n$J_{ij}$ present between the spins, as reflected in the Hamiltonian\nassociated with the model:\n\n\\begin{align}H = \\sum_{i < j} J_{ij}(X_i X_j + Y_i Y_j + Z_i Z_j) .\\end{align}\n\nHere, we consider the family of Hamiltonians where all the couplings $J_{ij}$\nare sampled uniformly from [0, 2]. We build a coupling matrix $J$ by providing\nthe number of rows $N_r$ and columns $N_c$ present in the square lattice.\nThe dimensions of this matrix are $N_s \\times N_s$, where $N_s = N_r \\times N_c$\nis the total number of spin particles present in the model.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import itertools as it\nimport pennylane.numpy as np\nimport numpy as anp\n\ndef build_coupling_mats(num_mats, num_rows, num_cols):\n    num_spins = num_rows * num_cols\n    coupling_mats = np.zeros((num_mats, num_spins, num_spins))\n    coup_terms = anp.random.RandomState(24).uniform(0, 2,\n                        size=(num_mats, 2 * num_rows * num_cols - num_rows - num_cols))\n    # populate edges to build the grid lattice\n    edges = [(si, sj) for (si, sj) in it.combinations(range(num_spins), 2)\n                        if sj % num_cols and sj - si == 1 or sj - si == num_cols]\n    for itr in range(num_mats):\n        for ((i, j), term) in zip(edges, coup_terms[itr]):\n            coupling_mats[itr][i][j] = coupling_mats[itr][j][i] = term\n    return coupling_mats"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For this demo, we study a model with four spins arranged on the nodes of\na square lattice. We require four qubits for simulating this model;\none qubit for each spin. We start by building a coupling matrix ``J_mat``\nusing our previously defined function.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["Nr, Nc = 2, 2\nnum_qubits = Nr * Nc  # Ns\nJ_mat = build_coupling_mats(1, Nr, Nc)[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can now visualize the model instance by representing the coupling matrix as a\n``networkx`` graph:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nimport networkx as nx\n\nG = nx.from_numpy_matrix(np.matrix(J_mat), create_using=nx.DiGraph)\npos = {i: (i % Nc, -(i // Nc)) for i in G.nodes()}\nedge_labels = {(x, y): np.round(J_mat[x, y], 2) for x, y in G.edges()}\nweights = [x + 1.5 for x in list(nx.get_edge_attributes(G, \"weight\").values())]\n\nplt.figure(figsize=(4, 4))\nnx.draw(\n    G, pos, node_color=\"lightblue\", with_labels=True,\n    node_size=600, width=weights, edge_color=\"firebrick\",\n)\nnx.draw_networkx_edge_labels(G, pos=pos, edge_labels=edge_labels)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We then use the same coupling matrix ``J_mat`` to obtain the Hamiltonian\n$H$ for the model we have instantiated above.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import pennylane as qml\n\ndef Hamiltonian(J_mat):\n    coeffs, ops = [], []\n    ns = J_mat.shape[0]\n    for i, j in it.combinations(range(ns), r=2):\n        coeff = J_mat[i, j]\n        if coeff:\n            for op in [qml.PauliX, qml.PauliY, qml.PauliZ]:\n                coeffs.append(coeff)\n                ops.append(op(i) @ op(j))\n    H = qml.Hamiltonian(coeffs, ops)\n    return H\n\nprint(f\"Hamiltonian =\\n{Hamiltonian(J_mat)}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For the Heisenberg model, a property of interest is usually the two-body\ncorrelation function $C_{ij}$, which for a pair of spins $i$\nand $j$ is defined as the following operator:\n\n\\begin{align}\\hat{C}_{ij} = \\frac{1}{3} (X_i X_j + Y_iY_j + Z_iZ_j).\\end{align}\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def corr_function(i, j):\n    ops = []\n    for op in [qml.PauliX, qml.PauliY, qml.PauliZ]:\n        if i != j:\n            ops.append(op(i) @ op(j))\n        else:\n            ops.append(qml.Identity(i))\n    return ops"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The expectation value of each such operator $\\hat{C}_{ij}$ with respect to\nthe ground state $|\\psi_{0}\\rangle$ of the model can be used to build\nthe correlation matrix $C$:\n\n\\begin{align}{C}_{ij} = \\langle \\hat{C}_{ij} \\rangle = \\frac{1}{3} \\langle \\psi_{0} | X_i X_j + Y_iY_j + Z_iZ_j | \\psi_{0} \\rangle .\\end{align}\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Hence, to build $C$ for the model, we need to calculate its\nground state $|\\psi_{0}\\rangle$. We do this by diagonalizing\nthe Hamiltonian for the model. Then, we obtain the eigenvector corresponding\nto the smallest eigenvalue.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import scipy as sp\n\nham = Hamiltonian(J_mat)\neigvals, eigvecs = sp.sparse.linalg.eigs(qml.utils.sparse_hamiltonian(ham))\npsi0 = eigvecs[:, np.argmin(eigvals)]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We then build a circuit that initializes the qubits into the ground\nstate and measures the expectation value of the provided set of observables.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev_exact = qml.device(\"default.qubit\", wires=num_qubits) # for exact simulation\n\ndef circuit(psi, observables):\n    psi = psi / np.linalg.norm(psi) # normalize the state\n    qml.QubitStateVector(psi, wires=range(num_qubits))\n    return [qml.expval(o) for o in observables]\n\ncircuit_exact = qml.QNode(circuit, dev_exact)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, we execute this circuit to obtain the exact correlation matrix\n$C$. We compute the correlation operators $\\hat{C}_{ij}$ and\ntheir expectation values with respect to the ground state $|\\psi_0\\rangle$.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["coups = list(it.product(range(num_qubits), repeat=2))\ncorrs = [corr_function(i, j) for i, j in coups]\n\ndef build_exact_corrmat(coups, corrs, circuit, psi):\n    corr_mat_exact = np.zeros((num_qubits, num_qubits))\n    for idx, (i, j) in enumerate(coups):\n        corr = corrs[idx]\n        if i == j:\n            corr_mat_exact[i][j] = 1.0\n        else:\n            corr_mat_exact[i][j] = (\n                np.sum(np.array([circuit(psi, observables=[o]) for o in corr]).T) / 3\n            )\n            corr_mat_exact[j][i] = corr_mat_exact[i][j]\n    return corr_mat_exact\n\nexpval_exact = build_exact_corrmat(coups, corrs, circuit_exact, psi0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Once built, we can visualize the correlation matrix:\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 1, figsize=(4, 4))\nim = ax.imshow(expval_exact, cmap=plt.get_cmap(\"RdBu\"), vmin=-1, vmax=1)\nax.xaxis.set_ticks(range(num_qubits))\nax.yaxis.set_ticks(range(num_qubits))\nax.xaxis.set_tick_params(labelsize=14)\nax.yaxis.set_tick_params(labelsize=14)\nax.set_title(\"Exact Correlation Matrix\", fontsize=14)\n\nbar = fig.colorbar(im, pad=0.05, shrink=0.80    )\nbar.set_label(r\"$C_{ij}$\", fontsize=14, rotation=0)\nbar.ax.tick_params(labelsize=14)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Constructing Classical Shadows\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we have built the Heisenberg model, the next step is to construct\na :doc:`classical shadow <tutorial_classical_shadows>` representation for its ground state. To construct an\napproximate classical representation of an $n$-qubit quantum state $\\rho$,\nwe perform randomized single-qubit measurements on $T$-copies of\n$\\rho$. Each measurement is chosen randomly among the Pauli bases\n$X$, $Y$, or $Z$ to yield random $n$ pure product\nstates $|s_i\\rangle$ for each copy:\n\n\\begin{align}|s_{i}^{(t)}\\rangle \\in \\{|0\\rangle, |1\\rangle, |+\\rangle, |-\\rangle, |i+\\rangle, |i-\\rangle\\}.\\end{align}\n\n\\begin{align}S_T(\\rho) = \\big\\{|s_{i}^{(t)}\\rangle: i\\in\\{1,\\ldots, n\\},\\ t\\in\\{1,\\ldots, T\\} \\big\\}.\\end{align}\n\nEach of the $|s_i^{(t)}\\rangle$ provides us with a snapshot of the\nstate $\\rho$, and the $nT$ measurements yield the complete\nset $S_{T}$, which requires just $3nT$ bits to be stored\nin classical memory. This is discussed in further detail in our previous\ndemo about :doc:`classical shadows <tutorial_classical_shadows>`.\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. figure::  /demonstrations/ml_classical_shadows/class_shadow_prep.png\n   :align: center\n   :width: 100 %\n   :alt: Preparing Classical Shadows\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To prepare a classical shadow for the ground state of the Heisenberg\nmodel, we simply reuse the circuit template used above and reconstruct\na ``QNode`` utilizing a device that performs single-shot measurements.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev_oshot = qml.device(\"default.qubit\", wires=num_qubits, shots=1)\ncircuit_oshot = qml.QNode(circuit, dev_oshot)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, we define a function to build the classical shadow for the quantum\nstate prepared by a given $n$-qubit circuit using $T$-copies\nof randomized Pauli basis measurements\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def gen_class_shadow(circ_template, circuit_params, num_shadows, num_qubits):\n    # prepare the complete set of available Pauli operators\n    unitary_ops = [qml.PauliX, qml.PauliY, qml.PauliZ]\n    # sample random Pauli measurements uniformly\n    unitary_ensmb = np.random.randint(0, 3, size=(num_shadows, num_qubits), dtype=int)\n\n    outcomes = np.zeros((num_shadows, num_qubits))\n    for ns in range(num_shadows):\n        # for each snapshot, extract the Pauli basis measurement to be performed\n        meas_obs = [unitary_ops[unitary_ensmb[ns, i]](i) for i in range(num_qubits)]\n        # perform single shot randomized Pauli measuremnt for each qubit\n        outcomes[ns, :] = circ_template(circuit_params, observables=meas_obs)\n\n    return outcomes, unitary_ensmb\n\n\noutcomes, basis = gen_class_shadow(circuit_oshot, psi0, 100, num_qubits)\nprint(\"First five measurement outcomes =\\n\", outcomes[:5])\nprint(\"First five measurement bases =\\n\", basis[:5])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Furthermore, $S_{T}$ can be used to construct an approximation\nof the underlying $n$-qubit state $\\rho$ by averaging over $\\sigma_t$:\n\n\\begin{align}\\sigma_T(\\rho) = \\frac{1}{T} \\sum_{1}^{T} \\big(3|s_{1}^{(t)}\\rangle\\langle s_1^{(t)}| - \\mathbb{I}\\big)\\otimes \\ldots \\otimes \\big(3|s_{n}^{(t)}\\rangle\\langle s_n^{(t)}| - \\mathbb{I}\\big).\\end{align}\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def snapshot_state(meas_list, obs_list):\n    # undo the rotations done for performing Pauli measurements in the specific basis\n    rotations = [\n        qml.matrix(qml.Hadamard(wires=0)), # X-basis\n        qml.matrix(qml.Hadamard(wires=0)) @ qml.matrix(qml.S(wires=0).inv()), # Y-basis\n        qml.matrix(qml.Identity(wires=0)), # Z-basis\n    ]\n\n    # reconstruct snapshot from local Pauli measurements\n    rho_snapshot = [1]\n    for meas_out, basis in zip(meas_list, obs_list):\n        # preparing state |s_i><s_i| using the post measurement outcome:\n        # |0><0| for 1 and |1><1| for -1\n        state = np.array([[1, 0], [0, 0]]) if meas_out == 1 else np.array([[0, 0], [0, 1]])\n        local_rho = 3 * (rotations[basis].conj().T @ state @ rotations[basis]) - np.eye(2)\n        rho_snapshot = np.kron(rho_snapshot, local_rho)\n\n    return rho_snapshot\n\ndef shadow_state_reconst(shadow):\n    num_snapshots, num_qubits = shadow[0].shape\n    meas_lists, obs_lists = shadow\n\n    # Reconstruct the quantum state from its classical shadow\n    shadow_rho = np.zeros((2 ** num_qubits, 2 ** num_qubits), dtype=complex)\n    for i in range(num_snapshots):\n        shadow_rho += snapshot_state(meas_lists[i], obs_lists[i])\n\n    return shadow_rho / num_snapshots"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To see how well the reconstruction works for different values of $T$,\nwe look at the `fidelity <https://en.wikipedia.org/wiki/Fidelity_of_quantum_states>`_\nof the actual quantum state with respect to the reconstructed quantum state from\nthe classical shadow with $T$ copies. On average, as the number of copies\n$T$ is increased, the reconstruction becomes more effective with average\nhigher fidelity values (orange) and lower variance (blue). Eventually, in the\nlimit $T\\rightarrow\\infty$, the reconstruction will be exact.\n\n.. figure:: /demonstrations/ml_classical_shadows/fidel_snapshot.png\n   :align: center\n   :width: 80 %\n   :alt: Fidelity of reconstructed ground state with different shadow sizes $T$\n\n   Fidelity of the reconstructed ground state with different shadow sizes $T$\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The reconstructed quantum state $\\sigma_T$ can also be used to\nevaluate expectation values $\\text{Tr}(O\\sigma_T)$ for some localized\nobservable $O = \\bigotimes_{i}^{n} P_i$, where $P_i \\in \\{I, X, Y, Z\\}$.\nHowever, as shown above, $\\sigma_T$ would be only an approximation of\n$\\rho$ for finite values of $T$. Therefore, to estimate\n$\\langle O \\rangle$ robustly, we use the median of means\nestimation. For this purpose, we split up the $T$ shadows into\n$K$ equally-sized groups and evaluate the median of the mean\nvalue of $\\langle O \\rangle$ for each of these groups.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def estimate_shadow_obs(shadow, observable, k=10):\n    shadow_size = shadow[0].shape[0]\n\n    # convert Pennylane observables to indices\n    map_name_to_int = {\"PauliX\": 0, \"PauliY\": 1, \"PauliZ\": 2}\n    if isinstance(observable, (qml.PauliX, qml.PauliY, qml.PauliZ)):\n        target_obs = np.array([map_name_to_int[observable.name]])\n        target_locs = np.array([observable.wires[0]])\n    else:\n        target_obs = np.array([map_name_to_int[o.name] for o in observable.obs])\n        target_locs = np.array([o.wires[0] for o in observable.obs])\n\n    # perform median of means to return the result\n    means = []\n    meas_list, obs_lists = shadow\n    for i in range(0, shadow_size, shadow_size // k):\n        meas_list_k, obs_lists_k = (\n            meas_list[i : i + shadow_size // k],\n            obs_lists[i : i + shadow_size // k],\n        )\n        indices = np.all(obs_lists_k[:, target_locs] == target_obs, axis=1)\n        if sum(indices):\n            means.append(\n                np.sum(np.prod(meas_list_k[indices][:, target_locs], axis=1)) / sum(indices)\n            )\n        else:\n            means.append(0)\n\n    return np.median(means)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we estimate the correlation matrix $C^{\\prime}$ from the\nclassical shadow approximation of the ground state.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["coups = list(it.product(range(num_qubits), repeat=2))\ncorrs = [corr_function(i, j) for i, j in coups]\nqbobs = [qob for qobs in corrs for qob in qobs]\n\ndef build_estim_corrmat(coups, corrs, num_obs, shadow):\n    k = int(2 * np.log(2 * num_obs)) # group size\n    corr_mat_estim = np.zeros((num_qubits, num_qubits))\n    for idx, (i, j) in enumerate(coups):\n        corr = corrs[idx]\n        if i == j:\n            corr_mat_estim[i][j] = 1.0\n        else:\n            corr_mat_estim[i][j] = (\n                np.sum(np.array([estimate_shadow_obs(shadow, o, k=k+1) for o in corr])) / 3\n            )\n            corr_mat_estim[j][i] = corr_mat_estim[i][j]\n    return corr_mat_estim\n\nshadow = gen_class_shadow(circuit_oshot, psi0, 1000, num_qubits)\nexpval_estmt = build_estim_corrmat(coups, corrs, len(qbobs), shadow)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This time, let us visualize the deviation observed between the exact correlation\nmatrix ($C$) and the estimated correlation matrix ($C^{\\prime}$)\nto assess the effectiveness of classical shadow formalism.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 1, figsize=(4.2, 4))\nim = ax.imshow(expval_exact-expval_estmt, cmap=plt.get_cmap(\"RdBu\"), vmin=-1, vmax=1)\nax.xaxis.set_ticks(range(num_qubits))\nax.yaxis.set_ticks(range(num_qubits))\nax.xaxis.set_tick_params(labelsize=14)\nax.yaxis.set_tick_params(labelsize=14)\nax.set_title(\"Error in estimating the\\ncorrelation matrix\", fontsize=14)\n\nbar = fig.colorbar(im, pad=0.05, shrink=0.80)\nbar.set_label(r\"$\\Delta C_{ij}$\", fontsize=14, rotation=0)\nbar.ax.tick_params(labelsize=14)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training Classical Machine Learning Models\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There are multiple ways in which we can combine classical shadows and\nmachine learning. This could include training a model to learn\nthe classical representation of quantum systems based on some system\nparameter, estimating a property from such learned classical representations,\nor a combination of both. In our case, we consider the problem of using\n:doc:`kernel-based models <tutorial_kernel_based_training>` to learn the ground-state representation of the\nHeisenberg model Hamiltonian $H(x_l)$ from the coupling vector $x_l$,\nwhere $x_l = [J_{i,j} \\text{ for } i < j]$. The goal is to predict the\ncorrelation functions $C_{ij}$:\n\n\\begin{align}\\big\\{x_l \\rightarrow \\sigma_T(\\rho(x_l)) \\rightarrow \\text{Tr}(\\hat{C}_{ij} \\sigma_T(\\rho(x_l))) \\big\\}_{l=1}^{N}.\\end{align}\n\nHere, we consider the following kernel-based machine learning model [#neurtangkernel]_:\n\n\\begin{align}\\hat{\\sigma}_{N} (x) = \\sum_{l=1}^{N} \\kappa(x, x_l)\\sigma_T (x_l) = \\sum_{l=1}^{N} \\left(\\sum_{l^{\\prime}=1}^{N} k(x, x_{l^{\\prime}})(K+\\lambda I)^{-1}_{l, l^{\\prime}} \\sigma_T(x_l) \\right),\\end{align}\n\nwhere $\\lambda > 0$ is a regularization parameter in cases when\n$K$ is not invertible, $\\sigma_T(x_l)$ denotes the classical\nrepresentation of the ground state $\\rho(x_l)$ of the Heisenberg\nmodel constructed using $T$ randomized Pauli measurements, and\n$K_{ij}=k(x_i, x_j)$ is the kernel matrix with\n$k(x, x^{\\prime})$ as the kernel function.\n\nSimilarly, estimating an expectation value on the predicted ground state\n$\\sigma_T(x_l)$ using the trained model can then be done by\nevaluating:\n\n\\begin{align}\\text{Tr}(\\hat{O} \\hat{\\sigma}_{N} (x)) = \\sum_{l=1}^{N} \\kappa(x, x_l)\\text{Tr}(O\\sigma_T (x_l)).\\end{align}\n\nWe train the classical kernel-based models using $N = 70$\nrandomly chosen values of the coupling matrices $J$.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# imports for ML methods and techniques\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import svm\nfrom sklearn.kernel_ridge import KernelRidge"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First, to build the dataset, we use the function ``build_dataset`` that\ntakes as input the size of the dataset (``num_points``), the topology of\nthe lattice (``Nr`` and ``Nc``), and the number of randomized\nPauli measurements ($T$) for the construction of classical shadows.\nThe ``X_data`` is the set of coupling vectors that are defined as a\nstripped version of the coupling matrix $J$, where only non-duplicate\nand non-zero $J_{ij}$ are considered. The ``y_exact`` and\n``y_clean`` are the set of correlation vectors, i.e., the flattened\ncorrelation matrix $C$, computed with respect to the ground-state\nobtained from exact diagonalization and classical shadow representation\n(with $T=500$), respectively.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def build_dataset(num_points, Nr, Nc, T=500):\n\n    num_qubits = Nr * Nc\n    X, y_exact, y_estim = [], [], []\n    coupling_mats = build_coupling_mats(num_points, Nr, Nc)\n\n    for coupling_mat in coupling_mats:\n        ham = Hamiltonian(coupling_mat)\n        eigvals, eigvecs = sp.sparse.linalg.eigs(qml.utils.sparse_hamiltonian(ham))\n        psi = eigvecs[:, np.argmin(eigvals)]\n        shadow = gen_class_shadow(circuit_oshot, psi, T, num_qubits)\n\n        coups = list(it.product(range(num_qubits), repeat=2))\n        corrs = [corr_function(i, j) for i, j in coups]\n        qbobs = [x for sublist in corrs for x in sublist]\n\n        expval_exact = build_exact_corrmat(coups, corrs, circuit_exact, psi)\n        expval_estim = build_estim_corrmat(coups, corrs, len(qbobs), shadow)\n\n        coupling_vec = []\n        for coup in coupling_mat.reshape(1, -1)[0]:\n            if coup and coup not in coupling_vec:\n                coupling_vec.append(coup)\n        coupling_vec = np.array(coupling_vec) / np.linalg.norm(coupling_vec)\n\n        X.append(coupling_vec)\n        y_exact.append(expval_exact.reshape(1, -1)[0])\n        y_estim.append(expval_estim.reshape(1, -1)[0])\n\n    return np.array(X), np.array(y_exact), np.array(y_estim)\n\nX, y_exact, y_estim = build_dataset(100, Nr, Nc, 500)\nX_data, y_data = X, y_estim\nX_data.shape, y_data.shape, y_exact.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that our dataset is ready, we can shift our focus to the ML models.\nHere, we use two different Kernel functions: (i) Gaussian Kernel and\n(ii) Neural Tangent Kernel. For both of them, we consider the\nregularization parameter $\\lambda$ from the following set of values:\n\n\\begin{align}\\lambda = \\left\\{ 0.0025, 0.0125, 0.025, 0.05, 0.125, 0.25, 0.5, 1.0, 5.0, 10.0 \\right\\}.\\end{align}\n\nNext, we define the kernel functions $k(x, x^{\\prime})$ for each\nof the mentioned kernels:\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\\begin{align}k(x, x^{\\prime}) = e^{-\\gamma||x - x^{\\prime}||^{2}_{2}}. \\tag{Gaussian Kernel}\\end{align}\n\nFor the Gaussian kernel, the hyperparameter\n$\\gamma = N^{2}/\\sum_{i=1}^{N} \\sum_{j=1}^{N} ||x_i-x_j||^{2}_{2} > 0$\nis chosen to be the inverse of the average Euclidean distance\n$x_i$ and $x_j$. The kernel is implemented using the\nradial-basis function (rbf) kernel in the ``sklearn`` library.\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\\begin{align}k(x, x^{\\prime}) = k^{\\text{NTK}}(x, x^{\\prime}). \\tag{Neural Tangent Kernel}\\end{align}\n\nThe neural tangent kernel $k^{\\text{NTK}}$ used here is equivalent\nto an infinite-width feed-forward neural network with four hidden\nlayers and that uses the rectified linear unit (ReLU) as the activation\nfunction. This is implemented using the ``neural_tangents`` library.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from neural_tangents import stax\ninit_fn, apply_fn, kernel_fn = stax.serial(\n    stax.Dense(32),\n    stax.Relu(),\n    stax.Dense(32),\n    stax.Relu(),\n    stax.Dense(32),\n    stax.Relu(),\n    stax.Dense(32),\n    stax.Relu(),\n    stax.Dense(1),\n)\nkernel_NN = kernel_fn(X_data, X_data, \"ntk\")\n\nfor i in range(len(kernel_NN)):\n    for j in range(len(kernel_NN)):\n        kernel_NN[i][j] /= (kernel_NN[i][i] * kernel_NN[j][j]) ** 0.5"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For the above two defined kernel methods, we obtain the best learning\nmodel by performing hyperparameter tuning using cross-validation for\nthe prediction task of each $C_{ij}$. For this purpose, we\nimplement the function ``fit_predict_data``, which takes input as the\ncorrelation function index ``cij``, kernel matrix ``kernel``, and internal\nkernel mapping ``opt`` required by the kernel-based regression models\nfrom the ``sklearn`` library.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from sklearn.metrics import mean_squared_error\n\ndef fit_predict_data(cij, kernel, opt=\"linear\"):\n\n    # training data (estimated from measurement data)\n    y = np.array([y_estim[i][cij] for i in range(len(X_data))])\n    X_train, X_test, y_train, y_test = train_test_split(\n        kernel, y, test_size=0.3, random_state=24\n    )\n\n    # testing data (exact expectation values)\n    y_clean = np.array([y_exact[i][cij] for i in range(len(X_data))])\n    _, _, _, y_test_clean = train_test_split(kernel, y_clean, test_size=0.3, random_state=24)\n\n    # hyperparameter tuning with cross validation\n    models = [\n        # Epsilon-Support Vector Regression\n        (lambda Cx: svm.SVR(kernel=opt, C=Cx, epsilon=0.1)),\n        # Kernel-Ridge based Regression\n        (lambda Cx: KernelRidge(kernel=opt, alpha=1 / (2 * Cx))),\n    ]\n\n    # Regularization parameter\n    hyperparams = [0.0025, 0.0125, 0.025, 0.05, 0.125, 0.25, 0.5, 1.0, 5.0, 10.0]\n    best_pred, best_cv_score, best_test_score = None, np.inf, np.inf\n    for model in models:\n        for hyperparam in hyperparams:\n            cv_score = -np.mean(\n                cross_val_score(\n                    model(hyperparam), X_train, y_train, cv=5,\n                    scoring=\"neg_root_mean_squared_error\",\n                )\n            )\n            if best_cv_score > cv_score:\n                best_model = model(hyperparam).fit(X_train, y_train)\n                best_pred = best_model.predict(X_test)\n                best_cv_score = cv_score\n                best_test_score = mean_squared_error(\n                    best_model.predict(X_test).ravel(), y_test_clean.ravel(), squared=False\n                )\n\n    return (\n        best_pred, y_test_clean, np.round(best_cv_score, 5), np.round(best_test_score, 5)\n    )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We perform the fitting and prediction for each $C_{ij}$ and print\nthe output in a tabular format.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["kernel_list = [\"Gaussian kernel\", \"Neural Tangent kernel\"]\nkernel_data = np.zeros((num_qubits ** 2, len(kernel_list), 2))\ny_predclean, y_predicts1, y_predicts2 = [], [], []\n\nfor cij in range(num_qubits ** 2):\n    y_predict, y_clean, cv_score, test_score = fit_predict_data(cij, X_data, opt=\"rbf\")\n    y_predclean.append(y_clean)\n    kernel_data[cij][0] = (cv_score, test_score)\n    y_predicts1.append(y_predict)\n    y_predict, y_clean, cv_score, test_score = fit_predict_data(cij, kernel_NN)\n    kernel_data[cij][1] = (cv_score, test_score)\n    y_predicts2.append(y_predict)\n\n# For each C_ij print (best_cv_score, test_score) pair\nrow_format = \"{:>25}{:>35}{:>35}\"\nprint(row_format.format(\"Correlation\", *kernel_list))\nfor idx, data in enumerate(kernel_data):\n    print(\n        row_format.format(\n            f\"\\t C_{idx//num_qubits}{idx%num_qubits} \\t| \",\n            str(data[0]),\n            str(data[1]),\n        )\n    )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Overall, we find that the models with the Gaussian kernel performed\nbetter than those with NTK for predicting the expectation value\nof the correlation function $C_{ij}$ for the ground state of\nthe Heisenberg model. However, the best choice of $\\lambda$\ndiffered substantially across the different $C_{ij}$ for both\nkernels. We present the predicted correlation matrix $C^{\\prime}$\nfor randomly selected Heisenberg models from the test set below for\ncomparison against the actual correlation matrix $C$, which is\nobtained from the ground state found using exact diagonalization.\n\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["fig, axes = plt.subplots(3, 3, figsize=(14, 14))\ncorr_vals = [y_predclean, y_predicts1, y_predicts2]\nplt_plots = [1, 14, 25]\n\ncols = [\n    \"From {}\".format(col)\n    for col in [\"Exact Diagonalization\", \"Gaussian Kernel\", \"Neur. Tang. Kernel\"]\n]\nrows = [\"Model {}\".format(row) for row in plt_plots]\n\nfor ax, col in zip(axes[0], cols):\n    ax.set_title(col, fontsize=18)\n\nfor ax, row in zip(axes[:, 0], rows):\n    ax.set_ylabel(row, rotation=90, fontsize=24)\n\nfor itr in range(3):\n    for idx, corr_val in enumerate(corr_vals):\n        shw = axes[itr][idx].imshow(\n            np.array(corr_vals[idx]).T[plt_plots[itr]].reshape(Nr * Nc, Nr * Nc),\n            cmap=plt.get_cmap(\"RdBu\"), vmin=-1, vmax=1,\n        )\n        axes[itr][idx].xaxis.set_ticks(range(Nr * Nc))\n        axes[itr][idx].yaxis.set_ticks(range(Nr * Nc))\n        axes[itr][idx].xaxis.set_tick_params(labelsize=18)\n        axes[itr][idx].yaxis.set_tick_params(labelsize=18)\n\nfig.subplots_adjust(right=0.86)\ncbar_ax = fig.add_axes([0.90, 0.15, 0.015, 0.71])\nbar = fig.colorbar(shw, cax=cbar_ax)\n\nbar.set_label(r\"$C_{ij}$\", fontsize=18, rotation=0)\nbar.ax.tick_params(labelsize=16)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, we also attempt to showcase the effect of the size of training data\n$N$ and the number of Pauli measurements $T$. For this, we look\nat the average root-mean-square error (RMSE) in prediction for each kernel\nover all two-point correlation functions $C_{ij}$. Here, the first\nplot looks at the different training sizes $N$ with a fixed number of\nrandomized Pauli measurements $T=100$. In contrast, the second plot\nlooks at the different shadow sizes $T$ with a fixed training data size\n$N=70$. The performance improvement seems to be saturating after a\nsufficient increase in $N$ and $T$ values for all two kernels\nin both the cases.\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"file://demonstrations/ml_classical_shadows/rmse_training.png\" width=\"47 %\">\n\n<img src=\"file://demonstrations/ml_classical_shadows/rmse_shadow.png\" width=\"47 %\">\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Conclusion\n\nThis demo illustrates how classical machine learning models can benefit from\nthe classical shadow formalism for learning characteristics and predicting\nthe behavior of quantum systems. As argued in Ref. [#preskill]_, this raises\nthe possibility that models trained on experimental or quantum data data can\neffectively address quantum many-body problems that cannot be solved using\nclassical methods alone.\n\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n## References\n\n.. [#preskill]\n\n   H. Y. Huang, R. Kueng, G. Torlai, V. V. Albert, J. Preskill, \"Provably\n   efficient machine learning for quantum many-body problems\",\n   `arXiv:2106.12627 [quant-ph] <https://arxiv.org/abs/2106.12627>`__ (2021)\n\n.. [#neurtangkernel]\n\n   A. Jacot, F. Gabriel, and C. Hongler. \"Neural tangent kernel:\n   Convergence and generalization in neural networks\". `NeurIPS, 8571\u20138580\n   <https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf>`__ (2018)\n\n|\n\n"]}, {"cell_type": "markdown", "metadata": {}, "source": [".. bio:: Utkarsh Azad\n    :photo: ../_static/authors/utkarsh_azad.png\n\n    Utkarsh is a quantum software researcher at Xanadu, working on making quantum computing more useful and accessible, with a focus on exploring its applications in natural sciences. Whenever he's not tinkering with code, he will be walking miles searching either for fractals in nature or a cup of filter coffee.\n\n"]}], "metadata": {"kernelspec": {"display_name": "PennyLane", "language": "python", "name": "pennylane"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.10"}}, "nbformat": 4, "nbformat_minor": 0}