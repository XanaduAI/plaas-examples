{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n\n# How to approximate a classical kernel with a quantum computer\n\n.. meta::\n    :property=\"og:description\": Finding a QK to approximate the Gaussian\n    kernel.\n    :property=\"og:image\": https://pennylane.ai/qml/_images/toy_qek.png\n\n.. related::\n\n   tutorial_kernels_module Training and evaluating quantum kernels\n   tutorial_kernel_based_training Kernel-based training of quantum models with\n   scikit-learn\n   tutorial_expressivity_fourier_series Quantum models as Fourier series\n\n\n*Author: Elies Gil-Fuster (Xanadu resident). Posted: 01 Mar 2022.*\n\nForget about advantages, supremacies, or speed-ups.\nLet us understand better what we can and cannot do with a quantum computer.\nMore specifically, in this demo, we want to look into quantum kernels and ask\nwhether we can replicate classical kernel functions with a quantum computer.\nLots of researchers have lengthily stared at the opposite question, namely that\nof classical simulation of quantum algorithms.\nYet, by studying what classes of functions we can realize with quantum kernels,\nwe can gain some insight into their inner workings.\n\nUsually, in quantum machine learning (QML), we use parametrized quantum circuits\n(PQCs) to find good functions, whatever *good* means here.\nSince kernels are just one specific kind of well-defined\nfunctions, the task of finding a quantum kernel (QK) that approximates a given\nclassical one could be posed as an optimization problem.\nOne way to attack this task is to define a loss function\nquantifying the distance between both functions (the classical kernel function\nand the PQC-based hypothesis).\nThis sort of approach does not help us much to gain theoretical insights about the \nstructure of kernel-emulating quantum circuits, though.\n\nIn order to build intuition, we will instead study the link between classical and quantum kernels through the\nlens of the Fourier representation of a kernel, which is a common tool in\nclassical machine learning.\nTwo functions can only have the same Fourier spectrum if they are the same\nfunction. It turns out that, for certain classes of quantum circuits, `we can\ntheoretically describe the Fourier spectrum rather well\n<https://pennylane.ai/qml/demos/tutorial_expressivity_fourier_series.html>`_.\n\nUsing this theory, together with some good old-fashioned convex optimization, we\nwill derive a quantum circuit that approximates the famous Gaussian kernel.\n\nIn order to keep the demo short and sweet, we focus on one simple example. The\nsame ideas apply to more general scenarios.\nAlso, Refs. [#QEK]_, [#Fourier]_, and [#qkernels]_ should be helpful for those\nwho'd like to see the underlying theory of QKs (and also so-called *Quantum Embedding\nKernels*) and their Fourier representation.\nSo tag along if you'd like to see how we build a quantum kernel that\napproximates the well-known Gaussian kernel function!\n\n|\n\n.. figure:: ../demonstrations/classical_kernels/flowchart.PNG\n    :align: center\n    :width: 60%\n    :target: javascript:void(0)\n\n    Schematic of the steps covered in this demo.\n\n\n\n## Kernel-based Machine Learning\n\nWe will not be reviewing all the notions of kernels in-depth here.\nInstead, we only need to know that an entire branch of machine learning\nrevolves around some functions we call kernels.\nIf you'd like to learn more about where these functions\ncome from, why they're important, and how we can use them (e.g. with\nPennyLane), check out the following demos, which cover different\naspects extensively:\n\n#. `Training and evaluating quantum kernels <https://pennylane.ai/qml/demos/tutorial_kernels_module.html>`_\n#. `Kernel-based training of quantum models with scikit-learn <https://pennylane.ai/qml/demos/tutorial_kernel_based_training.html>`_\n\nFor the purposes of this demo, a *kernel* is a real-valued function of two\nvariables $k(x_1,x_2)$ from a given data domain $x_1,\nx_2\\in\\mathcal{X}$.\nIn this demo, we'll deal with real vector spaces as the data\ndomain $\\mathcal{X}\\subseteq\\mathbb{R}^d$, of some dimension $d$.\nA kernel has to be symmetric with respect to exchanging both variables\n$k(x_1,x_2) = k(x_2,x_1)$.\nWe also enforce kernels to be positive semi-definite, but let's avoid getting\nlost in mathematical lingo. You can trust that all kernels featured in this\ndemo are positive semi-definite.\n\n## Shift-invariant kernels\n\nSome kernels fulfill another important restriction, called *shift-invariance*.\nShift-invariant kernels are those whose value doesn't change if we add a shift\nto both inputs.\nExplicitly, for any suitable shift vector $\\zeta\\in\\mathcal{X}$,\nshift-invariant kernels are those for which\n$k(x_1+\\zeta,x_2+\\zeta)=k(x_1,x_2)$ holds.\nHaving this property means the function can be written in\nterms of only one variable, which we call the *lag vector*\n$\\delta:=x_1-x_2\\in\\mathcal{X}$. Abusing notation a bit:\n\n\\begin{align}k(x_1,x_2)=k(x_1-x_2,0) = k(\\delta).\\end{align}\n\nFor shift-invariant kernels, the exchange symmetry property\n$k(x_1,x_2)=k(x_2,x_1)$ translates into reflection symmetry\n$k(\\delta)=k(-\\delta)$.\nAccordingly, we say $k$ is an *even function*.\n\n## Warm up: Implementing the Gaussian kernel\n\nFirst, let's introduce a simple classical kernel that we will\napproximate on the quantum computer.\nStart importing the usual suspects:\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["import pennylane as qml\nfrom pennylane import numpy as np\nimport matplotlib.pyplot as plt\nimport math\nnp.random.seed(53173)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll look at the Gaussian kernel:\n$k_\\sigma(x_1,x_2):=e^{-\\lVert x_1-x_2\\rVert^2/2\\sigma^2}$.\nThis function is clearly shift-invariant:\n\n\\begin{align}k_\\sigma(x_1+\\zeta,x_2+\\zeta) &= e^{-\\lVert(x_1+\\zeta)-(x_2+\\zeta)\\rVert^2/2\\sigma^2} \\\\\n  & = e^{-\\lVert x_1-x_2\\rVert^2/2\\sigma^2} \\\\\n  & = k_\\sigma(x_1,x_2).\\end{align}\n\nThe object of our study will be a simple version of the Gaussian kernel,\nwhere we consider $1$-dimensional data, so $\\lVert\nx_1-x_2\\rVert^2=(x_1-x_2)^2$.\nAlso, we take $\\sigma=1/\\sqrt{2}$ so that we further simplify the\nexponent.\nWe can always re-introduce it later by rescaling the data.\nAgain, we can write the function in terms of the lag vector only:\n\n\\begin{align}k(\\delta)=e^{-\\delta^2}.\\end{align}\n\nNow let's write a few lines to plot the Gaussian kernel:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def gaussian_kernel(delta):\n    return math.exp(-delta ** 2)\n\ndef make_data(n_samples, lower=-np.pi, higher=np.pi):\n    x = np.linspace(lower, higher, n_samples)\n    y = np.array([gaussian_kernel(x_) for x_ in x])\n    return x,y\n\nX, Y_gaussian = make_data(100)\n\nplt.plot(X, Y_gaussian)\nplt.suptitle(\"The Gaussian kernel with $\\sigma=1/\\sqrt{2}$\")\nplt.xlabel(\"$\\delta$\")\nplt.ylabel(\"$k(\\delta)$\")\nplt.show();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this demo, we will consider only this one example. However, the arguments we\nmake and the code we use are also amenable to any kernel with the following\nmild restrictions:\n\n#. Shift-invariance\n#. Normalization $k(0)=1$.\n#. Smoothness (in the sense of a quickly decaying Fourier spectrum).\n\nNote that is a very large class of kernels!\nAnd also an important one for practical applications.\n\n## Fourier analysis of the Gaussian kernel\n\nThe next step will be to find the Fourier spectrum of the Gaussian\nkernel, which is an easy problem for classical computers.\nOnce we've found it, we'll build a QK that produces a finite Fourier series\napproximation to that spectrum.\n\nLet's briefly recall that a Fourier series is the representation of a\nperiodic function using the sine and cosine functions.\nFourier analysis tells us that we can write any given periodic function as\n\n\\begin{align}f(x) = a_0 + \\sum_{n=1}^\\infty a_n\\cos(n\\omega_0x) + b_n\\sin(n\\omega_0x).\\end{align}\n\nFor that, we only need to find the suitable base frequency $\\omega_0$\nand coefficients $a_0, a_1, \\ldots, b_0, b_1,\\ldots$.\n\nBut the Gaussian kernel is an aperiodic function, whereas the Fourier series\nonly makes sense for periodic functions!\n\n*What can we do?!*\n\nWe can cook up a periodic extension to the Gaussian kernel, for a given\nperiod $2L$ (we take $L=\\pi$ as default):\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def Gauss_p(x, L=np.pi):\n    # Send x to x_mod in the period around 0\n    x_mod = np.mod(x+L, 2*L) - L\n    return gaussian_kernel(x_mod)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["which we can now plot\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["x_func = np.linspace(-10, 10, 321)\ny_func = [Gauss_p(x) for x in x_func]\n\nplt.plot(x_func, y_func)\nplt.xlabel(\"$\\delta$\")\nplt.suptitle(\"Periodic extension to the Gaussian kernel\")\nplt.show();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In practice, we would construct several periodic extensions of the aperiodic\nfunction, with increasing periods.\nThis way, we can study the behaviour when the period approaches\ninfinity, i.e. the regime where the function stops being periodic.\n\nNext up, how does the Fourier spectrum of such an object look like?\nWe can find out using PennyLane's ``fourier`` module!\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["from pennylane.fourier import coefficients"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The function ``coefficients`` computes for us the coefficients of the Fourier\nseries up to a fixed term.\nOne tiny detail here: ``coefficients`` returns one complex number $c_n$\nfor each frequency $n$.\nThe real part corresponds to the $a_n$ coefficient, and the imaginary\npart to the $b_n$ coefficient: $c_n=a_n+ib_n$.\nBecause the Gaussian kernel is an even function, we know that the imaginary part\nof every coefficient will be zero, so $c_n=a_n$.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def fourier_p(d):\n    \"\"\"\n    We only take the first d coefficients [:d]\n    because coefficients() treats the negative frequencies\n    as different from the positive ones.\n    For real functions, they are the same.\n    \"\"\"\n    return np.real(coefficients(Gauss_p, 1, d-1)[:d])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We are restricted to considering only a finite number of Fourier\nterms.\nBut isn't that problematic, one may say?\nWell, maybe.\nSince we know the Gaussian kernel is a smooth function, we expect that the\ncoefficients converge to $0$ at some point, and we will only need to \nconsider terms up to this point.\nLet's look at the coefficients we obtain by setting a low value for the\nnumber of coefficients and then slowly letting it grow:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["N = [0]\nfor n in range(2,7):\n    N.append(n)\n    F = fourier_p(n)\n    plt.plot(N, F, 'x', label='{}'.format(n))\n\nplt.legend()\nplt.xlabel(\"frequency $n$\")\nplt.ylabel(\"Fourier coefficient $c_n$\")\nplt.show();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What do we see?\nFor very small coefficient counts, like $2$ and $3$, we see that\nthe last allowed coefficient is still far from $0$.\nThat's a very clear indicator that we need to consider more frequencies.\nAt the same time, it seems like starting at $5$ or $6$ all the\nnon-zero contributions have already been well captured.\nThis is important for us, since it tells us the minimum number of qubits we should use.\nOne can see that every new qubit doubles the number of frequencies we can\nuse, so for $n$ qubits, we will have $2^n$.\nAt minimum of $6$ frequencies means at least $3$ qubits, corresponding\nto $2^3=8$ frequencies.\nAs we'll see later, we'll work with $5$ qubits, so $32$\nfrequencies.\nThat means the spectrum we will be trying to replicate will be the following:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.plot(range(32), fourier_p(32), 'x')\nplt.xlabel(\"frequency $n$\")\nplt.ylabel(\"Fourier coefficient $c_n$\")\nplt.suptitle(\"Fourier spectrum of the Gaussian kernel\")\nplt.show();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We just need a QK with the same Fourier spectrum!\n\n## Designing a suitable QK\n\nDesigning a suitable QK amounts to designing a suitable parametrized quantum\ncircuit.\nLet's take a moment to refresh the big scheme of things with the following\npicture:\n\n|\n\n.. figure:: ../demonstrations/classical_kernels/QEK.jpg\n    :align: center\n    :width: 70%\n    :target: javascript:void(0)\n\n    The quantum kernel considered in this demo.\n\nWe construct the quantum kernel from a quantum embedding (see the demo on\n`Quantum Embedding Kernels <pennylane.ai/qml/demos/tutorial_kernels_module.html>`_).\nThe quantum embedding circuit will consist of two parts.\nThe first one, trainable, will be a parametrized general state preparation\nscheme $W_a$, with parameters $a$. \nIn the second one, we input the data, denoted by $S(x)$.\n\nStart with the non-trainable gate we'll use to encode the data $S(x)$.\nIt consists of applying one Pauli-$Z$ rotation to each qubit with\nrotation parameter $x$ times some constant $\\vartheta_i$, for\nthe $i^\\text{th}$ qubit.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def S(x, thetas, wires):\n    for (i, wire) in enumerate(wires):\n        qml.RZ(thetas[i] * x, wires = [wire])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["By setting the ``thetas`` properly, we achieve the integer-valued spectrum,\nas required by the Fourier series expansion of a function of period\n$2\\pi$:\n$\\{0, 1, \\ldots, 2^n-2, 2^n-1\\}$, for $n$ qubits.\nSome math shows that setting $\\vartheta_i=2^{n-i}$, for\n$\\{1,\\ldots,n\\}$ produces the desired outcome.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def make_thetas(n_wires):\n    return [2 ** i for i in range(n_wires-1, -1, -1)]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, we introduce the only trainable gate we need to make use of.\nContrary to the usual Ans\u00e4tze used in supervised and unsupervised learning,\nwe use a state preparation template called ``MottonenStatePreparation``.\nThis is one option for amplitude encoding already implemented in PennyLane,\nso we don't need to code it ourselves.\nAmplitude encoding is a common way of embedding classical data into a quantum\nsystem in QML.\nThe unitary associated to this template transforms the $\\lvert0\\rangle$\nstate into a state with amplitudes $a=(a_0,a_1,\\ldots,a_{2^n-1})$,\nnamely $\\lvert a\\rangle=\\sum_j a_j\\lvert j\\rangle$, provided\n$\\lVert a\\rVert^2=1$.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def W(features, wires):\n    qml.templates.state_preparations.MottonenStatePreparation(features, wires)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With that, we have the feature map onto the Hilbert space of the quantum\ncomputer:\n\n\\begin{align}\\lvert x_a\\rangle = S(x)W_a\\lvert0\\rangle,\\end{align}\n\nfor a given $a$, which we will specify later.\n\nAccordingly, we can build the QK corresponding to this feature map as\n\n\\begin{align}k_a(x_1,x_2) &= \\lvert\\langle0\\rvert W_a^\\dagger S^\\dagger(x_1)\n  S(x_2)W_a\\lvert0\\rangle\\rvert^2 \\\\\n  &= \\lvert\\langle0\\rvert W_a^\\dagger S(x_2-x_1) W_a\\lvert0\\rangle\\rvert^2.\\end{align}\n\nIn the code below, the variable ``amplitudes`` corresponds to our set  $a$.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def ansatz(x1, x2, thetas, amplitudes, wires):\n    W(amplitudes, wires)\n    S(x1 - x2, thetas, wires)\n    qml.adjoint(W)(amplitudes, wires)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Since this kernel is by construction real-valued, we also have\n\n\\begin{align}(k_a(x_1,x_2))^\\ast &= k_a(x_1,x_2) \\\\\n  &= \\lvert\\langle0\\rvert W_a^\\dagger S(x_1-x_2) W_a\\lvert0\\rangle\\rvert^2 \\\\\n  &= k_a(x_2,x_1).\\end{align}\n\nFurther, this QK is also shift-invariant $k_a(x_1,x_2) = k_a(x_1+\\zeta,\nx_2+\\zeta)$ for any $\\zeta\\in\\mathbb{R}$.\nSo we can also write it in terms of the lag $\\delta=x_1-x_2$:\n\n\\begin{align}k_a(\\delta) = \\lvert\\langle0\\rvert W_a^\\dagger\n  S(\\delta)W_a\\lvert0\\rangle\\rvert^2.\\end{align}\n\nSo far, we only wrote the gate layout for the quantum circuit, no measurement!\nWe need a few more functions for that!\n\n## Computing the QK function on a quantum device\n\nAlso, at this point, we need to set the number of qubits of our computer.\nFor this example, we'll use the variable ``n_wires``, and set it to $5$.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["n_wires = 5"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We initialize the quantum simulator:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["dev = qml.device(\"default.qubit\", wires = n_wires, shots = None)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, we construct the quantum node:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["@qml.qnode(dev)\ndef QK_circuit(x1, x2, thetas, amplitudes):\n    ansatz(x1, x2, thetas, amplitudes, wires = range(n_wires))\n    return qml.probs(wires = range(n_wires))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Recall that the output of a QK is defined as the probability of obtaining\nthe outcome $\\lvert0\\rangle$ when measuring in the computational basis.\nThat corresponds to the $0^\\text{th}$ entry of ``qml.probs``:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def QK_2(x1, x2, thetas, amplitudes):\n    return QK_circuit(x1, x2, thetas, amplitudes)[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As a couple of quality-of-life improvements, we write a function that implements\nthe QK with the lag $\\delta$ as its argument, and one that implements it\non a given set of data:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def QK(delta, thetas, amplitudes):\n    return QK_2(delta, 0, thetas, amplitudes)\n\ndef QK_on_dataset(deltas, thetas, amplitudes):\n    y = np.array([QK(delta, thetas, amplitudes) for delta in deltas])\n    return y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is also a good place to fix the ``thetas`` array, so that we don't forget\nlater.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["thetas = make_thetas(n_wires)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's see how this looks like for one particular choice of ``amplitudes``.\nWe need to make sure the array fulfills the normalization conditions.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["test_features = np.asarray([1./(1+i) for i in range(2 ** n_wires)])\ntest_amplitudes = test_features / np.sqrt(np.sum(test_features ** 2))\n\nY_test = QK_on_dataset(X, thetas, test_amplitudes)\n\nplt.plot(X, Y_test)\nplt.xlabel(\"$\\delta$\")\nplt.suptitle(\"QK with test amplitudes\")\nplt.show();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["One can see that the stationary kernel with this particular initial state has\na decaying spectrum that looks similar to $1/\\lvert x\\rvert$ \u2014 but not\nyet like a Gaussian.\n\n### How to find the amplitudes emulating a Gaussian kernel\n\nIf we knew exactly which amplitudes to choose in order to build a given\nFourier spectrum, our job would be done here.\nHowever, the equations derived in the literature are not trivial to solve.\n\nAs mentioned in the introduction, one could just \"learn\" this relation, that\nis, tune the parameters of the quantum kernel in a gradient-based manner\nuntil it matches the classical one.\n\nWe want to take an intermediate route between analytical solution and\nblack-box optimization.\nFor that, we derive an equation that links the amplitudes to the spectrum we\nwant to construct and then use old-fashioned convex optimization to find the\nsolution.\nIf you are not interested in the details, you can just jump to the last plots\nof this demo and confirm that we can to emulate the Gaussian kernel\nusing the ansatz for our QK constructed above.\n\nIn order to simplify the formulas, we introduce new variables, which we call\n``probabilities`` $(p_0, p_1, p_2, \\ldots, p_{2^n-1})$, and we define as\n$p_j=\\lvert a_j\\rvert^2$.\nFollowing the normalization property above, we have $\\sum_j p_j=1$.\nDon't get too fond of them, we only need them for this step!\nRemember we introduced the vector $a$ for the\n``MottonenStatePreparation`` as the *amplitudes* of a quantum state?\nThen it makes sense that we call its squares *probabilities*, doesn't it?\n\nThere is a crazy formula that matches the entries of *probabilities* with the\nFourier series of the resulting QK function:\n\n\\begin{align}\\text{probabilities} &\\longrightarrow \\text{Fourier coefficients} \\\\\n  \\begin{pmatrix} p_0 \\\\ p_1 \\\\ p_2 \\\\ \\vdots \\\\ p_{2^n-1} \\end{pmatrix}\n  &\\longmapsto \\begin{pmatrix} \\sum_{j=0}^{2^n-1} p_j^2 \\\\ \\sum_{j=1}^{2^n-1}\n  p_j p_{j-1} \\\\ \\sum_{j=2}^{2^n-1} p_j p_{j-2} \\\\ \\vdots \\\\ p_{2^n-1} p_0\n  \\end{pmatrix}\\end{align}\n\nThis looks a bit scary, it follows from expanding the matrix product\n$W_a^\\dagger S(\\delta)W_a$, and then collecting terms according to\nFourier basis monomials.\nIn this sense, the formula is general and it applies to any shift-invariant\nkernel we might want to approximate, not only the Gaussian kernel.\n\nOur goal is to find the set of $p_j$'s that produces the Fourier\ncoefficients of a given kernel function (in our case, the Gaussian kernel),\nnamely its spectrum $(s_0, s_1, s_2, \\ldots, s_{2^n-1})$.\nWe consider now a slightly different map $F_s$, for a given spectrum\n$(s_0, s_1, \\ldots, s_{2^n-1})$:\n\n\\begin{align}F_s: \\text{probabilities} &\\longrightarrow \\text{Difference between Fourier\n  coefficients} \\\\\n  \\begin{pmatrix} p_0 \\\\ p_1 \\\\ p_2 \\\\ \\vdots \\\\ p_{2^n-1} \\end{pmatrix}\n  &\\longmapsto \\begin{pmatrix} \\sum_{j=0}^{2^n-1} p_j^2 - s_0 \\\\\n  \\sum_{j=1}^{2^n-1} p_j p_{j-1} - s_1 \\\\ \\sum_{j=2}^{2^n-1} p_j\n  p_{j-2} - s_2 \\\\ \\vdots \\\\ p_{2^n-1}p_0 - s_{2^n-1} \\end{pmatrix}.\\end{align}\n\nIf you look at it again, you'll see that the zero (or solution) of this\nsecond map $F_s$ is precisely the array of *probabilities* we are\nlooking for.\nWe can write down the first map as:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def predict_spectrum(probabilities):\n    d = len(probabilities)\n    spectrum = []\n    for s in range(d):\n        s_ = 0\n\n        for j in range(s, d):\n            s_ += probabilities[j] * probabilities[j - s]\n\n        spectrum.append(s_)\n\n    # This is to make the output have the same format as\n    # the output of pennylane.fourier.coefficients\n    for s in range(1,d):\n        spectrum.append(spectrum[d - s])\n\n    return spectrum"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And then $F_s$ is just ``predict_spectrum`` minus the spectrum we want to\npredict:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def F(probabilities, spectrum):\n    d = len(probabilities)\n    return predict_spectrum(probabilities)[:d] - spectrum[:d]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["These closed-form equations allow us to find the solution numerically, using\nNewton's method!\nNewton's method is a classical one from convex optimization theory.\nFor our case, since the formula is quadratic, we rest assured that we are\nwithin the realm of convex functions.\n\n## Finding the solution\n\nIn order to use Newton's method we need the Jacobian of $F_s$.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def J_F(probabilities):\n    d = len(probabilities)\n    J = np.zeros(shape=(d,d))\n    for i in range(d):\n        for j in range(d):\n            if (i + j < d):\n                J[i][j] += probabilities[i + j]\n            if(i - j <= 0):\n                J[i][j] += probabilities[j - i]\n    return J"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Showing that this is indeed $\\nabla F_s$ is left as an exercise for the\nreader.\nFor Newton's method, we also need an initial guess.\nFinding a good initial guess requires some tinkering; different problems will\nbenefit from different ones.\nHere is a tame one that works for the Gaussian kernel.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def make_initial_probabilities(d):\n    probabilities = np.ones(d)\n    deg = np.array(range(1, d + 1))\n    probabilities = probabilities / deg\n    return probabilities\n\nprobabilities = make_initial_probabilities(2 ** n_wires)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Recall the ``spectrum`` we want to match is that of the periodic extension of\nthe Gaussian kernel.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["spectrum = fourier_p(2 ** n_wires)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We fix the hyperparameters for Newton's method:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["d = 2 ** n_wires\nmax_steps = 100\ntol = 1.e-20"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And we're good to go!\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["for step in range(max_steps):\n    inc = np.linalg.solve(J_F(probabilities), -F(probabilities, spectrum))\n    probabilities = probabilities + inc\n    if (step+1) % 10 == 0:\n        print(\"Error norm at step {0:3}: {1}\".format(step + 1,\n                                               np.linalg.norm(F(probabilities,\n                                                                spectrum))))\n        if np.linalg.norm(F(probabilities, spectrum)) < tol:\n            print(\"Tolerance trespassed! This is the end.\")\n            break"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The tolerance we set was fairly low, one should expect good things to come\nout of this.\nLet's have a look at the solution:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.plot(range(d), probabilities, 'x')\nplt.xlabel(\"array entry $j$\")\nplt.ylabel(\"probabilities $p_j$\")\nplt.show();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Would you be able to tell whether this is correct?\nMe neither!\nBut all those probabilities being close to $0$ should make us fear some\nof them must've turned negative.\nThat would be fatal for us.\nFor ``MottonenStatePreparation``, we'll need to give ``amplitudes`` as one of the\narguments, which is the component-wise square root of ``probabilities``.\nAnd hence the problem!\nEven if they are very small values, if any entry of ``probabilities`` is\nnegative, the square root will give ``nan``.\nIn order to avoid that, we use a simple thresholding where we replace very\nsmall entries by $0$.\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def probabilities_threshold_normalize(probabilities, thresh = 1.e-10):\n    d = len(probabilities)\n    p_t = probabilities.copy()\n    for i in range(d):\n        if np.abs(probabilities[i] < thresh):\n            p_t[i] = 0.0\n\n    p_t = p_t / np.sum(p_t)\n\n    return p_t"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Then, we need to take the square root:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["probabilities = probabilities_threshold_normalize(probabilities)\namplitudes = np.sqrt(probabilities)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A little plotting never killed nobody\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.plot(range(d), probabilities, '+', label = \"probability $p_j = |a_j|^2$\")\nplt.plot(range(d), amplitudes, 'x', label = \"amplitude $a_j$\")\nplt.xlabel(\"array entry $j$\")\nplt.legend()\nplt.show();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Visualizing the solution\n\nAnd the moment of truth!\nDoes the solution really match the spectrum?\nWe try it first using ``predict_spectrum`` only\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.plot(range(d), fourier_p(d)[:d], '+', label = \"Gaussian kernel\")\nplt.plot(range(d), predict_spectrum(probabilities)[:d], 'x', label = \"QK predicted\")\nplt.xlabel(\"frequency $n$\")\nplt.ylabel(\"Fourier coefficient\")\nplt.suptitle(\"Fourier spectrum of the Gaussian kernel\")\nplt.legend()\nplt.show();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It seems like it does!\nBut as we just said, this is still only the predicted spectrum.\nWe haven't called the quantum computer at all yet!\n\nLet's see what happens when we call the function ``coefficients`` on the QK\nfunction we defined earlier.\nGood coding practice tells us we should probably turn this step into a function\nitself, in case it is of use later:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["def fourier_q(d, thetas, amplitudes):\n    return np.real(coefficients(lambda x: QK(x, thetas, amplitudes), 1, d-1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And with this, we can finally visualize how the Fourier spectrum of the\nQK function compares to that of the Gaussian kernel:\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.plot(range(d), fourier_p(d)[:d], '+', label = \"Gaussian kernel\")\nplt.plot(range(d), predict_spectrum(probabilities)[:d], 'x', label=\"QK predicted\")\nplt.plot(range(d), fourier_q(d, thetas, amplitudes)[:d], '.', label = \"QK computer\")\nplt.xlabel(\"frequency $n$\")\nplt.ylabel(\"Fourier coefficient\")\nplt.suptitle(\"Fourier spectrum of the Gaussian kernel\")\nplt.legend()\nplt.show();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It seems it went well!\nMatching spectra should mean matching kernel functions, right?\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["Y_learned = QK_on_dataset(X, thetas, amplitudes)\nY_truth = [Gauss_p(x_) for x_ in X]\n\nplt.plot(X, Y_learned, '-.', label = \"QK\")\nplt.plot(X, Y_truth, '--', label = \"Gaussian kernel\")\nplt.xlabel(\"$\\delta$\")\nplt.ylabel(\"$k(\\delta)$\")\nplt.legend()\nplt.show();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Yeah!\nWe did it!\n\n.. figure:: ../demonstrations/classical_kernels/salesman.PNG\n      :align: center\n      :width: 70%\n      :target: javascript:void(0)\n\n## References\n\n.. [#QEK]\n\n      Thomas Hubregtsen, David Wierichs, Elies Gil-Fuster, Peter-Jan HS\n      Derks, Paul K Faehrmann, Johannes Jakob Meyer.\n      \"Training Quantum Embedding Kernels on Near-Term Quantum Computers\".\n      `arXiv preprint arXiv:2105.02276 <https://arxiv.org/abs/2105.02276>`__.\n\n.. [#Fourier]\n\n      Maria Schuld, Ryan Sweke, Johannes Jakob Meyer.\n      \"The effect of data encoding on the expressive power of variational\n      quantum machine learning models\".\n      `Phys. Rev. A 103, 032430 <https://journals.aps.org/pra/abstract/10.1103/PhysRevA.103.032430>`__,\n      `arXiv preprint arXiv:2008.08605 <https://arxiv.org/abs/2008.08605>`__.\n\n.. [#qkernels]\n\n      Maria Schuld.\n      \"Supervised quantum machine learning models are kernel methods\".\n      `arXiv preprint arXiv:2101.11020 <https://arxiv.org/abs/2101.11020>`__.\n\n"]}], "metadata": {"kernelspec": {"display_name": "PennyLane", "language": "python", "name": "pennylane"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.10"}}, "nbformat": 4, "nbformat_minor": 0}